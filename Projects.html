<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Projects</title>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
<link rel="stylesheet" type="text/css" href="style.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<style>
        /* Hero Section with 3D Torus */
        .projects-hero {
            min-height: 100vh;
            background: linear-gradient(135deg, var(--primary-color) 0%, var(--secondary-color) 100%);
            display: flex;
            align-items: center;
            justify-content: center;
            position: relative;
            overflow: hidden;
        }

        #canvas3d {
            color: var(--primary-color);
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            z-index: 1;
        }

        .hero-overlay {
            position: relative;
            z-index: 2;
            text-align: center;
            max-width: 900px;
            padding: 20px;
            background: rgba(0, 0, 0, 0.3);
            border-radius: 20px;
            backdrop-filter: blur(10px);
        }

        .hero-overlay h1 {
            font-size: 4rem;
            color: white;
            margin-bottom: 20px;
            text-shadow: 3px 3px 6px rgba(0,0,0,0.5);
            animation: fadeInDown 1s ease-out;
        }

        .hero-subtitle {
            font-size: 1.6rem;
            color: rgba(255, 255, 255, 0.95);
            margin-bottom: 25px;
            animation: fadeInUp 1s ease-out 0.3s both;
        }

        .hero-description {
            font-size: 1.2rem;
            color: rgba(255, 255, 255, 0.9);
            line-height: 1.8;
            animation: fadeIn 1s ease-out 0.6s both;
        }

        @keyframes fadeInDown {
            from {
                opacity: 0;
                transform: translateY(-30px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        @keyframes fadeInUp {
            from {
                opacity: 0;
                transform: translateY(30px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }

        /* Main Container */
        .main-container {
            max-width: 1550px;
            margin: 10px auto 50px;
            padding: 0 20px;
            position: relative;
            z-index: 2;
            margin-bottom: 10px;
        }

        /* Project Card */
        .project-card {
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.15);
            margin-bottom: 30px;
            overflow: hidden;
            transition: all 0.4s ease;
        }

        .project-card:hover {
            transform: translateY(-10px);
            box-shadow: 0 30px 80px rgba(0, 0, 0, 0.2);
        }

        .project-header {
            background: linear-gradient(135deg, var(--primary-color), var(--secondary-color));
            color: white;
            padding: 30px;
            cursor: pointer;
            position: relative;
            transition: all 0.3s ease;
        }

        .project-header:hover {
            background: linear-gradient(135deg, var(--secondary-color), var(--primary-color));
        }

        .project-meta {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-bottom: 15px;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 8px;
            font-size: 0.95rem;
            opacity: 0.9;
        }

        .meta-item i {
            font-size: 1.1rem;
        }

        .project-title {
            color: white;
            font-size: 2rem;
            margin-bottom: 10px;
            font-weight: 700;
        }

        .project-subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
        }

        .expand-btn {
            position: absolute;
            top: 50%;
            right: 30px;
            transform: translateY(-50%);
            width: 50px;
            height: 50px;
            background: rgba(255, 255, 255, 0.2);
            border: 2px solid white;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.5rem;
            cursor: pointer;
            transition: all 0.3s ease;
        }

        .expand-btn:hover {
            background: white;
            color: var(--primary-color);
            transform: translateY(-50%) rotate(90deg);
        }

        .expand-btn.expanded {
            transform: translateY(-50%) rotate(45deg);
        }

        /* Project Content */
        .project-content {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.5s ease;
        }

        .project-content.expanded {
            max-height: 40000px;
        }

        .content-inner {
            padding: 40px;
        }

        .math-content {
            background: #f8f9fa;
            padding: 30px;
            border-radius: 15px;
            margin: 20px 0;
            border-left: 5px solid var(--secondary-color);
        }

        .math-content h3 {
            color: var(--primary-color);
            margin-bottom: 15px;
            font-size: 1.5rem;
        }

        .theorem, .definition {
            background: white;
            padding: 20px;
            border-radius: 10px;
            margin: 15px 0;
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .theorem-title, .definition-title {
            font-weight: 700;
            color: var(--secondary-color);
            margin-bottom: 10px;
        }

        /* Reading Progress Bar */
        .reading-progress {
            position: fixed;
            top: 0;
            left: 0;
            width: 0%;
            height: 4px;
            background: linear-gradient(90deg, var(--secondary-color), var(--accent-color));
            z-index: 9999;
            transition: width 0.3s ease;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .hero-overlay h1 {
                font-size: 2.5rem;
            }

            .hero-subtitle {
                font-size: 1.2rem;
            }

            .project-title {
                font-size: 1.5rem;
            }

            .expand-btn {
                right: 20px;
            }

            .content-inner {
                padding: 20px;
            }
        }

    .table-container {
    width: 100%;
    max-width: 600px;
    margin: 2rem auto;
    padding: 1.5rem;
    background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
    border-radius: 12px;
    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1), 0 1px 3px rgba(0, 0, 0, 0.08);
    border: 1px solid #dee2e6;
    overflow-x: auto;
    transition: all 0.3s ease;
}

.table-container:hover {
    box-shadow: 0 8px 15px rgba(0, 0, 0, 0.15), 0 3px 6px rgba(0, 0, 0, 0.1);
    transform: translateY(-2px);
}

.table-container table {
    width: 100%;
    border-collapse: collapse;
    font-family: 'Latin Modern Math', 'STIX Two Math', 'Cambria Math', serif;
    font-size: 1.1rem;
    text-align: center;
    background: white;
    border-radius: 8px;
    overflow: hidden;
}

.table-container thead {
    background: linear-gradient(135deg,var(--primary-color) 0%, var(--secondary-color) 100%);
    color: white;
}

.table-container th {
    padding: 1rem 1.5rem;
    font-weight: 600;
    text-transform: uppercase;
    letter-spacing: 0.5px;
    border: none;
    position: relative;
}

.table-container th:after {
    content: '';
    position: absolute;
    bottom: 0;
    left: 10%;
    width: 80%;
    height: 2px;
    background: linear-gradient(90deg, transparent, #3498db, transparent);
}

.table-container tbody tr {
    transition: all 0.2s ease;
    border-bottom: 1px solid #e9ecef;
}

.table-container tbody tr:nth-child(even) {
    background-color: #f8f9fa;
}

.table-container tbody tr:nth-child(odd) {
    background-color: #ffffff;
}

.table-container tbody tr:hover {
    background-color: #e3f2fd;
    transform: scale(1.02);
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
}

.table-container td {
    padding: 1rem 1.5rem;
    border: none;
    font-weight: 500;
    color: #2c3e50;
    position: relative;
}

.table-container td:not(:last-child):after {
    content: '';
    position: absolute;
    right: 0;
    top: 15%;
    height: 70%;
    width: 1px;
    background: linear-gradient(to bottom, transparent, #dee2e6, transparent);
}

/* Mathematical expression styling */
.table-container mjx-container {
    font-size: 1.2em !important;
}

/* Responsive design */
@media (max-width: 768px) {
    .table-container {
        margin: 1rem auto;
        padding: 1rem;
        max-width: 95%;
    }
    
    .table-container table {
        font-size: 1rem;
    }
    
    .table-container th,
    .table-container td {
        padding: 0.75rem 1rem;
    }
}

@media (max-width: 480px) {
    .table-container {
        padding: 0.5rem;
    }
    
    .table-container table {
        font-size: 0.9rem;
    }
    
    .table-container th,
    .table-container td {
        padding: 0.5rem 0.75rem;
    }
}

/* Print styles */
@media print {
    .table-container {
        box-shadow: none;
        border: 2px solid #000;
        background: white;
    }
    
    .table-container thead {
        background: var(--primary-color) !important;

    }
}
    </style>

</head>

<body>

<header>
  <div class="header-container">
    <button class="hamburger" id="hamburger">
      <span></span>
      <span></span>
      <span></span>
    </button>
  </div>
 
</header>

<!-- Theme Toggle Button -->
<div class="theme-toggle-container">
  <button class="theme-toggle-btn" id="themeToggleBtn">
    <span>ðŸŽ¨</span>
    <span id="currentThemeName">Default</span>
  </button>
  <div class="theme-menu" id="themeMenu" tabindex="0">
    <div class="theme-option active" data-theme="default">
      <div class="theme-color-preview" style="background: #1e3a5f;"></div>
      <span>Default</span>
    </div>
    <div class="theme-option" data-theme="copper">
      <div class="theme-color-preview" style="background: #c17d4d;"></div>
      <span>Copper</span>
    </div>
    <div class="theme-option" data-theme="dark">
      <div class="theme-color-preview" style="background: #1a1a2e;"></div>
      <span>Dark Mode</span>
    </div>
    <div class="theme-option" data-theme="monochrome">
      <div class="theme-color-preview" style="background: #ffff;"></div>
      <span>Monochrome</span>
    </div>
    <div class="theme-option" data-theme="nature">
      <div class="theme-color-preview" style="background: #2d6a4f;"></div>
      <span>Green</span>
    </div>
    <div class="theme-option" data-theme="purple">
      <div class="theme-color-preview" style="background: #6a4c93;"></div>
      <span>Modern Purple</span>
    </div>
    <div class="theme-option" data-theme="gray">
      <div class="theme-color-preview" style="background: #444;"></div>
      <span>Gray</span>
    </div>
     <div class="theme-option" data-theme="red">
      <div class="theme-color-preview" style="background: #800000;"></div>
      <span>Red</span>
    </div>
     <div class="theme-option" data-theme="cyan">
      <div class="theme-color-preview" style="background: #006064;"></div>
      <span>Cyan</span>
    </div>
    <div class="theme-option" data-theme="gold">
      <div class="theme-color-preview" style="background: #7a5901;"></div>
      <span>Gold</span>
    </div>
     <div class="theme-option" data-theme="midnight">
      <div class="theme-color-preview" style="background: #0d1b2a;"></div>
      <span>Midnight</span>
    </div>
    <div class="theme-option" data-theme="brown">
      <div class="theme-color-preview" style="background: #5c4033;"></div>
      <span>Brown</span>
    </div>
     <div class="theme-option" data-theme="yellow">
      <div class="theme-color-preview" style="background: #d4a017;"></div>
      <span>Yellow</span>
    </div>
    <div class="theme-option" data-theme="orange">
      <div class="theme-color-preview" style="background:  #ff6b35;"></div>
      <span>Orange</span>
    </div>
    <div class="theme-option" data-theme="pink">
      <div class="theme-color-preview" style="background:  #d16ba5;"></div>
      <span>Pink</span>
    </div>
    <div class="theme-option" data-theme="teal">
      <div class="theme-color-preview" style="background: #008b8b;"></div>
      <span>Teal</span>
    </div>
     <div class="theme-option" data-theme="olive">
      <div class="theme-color-preview" style="background: #708238;"></div>
      <span>Olive</span>
    </div>
     <div class="theme-option" data-theme="crimson">
      <div class="theme-color-preview" style="background:  #a4161a;"></div>
      <span>Crimson</span>
    </div>
    <div class="theme-option" data-theme="amethyst">
      <div class="theme-color-preview" style="background: #9b5de5;"></div>
      <span>Amethyst</span>
    </div>

  </div>
</div>
<div><button id="textToggle" class="text-toggle">ðŸŒ“ Toggle Text Color</button></div>



<!-- Sidebar -->
<div class="sidebar" id="sidebar">
  <a href="index.html">Home</a>
  <a href="About.html">About</a>
  <a href="Research.html">Research</a>
  <a href="Teaching.html">Teaching</a>
  <a href="Projects.html">Projects</a>
  <a href="Software.html">Software</a>
  <a href="Services.html">Services</a>
  <a href="Awards.html">Achievements</a>
  <a href="Resources.html">Resources</a>
</div>

<!-- Overlay -->
<div class="overlay" id="overlay"></div>

<section class="hero">
  <div class="container">
    <h1 class="typewriter">Clinton Oluranran Kayoh</h1>
     <a href="index.html" class="btn">Home</a>
      <a href="About.html" class="btn">About</a>
    <a href="Research.html" class="btn">Research</a>
    <a href="Teaching.html" class="btn">Teaching</a>
     <a href="Projects.html" class="btn">Projects</a>
    <a href="Software.html" class="btn">Software</a>
     <a href="Services.html" class="btn">Services</a>
     <a href="Awards.html" class="btn">Achievements</a>
     <a href="Resources.html" class="btn">Resources</a>
  </div>
</section>

   

    <!-- Reading Progress Bar -->
    <div class="reading-progress" id="readingProgress"></div>

    <!-- Hero Section with 3D Torus -->
    <div class="projects-hero">
        <canvas id="canvas3d"></canvas>
        <div class="hero-overlay">
            <h1>Research Projects</h1>
            <p class="hero-subtitle">Exploring the Architecture of Mathematical Thought</p>
            <p class="hero-description">
            Welcome to my mathematical research project: a curated collection of expository works and 
            research projects bridging abstract theory with profound insight. Each piece represents a 
            journey into the elegant structures that underpin our mathematical universe. 
          </p>
        </div>
    </div>


 <style>
        :root {
            --success-color: #27ae60;
            --warning-color: #f39c12;
            --text-dark: #2c3748;
            --text-light: #7f8c8d;
            --bg-light: #ecf0f1;
            --border-color: #bdc3c7;
            --card-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            --card-shadow-hover: 0 8px 15px rgba(0, 0, 0, 0.2);
        }

        .research-container {
            max-width: 1400px;
            margin: 0 auto;
        }

        .research-header {
            margin-top: 20px;
            text-align: center;
            margin-bottom: 3rem;
            padding: 2rem;
            background: white;
            border-radius: 15px;
            box-shadow: var(--card-shadow);
        }

        .research-header h1 {
            font-size: 2.5rem;
            color: var(--primary-color);
            margin-bottom: 0.5rem;
            font-weight: 700;
            letter-spacing: -0.5px;
        }

        .research-header .subtitle {
            font-size: 1.1rem;
            color: var(--text-light);
            font-style: italic;
        }

        .research-header .divider {
            width: 100px;
            height: 3px;
            background: linear-gradient(90deg, var(--secondary-color), var(--accent-color));
            margin: 1rem auto;
        }

        .filter-controls {
            display: flex;
            justify-content: center;
            gap: 1rem;
            margin-bottom: 2rem;
            flex-wrap: wrap;
        }

        .filter-btn {
            padding: 0.75rem 1.5rem;
            background: white;
            border: 2px solid var(--border-color);
            border-radius: 25px;
            cursor: pointer;
            transition: all 0.3s ease;
            font-size: 0.95rem;
            font-weight: 600;
            color: var(--text-dark);
        }

        .filter-btn:hover {
            border-color: var(--secondary-color);
            color: var(--secondary-color);
            transform: translateY(-2px);
        }

        .filter-btn.active {
            background: var(--secondary-color);
            color: white;
            border-color: var(--secondary-color);
        }

        .research-grid {
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            gap: 2rem;
            margin-bottom: 3rem;
        }

        .research-card {
            background: white;
            border-radius: 12px;
            overflow: hidden;
            box-shadow: var(--card-shadow);
            transition: all 0.4s cubic-bezier(0.175, 0.885, 0.32, 1.275);
            cursor: pointer;
            position: relative;
            display: flex;
            flex-direction: column;
            height: 100%;
        }

        .research-card:hover {
            transform: translateY(-10px);
            box-shadow: var(--card-shadow-hover);
        }

        .research-card::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 5px;
            background: linear-gradient(90deg, var(--secondary-color), var(--accent-color));
            transform: scaleX(0);
            transition: transform 0.3s ease;
        }

        .research-card:hover::before {
            transform: scaleX(1);
        }

        .card-image-container {
            position: relative;
            width: 100%;
            height: 200px;
            overflow: hidden;
            background: linear-gradient(135deg, var(--primary-color), var(--secondary-color));
        }

        .card-image {
            width: 100%;
            height: 100%;
            object-fit: cover;
            transition: transform 0.5s ease;
        }

        .research-card:hover .card-image {
            transform: scale(1.1);
        }

        .image-placeholder {
            width: 100%;
            height: 100%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 3rem;
            color: rgba(255, 255, 255, 0.3);
        }

        .card-badge {
            position: absolute;
            top: 15px;
            right: 15px;
            background: rgba(255, 255, 255, 0.95);
            padding: 0.4rem 0.8rem;
            border-radius: 20px;
            font-size: 0.75rem;
            font-weight: 700;
            color: var(--primary-color);
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2);
        }

        .status-badge {
            position: absolute;
            top: 15px;
            left: 15px;
            padding: 0.3rem 0.7rem;
            border-radius: 15px;
            font-size: 0.7rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        .status-ongoing {
            background: var(--success-color);
            color: white;
        }

        .status-planned {
            background: var(--warning-color);
            color: white;
        }

        .card-content {
            padding: 1.5rem;
            flex: 1;
            display: flex;
            flex-direction: column;
        }

        .card-title {
            font-size: 1.2rem;
            font-weight: 700;
            color: var(--primary-color);
            margin-bottom: 0.75rem;
            line-height: 1.4;
            min-height: 3.5rem;
        }

        .card-overview {
            font-size: 0.95rem;
            color: var(--text-light);
            line-height: 1.6;
            margin-bottom: 1rem;
            flex: 1;
            display: -webkit-box;
            line-clamp: 4;
            -webkit-box-orient: vertical;
            overflow: hidden;
        }

        .card-tags {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-bottom: 1rem;
        }

        .card-tag {
            padding: 0.3rem 0.7rem;
            background: var(--bg-light);
            border-radius: 12px;
            font-size: 0.75rem;
            color: var(--text-dark);
            border: 1px solid var(--border-color);
        }

        .card-footer {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding-top: 1rem;
            border-top: 1px solid var(--border-color);
        }

        .card-meta {
            display: flex;
            gap: 1rem;
            font-size: 0.85rem;
            color: var(--text-light);
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .card-action {
            background: var(--secondary-color);
            color: white;
            padding: 0.5rem 1rem;
            border-radius: 20px;
            border: none;
            cursor: pointer;
            font-size: 0.85rem;
            font-weight: 600;
            transition: all 0.3s ease;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .card-action:hover {
            background: var(--primary-color);
            transform: translateX(5px);
        }

        .research-modal {
            display: none;
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.8);
            z-index: 10000;
            padding: 2rem;
            overflow-y: auto;
            animation: fadeInModal 0.3s ease-out;
        }

        @keyframes fadeInModal {
            from {
                opacity: 0;
            }
            to {
                opacity: 1;
            }
        }

        .research-modal.active {
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .modal-content {
            background: white;
            max-width: 900px;
            width: 100%;
            border-radius: 15px;
            overflow: hidden;
            animation: slideUpModal 0.4s ease-out;
            max-height: 90vh;
            overflow-y: auto;
        }

        @keyframes slideUpModal {
            from {
                transform: translateY(50px);
                opacity: 0;
            }
            to {
                transform: translateY(0);
                opacity: 1;
            }
        }

        .modal-header {
            padding: 2rem;
            background: linear-gradient(135deg, var(--primary-color), var(--secondary-color));
            color: white;
            position: relative;
        }

        .modal-close {
            position: absolute;
            top: 1rem;
            right: 1rem;
            background: rgba(255, 255, 255, 0.2);
            border: none;
            color: white;
            width: 40px;
            height: 40px;
            border-radius: 50%;
            cursor: pointer;
            font-size: 1.5rem;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: all 0.3s ease;
        }

        .modal-close:hover {
            background: rgba(255, 255, 255, 0.3);
            transform: rotate(90deg);
        }

        .modal-body {
            padding: 2rem;
        }

        .modal-image {
            width: 100%;
            max-height: 400px;
            object-fit: cover;
            border-radius: 10px;
            margin-bottom: 2rem;
        }

        .modal-section {
            margin-bottom: 2rem;
        }

        .modal-section h3 {
            color: var(--primary-color);
            margin-bottom: 1rem;
            font-size: 1.4rem;
            border-bottom: 2px solid var(--secondary-color);
            padding-bottom: 0.5rem;
        }

        .modal-section p {
            color: var(--text-dark);
            line-height: 1.8;
            text-align: justify;
        }

        .modal-section ul {
            margin-left: 2rem;
            margin-top: 1rem;
        }

        .modal-section li {
            margin-bottom: 0.5rem;
            color: var(--text-dark);
        }

        @media (max-width: 1200px) {
            .research-grid {
                grid-template-columns: repeat(2, 1fr);
            }
        }

        @media (max-width: 768px) {
            .research-grid {
                grid-template-columns: 1fr;
            }

            .research-header h1 {
                font-size: 2rem;
            }

            .card-content {
                padding: 1rem;
            }
        }

        .loading-spinner {
            display: inline-block;
            width: 20px;
            height: 20px;
            border: 3px solid rgba(255, 255, 255, 0.3);
            border-radius: 50%;
            border-top-color: white;
            animation: spin 1s linear infinite;
        }

        @keyframes spin {
            to {
                transform: rotate(360deg);
            }
        }

        .research-tooltip {
            position: fixed;
            background: var(--primary-color);
            color: white;
            padding: 0.5rem 1rem;
            border-radius: 8px;
            font-size: 0.85rem;
            pointer-events: none;
            opacity: 0;
            transition: opacity 0.3s ease;
            z-index: 10001;
            white-space: nowrap;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
        }

        .research-tooltip.show {
            opacity: 1;
        }

        .scroll-top-btn {
            position: fixed;
            bottom: 2rem;
            left: 0.5rem;
            width: 50px;
            height: 50px;
            background: var(--primary-color);
            color: white;
            border: none;
            border-radius: 50%;
            cursor: pointer;
            display: none;
            align-items: center;
            justify-content: center;
            font-size: 1.2rem;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
            transition: all 0.3s ease;
            z-index: 9999;
        }

        .scroll-top-btn.show {
            display: flex;
        }

        .scroll-top-btn:hover {
            background: var(--secondary-color);
            transform: translateY(-5px);
        }
    </style>
 

   
</head>
<body>
    <div class="research-container">
        <!-- Research Header -->
        <div class="research-header">
            <h1>Mathematical Projects</h1>
            <div class="divider"></div>
            <p class="subtitle">Pushing Limits, Breaking Ground</p>
        </div>

        <!-- Filter Controls -->
        <div class="filter-controls">
            <button class="filter-btn active" onclick="filterResearchProjects('all')">All Projects</button>
            <button class="filter-btn" onclick="filterResearchProjectsByStatus('ongoing')">Ongoing</button>
            <button class="filter-btn" onclick="filterResearchProjectsByStatus('completed')">Completed</button>
            <button class="filter-btn" onclick="filterResearchProjectsByStatus('planned')">Planned</button>
            <button class="filter-btn" onclick="filterResearchProjects('computational')">Computational Biology</button>
            <button class="filter-btn" onclick="filterResearchProjects('mathematical')">Mathematical Modeling</button>
            <button class="filter-btn" onclick="filterResearchProjects('network')">Network Analysis</button>
            <button class="filter-btn" onclick="filterResearchProjects('optimization')">Optimization</button>
        </div>

        <!-- Research Grid -->
        <div class="research-grid" id="researchGrid">
            <!-- Project 1: Astrocytes and Microglia -->
            <div class="research-card" data-category="computational mathematical" data-project-id="1">
                <div class="card-image-container">
                     <img src="project1_astrocytes_microglia.png" alt="Astrocytes and Microglia Research" class="card-image">
                    <span class="status-badge status-planned">Planned</span>
                    <span class="card-badge">Neuroscience</span>
                </div>
                <div class="card-content">
                    <h3 class="card-title">Mathematical Functions of Astrocytes and Microglia to Neurons</h3>
                    <p class="card-overview">
                        Investigating the complex mathematical relationships between glial cells (astrocytes and microglia) and neuronal networks using dynamical systems theory. This research models calcium signaling pathways, neurotransmitter uptake kinetics, and immune response dynamics in the central nervous system.
                    </p>
                    <div class="card-tags">
                        <span class="card-tag">Neuroscience</span>
                        <span class="card-tag">ODEs</span>
                        <span class="card-tag">Calcium Dynamics</span>
                    </div>
                    <div class="card-footer">
                        <div class="card-meta">
                            <span class="meta-item">
                                <i class="fas fa-calendar"></i>
                                2026-2031
                            </span>
                            <span class="meta-item">
                                <i class="fas fa-flask"></i>
                                Lab Study
                            </span>
                        </div>
                        <button class="card-action" onclick="openResearchModal(1)">
                            <span>Learn More</span>
                            <i class="fas fa-arrow-right"></i>
                        </button>
                    </div>
                </div>
            </div>

            <!-- Project 2: Phylogenetic Tree -->
            <div class="research-card" data-category="computational network" data-project-id="2">
                <div class="card-image-container">
                     <img src="project2_phylogenetic_tree.png" alt="Phylogenetic Tree Research" class="card-image">
                    <span class="status-badge status-planned">Planned</span>
                    <span class="card-badge">Evolution</span>
                </div>
                <div class="card-content">
                    <h3 class="card-title">Phylogenetic Tree Reconstruction</h3>
                    <p class="card-overview">
                        Developing advanced algorithms for reconstructing evolutionary relationships using maximum likelihood, Bayesian inference, and distance-based methods. Incorporates molecular sequence data, morphological characters, and statistical models to infer phylogenetic trees with high accuracy.
                    </p>
                    <div class="card-tags">
                        <span class="card-tag">Evolution</span>
                        <span class="card-tag">Algorithms</span>
                        <span class="card-tag">Bayesian</span>
                    </div>
                    <div class="card-footer">
                        <div class="card-meta">
                            <span class="meta-item">
                                <i class="fas fa-calendar"></i>
                                2026-2031
                            </span>
                            <span class="meta-item">
                                <i class="fas fa-code"></i>
                                Computational
                            </span>
                        </div>
                        <button class="card-action" onclick="openResearchModal(2)">
                            <span>Learn More</span>
                            <i class="fas fa-arrow-right"></i>
                        </button>
                    </div>
                </div>
            </div>

            <!-- Project 3: Food Web Analysis -->
            <div class="research-card" data-category="network mathematical" data-project-id="3">
                <div class="card-image-container">
                     <img src="project3_food_web.png" alt="Food Web Research" class="card-image">
                    <span class="status-badge status-ongoing">Ongoing</span>
                    <span class="card-badge">Ecology</span>
                </div>
                <div class="card-content">
                    <h3 class="card-title">Graph Theoretic Analysis of Food Webs</h3>
                    <p class="card-overview">
                        Applying graph theory and network analysis to study ecological food webs. Examines network topology, stability, robustness, and energy flow using centrality measures, community detection algorithms, and dynamical modeling to understand ecosystem resilience and biodiversity patterns.
                    </p>
                    <div class="card-tags">
                        <span class="card-tag">Ecology</span>
                        <span class="card-tag">Graph Theory</span>
                        <span class="card-tag">Networks</span>
                    </div>
                    <div class="card-footer">
                        <div class="card-meta">
                            <span class="meta-item">
                                <i class="fas fa-calendar"></i>
                                2024
                            </span>
                            <span class="meta-item">
                                <i class="fas fa-leaf"></i>
                                Field Study
                            </span>
                        </div>
                        <button class="card-action" onclick="openResearchModal(3)">
                            <span>Learn More</span>
                            <i class="fas fa-arrow-right"></i>
                        </button>
                    </div>
                </div>
            </div>

            <!-- Project 4: Neural Codes -->
            <div class="research-card" data-category="computational mathematical" data-project-id="4">
                <div class="card-image-container">
                     <img src="project4_neural_codes.png" alt="Neural Codes Research" class="card-image">
                    <span class="status-badge status-planned">Planned</span>
                    <span class="card-badge">Neuroscience</span>
                </div>
                <div class="card-content">
                    <h3 class="card-title">Combinatorial Neural Codes</h3>
                    <p class="card-overview">
                        Exploring the mathematical structure of neural codes using combinatorial topology and algebraic geometry. Investigates how neuronal populations encode sensory information through firing patterns, simplicial complexes, and convex codes to understand neural computation principles.
                    </p>
                    <div class="card-tags">
                        <span class="card-tag">Topology</span>
                        <span class="card-tag">Neural Coding</span>
                        <span class="card-tag">Algebra</span>
                    </div>
                    <div class="card-footer">
                        <div class="card-meta">
                            <span class="meta-item">
                                <i class="fas fa-calendar"></i>
                                2026-2031
                            </span>
                            <span class="meta-item">
                                <i class="fas fa-brain"></i>
                                Theory
                            </span>
                        </div>
                        <button class="card-action" onclick="openResearchModal(4)">
                            <span>Learn More</span>
                            <i class="fas fa-arrow-right"></i>
                        </button>
                    </div>
                </div>
            </div>

            <!-- Project 5: Drug Targets -->
            <div class="research-card" data-category="computational network" data-project-id="5">
                <div class="card-image-container">
                     <img src="project5_boolean_network.png" alt="Drug Target Research" class="card-image">
                    <span class="status-badge status-planned">Planned</span>
                    <span class="card-badge">Pharmacology</span>
                </div>
                <div class="card-content">
                    <h3 class="card-title">Identifying Drug Targets Through Boolean Gene Regulation Models</h3>
                    <p class="card-overview">
                        Utilizing Boolean network models to identify potential drug targets in gene regulatory networks. Analyzes network attractors, perturbation dynamics, and feedback loops to predict therapeutic interventions and understand disease mechanisms at the systems level.
                    </p>
                    <div class="card-tags">
                        <span class="card-tag">Boolean Networks</span>
                        <span class="card-tag">Drug Discovery</span>
                        <span class="card-tag">Systems Biology</span>
                    </div>
                    <div class="card-footer">
                        <div class="card-meta">
                            <span class="meta-item">
                                <i class="fas fa-calendar"></i>
                                2026-2031
                            </span>
                            <span class="meta-item">
                                <i class="fas fa-microscope"></i>
                                Clinical
                            </span>
                        </div>
                        <button class="card-action" onclick="openResearchModal(5)">
                            <span>Learn More</span>
                            <i class="fas fa-arrow-right"></i>
                        </button>
                    </div>
                </div>
            </div>

            <!-- Project 6: Drug Adherence -->
            <div class="research-card" data-category="mathematical" data-project-id="6">
                <div class="card-image-container">
                     <img src="project6_drug_adherence.png" alt="Drug Adherence Research" class="card-image">
                    <span class="status-badge status-planned">Planned</span>
                    <span class="card-badge">Healthcare</span>
                </div>
                <div class="card-content">
                    <h3 class="card-title">Modeling Drug Adherence and Forgiveness Mathematically</h3>
                    <p class="card-overview">
                        Developing mathematical models to quantify drug adherence patterns and forgiveness windows in pharmacokinetic/pharmacodynamic systems. Uses stochastic processes, Markov chains, and differential equations to optimize dosing regimens and predict therapeutic outcomes based on patient compliance.
                    </p>
                    <div class="card-tags">
                        <span class="card-tag">Pharmacokinetics</span>
                        <span class="card-tag">Stochastic Models</span>
                        <span class="card-tag">Healthcare</span>
                    </div>
                    <div class="card-footer">
                        <div class="card-meta">
                            <span class="meta-item">
                                <i class="fas fa-calendar"></i>
                                2026-2031
                            </span>
                            <span class="meta-item">
                                <i class="fas fa-hospital"></i>
                                Clinical
                            </span>
                        </div>
                        <button class="card-action" onclick="openResearchModal(6)">
                            <span>Learn More</span>
                            <i class="fas fa-arrow-right"></i>
                        </button>
                    </div>
                </div>
            </div>

            <!-- Project 7: Groebner Bases -->
            <div class="research-card" data-category="mathematical computational" data-project-id="7">
                <div class="card-image-container">
                     <img src="project7_groebner_bases.png" alt="Groebner Bases Research" class="card-image">
                    <span class="status-badge status-planned">Planned</span>
                    <span class="card-badge">Algebra</span>
                </div>
                <div class="card-content">
                    <h3 class="card-title">Reverse Engineering Gene Regulation Networks Using GrÃ¶bner Bases</h3>
                    <p class="card-overview">
                        Applying computational algebraic geometry, specifically GrÃ¶bner bases and polynomial ideals, to reverse engineer gene regulatory networks from expression data. Identifies regulatory relationships, feedback mechanisms, and network motifs through algebraic elimination and ideal operations.
                    </p>
                    <div class="card-tags">
                        <span class="card-tag">Algebraic Geometry</span>
                        <span class="card-tag">GrÃ¶bner Bases</span>
                        <span class="card-tag">Gene Networks</span>
                    </div>
                    <div class="card-footer">
                        <div class="card-meta">
                            <span class="meta-item">
                                <i class="fas fa-calendar"></i>
                                2026-2031
                            </span>
                            <span class="meta-item">
                                <i class="fas fa-dna"></i>
                                Genomics
                            </span>
                        </div>
                        <button class="card-action" onclick="openResearchModal(7)">
                            <span>Learn More</span>
                            <i class="fas fa-arrow-right"></i>
                        </button>
                    </div>
                </div>
            </div>

            <!-- Project 8: Fixed Point Methods -->
            <div class="research-card" data-category="optimization mathematical" data-project-id="8">
                <div class="card-image-container">
                     <img src="project8_fixed_point.png" alt="FPI IN ML Research" class="card-image">
                    <span class="status-badge status-planned">Planned</span>
                    <span class="card-badge">Machine Learning</span>
                </div>
                <div class="card-content">
                    <h3 class="card-title">Application of Fixed Point Iterative Methods in ML Optimization</h3>
                    <p class="card-overview">
                        Investigating the application of fixed point iteration schemes (Mann, Ishikawa, Noor) to machine learning optimization problems. Explores convergence properties, acceleration techniques, and applications to neural network training, reinforcement learning, and deep equilibrium models.
                    </p>
                    <div class="card-tags">
                        <span class="card-tag">Fixed Points</span>
                        <span class="card-tag">ML Optimization</span>
                        <span class="card-tag">Convergence</span>
                    </div>
                    <div class="card-footer">
                        <div class="card-meta">
                            <span class="meta-item">
                                <i class="fas fa-calendar"></i>
                                2026-2031
                            </span>
                            <span class="meta-item">
                                <i class="fas fa-robot"></i>
                                AI/ML
                            </span>
                        </div>
                        <button class="card-action" onclick="openResearchModal(8)">
                            <span>Learn More</span>
                            <i class="fas fa-arrow-right"></i>
                        </button>
                    </div>
                </div>
            </div>

            <!-- Project 9: Transformation and Semi-tensor Product -->
            <div class="research-card" data-category="mathematical network" data-project-id="9">
                <div class="card-image-container">
                     <img src="project9_semi_tensor.png" alt="Semi-Tensor Research" class="card-image">
                    <span class="status-badge status-planned">Planned</span>
                    <span class="card-badge">Systems Biology</span>
                </div>
                <div class="card-content">
                    <h3 class="card-title">Using Transformations and Semi-Tensor Products in Signal Pathway Attractors</h3>
                    <p class="card-overview">
                        Employing transformation monoids and semi-tensor product theory to discover and analyze attractors in cellular signal transduction pathways. Develops algebraic methods for studying pathway dynamics, crosstalk, and stability in Boolean and multi-valued network models.</p>
                    <div class="card-tags">
                        <span class="card-tag">Semi-Tensor Product</span>
                        <span class="card-tag">Signal Pathways</span>
                        <span class="card-tag">Attractors</span>
                    </div>
                    <div class="card-footer">
                        <div class="card-meta">
                            <span class="meta-item">
                                <i class="fas fa-calendar"></i>
                                2026-2031
                            </span>
                            <span class="meta-item">
                                <i class="fas fa-project-diagram"></i>
                                Theory
                            </span>
                        </div>
                        <button class="card-action" onclick="openResearchModal(9)">
                            <span>Learn More</span>
                            <i class="fas fa-arrow-right"></i>
                        </button>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <!-- Research Modal -->
    <div class="research-modal" id="researchModal">
        <div class="modal-content">
            <div class="modal-header">
                <button class="modal-close" onclick="closeResearchModal()">
                    <i class="fas fa-times"></i>
                </button>
                <h2 id="modalTitle">Research Project Title</h2>
            </div>
            <div class="modal-body" id="modalBody">
                <!-- Dynamic content will be loaded here -->
            </div>
        </div>
    </div>

    <!-- Tooltip -->
    <div class="research-tooltip" id="researchTooltip"></div>

    <!-- Scroll to Top Button -->
    <button class="scroll-top-btn" id="scrollTopBtn" onclick="scrollToTopResearch()">
        <i class="fas fa-arrow-up"></i>
    </button>

    
    <script>
        // Research Project Data
        const researchProjectData = {
            1: {
                title: "Mathematical Functions of Astrocytes and Microglia to Neurons",
                fullOverview: `This research project investigates the intricate mathematical relationships between glial cells, specifically astrocytes and microglia, and neuronal networks. Using advanced dynamical systems theory, we model calcium signaling pathways, neurotransmitter uptake kinetics, and immune response dynamics within the central nervous system.
                
                Astrocytes play crucial roles in synaptic transmission, ion homeostasis, and metabolic support. Our models capture the spatiotemporal dynamics of astrocytic calcium waves and their impact on neuronal firing patterns. We employ coupled ordinary differential equations (ODEs) and partial differential equations (PDEs) to describe these interactions.
                
                Microglia, the resident immune cells of the brain, exhibit complex activation states that influence neuroinflammation and synaptic pruning. Our Boolean and multi-state network models capture microglial state transitions and their effects on neuronal health.`,
                objectives: [
                    "Develop comprehensive ODE/PDE models of astrocyte-neuron interactions",
                    "Quantify calcium wave dynamics and their impact on synaptic plasticity",
                    "Model microglial activation states using Boolean networks",
                    "Investigate neuron-glia-neuron signaling pathways",
                    "Predict therapeutic targets for neurological disorders"
                ],
                methods: "Dynamical Systems, Bifurcation Analysis, Numerical Simulation (Python, MATLAB), Parameter Estimation",
                collaborators: "Neuroscience Department, Computational Biology Lab",
                publications: "Check back later..."
            },
            2: {
                title: "Phylogenetic Tree Reconstruction",
                fullOverview: `This project develops and implements state-of-the-art algorithms for reconstructing evolutionary relationships among species. We employ maximum likelihood methods, Bayesian inference, and distance-based approaches to infer phylogenetic trees from molecular sequence data.
                
                Our work incorporates models of nucleotide substitution (GTR, HKY85), rate heterogeneity across sites (gamma distribution), and phylogenetic uncertainty quantification. We use Markov Chain Monte Carlo (MCMC) methods for Bayesian tree estimation and develop efficient algorithms for tree space exploration.
                
                The research addresses computational challenges in analyzing large-scale genomic datasets and develops visualization tools for exploring phylogenetic hypotheses.`,
                objectives: [
                    "Implement efficient maximum likelihood tree estimation algorithms",
                    "Develop Bayesian MCMC methods for posterior tree distributions",
                    "Incorporate rate heterogeneity and molecular clock models",
                    "Create interactive visualization tools for phylogenetic trees",
                    "Apply methods to real-world datasets from genomics projects"
                ],
                methods: "Maximum Likelihood, Bayesian Inference, MCMC, Dynamic Programming, Python/R Implementation",
                collaborators: "Evolutionary Biology Group, Bioinformatics Core",
                publications: "Check back later..."
            },
            3: {
                title: "Graph Theoretic Analysis of Food Webs",
                fullOverview: `This research applies graph theory and network science to study the structure and dynamics of ecological food webs. We analyze trophic networks using centrality measures, community detection algorithms, and dynamical stability analysis.
                
                Our models examine how network topology influences ecosystem stability, resilience to perturbations, and energy flow efficiency. We use spectral graph theory to study matrix stability criteria and identify keystone species through network centrality analysis.
                
                The project combines empirical food web data with theoretical network models to understand biodiversity patterns and predict ecosystem responses to environmental changes.`,
                objectives: [
                    "Characterize food web topology using graph metrics",
                    "Develop stability criteria for complex food webs",
                    "Identify keystone species through centrality analysis",
                    "Model energy flow and biomass dynamics",
                    "Predict ecosystem responses to species loss"
                ],
                methods: "Graph Theory, Network Analysis, Dynamical Systems, Community Detection Algorithms, Python (NetworkX, igraph)",
                collaborators: "Ecology Department, Environmental Science Institute",
                publications: "Check back later..."
            },
            4: {
                title: "Combinatorial Neural Codes",
                fullOverview: `This project explores the mathematical structure of neural codes using tools from combinatorial topology and algebraic geometry. We investigate how neuronal populations encode sensory information through their collective firing patterns.
                
                Neural codes can be represented as simplicial complexes, where neurons correspond to vertices and stimulus-evoked activity patterns define simplices. We study convex codes, which arise from neurons with convex receptive fields, and their realization spaces.
                
                Our work connects neural coding theory with fundamental questions in discrete geometry and provides insights into how the brain represents high-dimensional sensory spaces.`,
                objectives: [
                    "Characterize the structure of convex neural codes",
                    "Develop algorithms for testing code realizability",
                    "Study the geometry of neural code space",
                    "Connect neural codes to place field representations",
                    "Investigate local obstruction theory for codes"
                ],
                methods: "Combinatorial Topology, Algebraic Geometry, Computational Homology, Python (Gudhi, SageMath)",
                collaborators: "Applied Topology Group, Computational Neuroscience Lab",
                publications: "Check back later..."
            },
            5: {
                title: "Identifying Drug Targets Through Boolean Gene Regulation Models",
                fullOverview: `This research utilizes Boolean network models to identify potential drug targets within gene regulatory networks. We analyze network attractors, perturbation dynamics, and feedback loops to predict effective therapeutic interventions.
                
                Boolean networks provide a discrete dynamical systems framework for modeling gene regulation. We compute all attractors (stable states) of regulatory networks and identify control nodes whose perturbation can shift the system from disease to healthy attractors.
                
                Our methods combine computational tools (SAT solvers, BDD algorithms) with biological network data to systematically screen for druggable targets.`,
                objectives: [
                    "Construct Boolean models of disease-relevant gene networks",
                    "Identify all network attractors and their basins",
                    "Perform systematic perturbation analysis",
                    "Rank potential drug targets by efficacy metrics",
                    "Validate predictions with experimental data"
                ],
                methods: "Boolean Networks, Attractor Analysis, SAT Solving, Binary Decision Diagrams, Python (PyBoolNet)",
                collaborators: "Systems Pharmacology Lab, Cancer Biology Institute",
                publications: "Check back later..."
            },
            6: {
                title: "Modeling Drug Adherence and Forgiveness Mathematically",
                fullOverview: `This project develops mathematical models to quantify drug adherence patterns and "forgiveness" windows the time periods during which missed doses minimally impact therapeutic efficacy.
                
                We employ pharmacokinetic/pharmacodynamic (PK/PD) models combined with stochastic processes to simulate realistic adherence scenarios. Markov chain models capture patient behavior patterns, while differential equations describe drug concentration dynamics and physiological responses.
                
                Our models inform optimal dosing regimen design by quantifying the relationship between adherence patterns and clinical outcomes.`,
                objectives: [
                    "Develop PK/PD models with adherence variability",
                    "Quantify forgiveness windows for different drug classes",
                    "Model patient adherence using Markov chains",
                    "Optimize dosing schedules for robustness",
                    "Create decision support tools for clinicians"
                ],
                methods: "Pharmacokinetic Modeling, Stochastic Processes, Markov Chains, Monte Carlo Simulation, MATLAB/R",
                collaborators: "Clinical Pharmacology Department, Public Health School",
                publications: "Check back later..."
            },
            7: {
                title: "Reverse Engineering Gene Regulation Networks Using GrÃ¶bner Bases",
                fullOverview: `This research applies computational algebraic geometry to reverse engineer gene regulatory networks from expression data. We use GrÃ¶bner bases and polynomial ideals to identify regulatory relationships and network structure.
                
                Gene expression data can be represented as polynomial systems where variables correspond to gene expression levels and polynomials encode regulatory relationships. GrÃ¶bner basis computation allows us to solve these systems and eliminate variables to reveal hidden dependencies.
                
                Our algebraic approach complements statistical methods by providing exact solutions to network inference problems under polynomial models.`,
                objectives: [
                    "Formulate gene regulation as polynomial systems",
                    "Apply GrÃ¶bner basis algorithms for network inference",
                    "Identify feedback loops and regulatory motifs",
                    "Develop efficient computational implementations",
                    "Validate inferred networks with experimental data"
                ],
                methods: "GrÃ¶bner Bases, Polynomial Ideals, Computational Algebra, SageMath, Macaulay2",
                collaborators: "Computational Algebra Group, Genomics Institute",
                publications: "Check back later..."
            },
            8: {
                title: "Application of Fixed Point Iterative Methods in ML Optimization",
                fullOverview: `This project investigates the application of fixed point iteration schemes from functional analysis to machine learning optimization problems. We study Mann, Ishikawa, and Noor iteration methods for nonexpansive operators in the context of neural network training.
                
                Fixed point iterations provide alternative optimization frameworks to gradient descent, with potential advantages in convergence properties and computational efficiency. We apply these methods to deep equilibrium models, where the network output is defined implicitly as a fixed point.
                
                Our research develops accelerated variants and proves convergence theorems for machine learning-specific scenarios.`,
                objectives: [
                    "Adapt fixed point methods to ML optimization",
                    "Prove convergence for neural network loss landscapes",
                    "Develop acceleration techniques (momentum, adaptive step sizes)",
                    "Apply to deep equilibrium models and implicit layers",
                    "Compare performance with standard optimizers (Adam, SGD)"
                ],
                methods: "Fixed Point Theory, Nonexpansive Operators, Convergence Analysis, Deep Learning (PyTorch), Numerical Experiments",
                collaborators: "Machine Learning Lab, Applied Mathematics Department",
                publications: "Check back later..."
            },
            9: {
                title: "Using Transformations and Semi-Tensor Products in Signal Pathway Attractors",
                fullOverview: `This research employs transformation monoids and semi-tensor product (STP) theory to discover and analyze attractors in cellular signal transduction pathways. The STP provides a powerful algebraic framework for studying Boolean and multi-valued network dynamics.
                
                Signal pathways can be represented as Boolean control networks, and the STP allows us to express network dynamics as algebraic equations. We use transformation semigroups to study the attractor structure and develop efficient algorithms for attractor identification.
                
                Our methods reveal pathway crosstalk, identify stable signaling states, and predict the effects of perturbations on cellular behavior.`,
                objectives: [
                    "Apply semi-tensor product theory to Boolean networks",
                    "Develop algebraic methods for attractor computation",
                    "Study transformation monoids in network dynamics",
                    "Identify pathway attractors and their basins",
                    "Predict cellular responses to signal perturbations"
                ],
                methods: "Semi-Tensor Product, Transformation Monoids, Boolean Control Networks, Matrix Analysis, MATLAB",
                collaborators: "Systems Biology Center, Control Theory Group",
                publications: "Check back later..."
            }
        };

         // Filter function for research projects
        function filterResearchProjects(category) {
            const cards = document.querySelectorAll('.research-card');
            const filterButtons = document.querySelectorAll('.filter-btn');
            
            // Update active button
            filterButtons.forEach(btn => btn.classList.remove('active'));
            event.target.classList.add('active');
            
            // Filter cards
            cards.forEach(card => {
                const cardCategories = card.getAttribute('data-category');
                
                if (category === 'all' || cardCategories.includes(category)) {
                    card.style.display = 'flex';
                    card.style.animation = 'fadeIn 0.5s ease-out';
                } else {
                    card.style.display = 'none';
                }
            });
        }

        // Filter function for research projects by status
function filterResearchProjectsByStatus(status) {
    const cards = document.querySelectorAll('.research-card');
    const filterButtons = document.querySelectorAll('.filter-btn');
    
    // Update active button
    filterButtons.forEach(btn => btn.classList.remove('active'));
    event.target.classList.add('active');
    
    // Filter cards based on status
    cards.forEach(card => {
        // Find the status badge element within the card
        const statusBadge = card.querySelector('.status-badge');
        let cardStatus = '';
        
        // Extract status from the status badge text content
        if (statusBadge) {
            cardStatus = statusBadge.textContent.trim().toLowerCase();
        }
        
        if (status === 'all' || cardStatus === status) {
            card.style.display = 'flex';
            card.style.animation = 'fadeIn 0.5s ease-out';
        } else {
            card.style.display = 'none';
        }
    });
}

        // Open modal function
        function openResearchModal(projectId) {
            const modal = document.getElementById('researchModal');
            const modalTitle = document.getElementById('modalTitle');
            const modalBody = document.getElementById('modalBody');
            const projectData = researchProjectData[projectId];
            
            modalTitle.textContent = projectData.title;
            
            modalBody.innerHTML = `
                <div class="modal-image">
                    <div style="width: 100%; height: 400px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); display: flex; align-items: center; justify-content: center; border-radius: 10px; color: white; font-size: 4rem;">
                        <i class="fas fa-image"></i>
                    </div>
                </div>
                
                <div class="modal-section">
                    <h3><i class="fas fa-file-alt"></i> Full Overview</h3>
                    <p>${projectData.fullOverview}</p>
                </div>
                
                <div class="modal-section">
                    <h3><i class="fas fa-bullseye"></i> Research Objectives</h3>
                    <ul>
                        ${projectData.objectives.map(obj => `<li>${obj}</li>`).join('')}
                    </ul>
                </div>
                
                <div class="modal-section">
                    <h3><i class="fas fa-tools"></i> Methods & Tools</h3>
                    <p>${projectData.methods}</p>
                </div>
                
                <div class="modal-section">
                    <h3><i class="fas fa-users"></i> Collaborators</h3>
                    <p>${projectData.collaborators}</p>
                </div>
                
                <div class="modal-section">
                    <h3><i class="fas fa-book"></i> Publications</h3>
                    <p>${projectData.publications}</p>
                </div>
            `;
            
            modal.classList.add('active');
            document.body.style.overflow = 'hidden';
        }

        // Close modal function
        function closeResearchModal() {
            const modal = document.getElementById('researchModal');
            modal.classList.remove('active');
            document.body.style.overflow = 'auto';
        }

        // Close modal on outside click
        document.getElementById('researchModal').addEventListener('click', function(e) {
            if (e.target === this) {
                closeResearchModal();
            }
        });

        // Scroll to top function
        function scrollToTopResearch() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        }

        // Show/hide scroll button
        window.addEventListener('scroll', function() {
            const scrollBtn = document.getElementById('scrollTopBtn');
            if (window.pageYOffset > 300) {
                scrollBtn.classList.add('show');
            } else {
                scrollBtn.classList.remove('show');
            }
        });

        // Tooltip functionality
        const researchTooltip = document.getElementById('researchTooltip');
        const tooltipElements = document.querySelectorAll('[data-research-tooltip]');

        tooltipElements.forEach(element => {
            element.addEventListener('mouseenter', function(e) {
                const text = this.getAttribute('data-research-tooltip');
                researchTooltip.textContent = text;
                researchTooltip.classList.add('show');
            });

            element.addEventListener('mousemove', function(e) {
                researchTooltip.style.left = e.pageX + 15 + 'px';
                researchTooltip.style.top = e.pageY + 15 + 'px';
            });

            element.addEventListener('mouseleave', function() {
                researchTooltip.classList.remove('show');
            });
        });

        // Keyboard navigation
        document.addEventListener('keydown', function(e) {
            if (e.key === 'Escape') {
                closeResearchModal();
            }
        });

        // Animation on scroll
        const researchObserver = new IntersectionObserver((entries) => {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    entry.target.style.animation = 'fadeIn 0.6s ease-out forwards';
                }
            });
        }, { threshold: 0.1 });

        document.querySelectorAll('.research-card').forEach(card => {
            researchObserver.observe(card);
        });

        // Console welcome message
        console.log('%c Research Portfolio Loaded Successfully! ', 
            'background: #2c3e50; color: white; padding: 10px 20px; font-size: 14px; font-weight: bold;');
    </script>



   <style>
        .main-wrapper {
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            max-width: 1490px;
            margin: 0 auto;
            width: 100%;
        }

        .expository-container {
            margin-top: 20px;
            max-width: 1550px;
            width: 100%;
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
            overflow: hidden;
            animation: slideIn 0.6s ease-out;
            margin-bottom: 2rem;
        }

        @keyframes slideIn {
            from {
                opacity: 0;
                transform: translateY(30px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .expository-header {
            background: linear-gradient(135deg, var(--primary-color) 0%, var(--secondary-color) 100%);
            padding: 2.5rem 2rem 2rem 2rem;
            position: relative;
            overflow: hidden;
        }

        .expository-header::before {
            content: '';
            position: absolute;
            top: -50%;
            right: -10%;
            width: 300px;
            height: 300px;
            background: rgba(255, 255, 255, 0.1);
            border-radius: 50%;
            animation: float 6s ease-in-out infinite;
        }

        @keyframes float {
            0%, 100% {
                transform: translateY(0) rotate(0deg);
            }
            50% {
                transform: translateY(-20px) rotate(180deg);
            }
        }

        .expository-badge {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            background: rgba(255, 255, 255, 0.2);
            backdrop-filter: blur(10px);
            padding: 0.5rem 1rem;
            border-radius: 50px;
            color: white;
            font-size: 0.85rem;
            font-weight: 600;
            letter-spacing: 1px;
            text-transform: uppercase;
            margin-bottom: 1rem;
            animation: pulse 2s ease-in-out infinite;
        }

        @keyframes pulse {
            0%, 100% {
                transform: scale(1);
            }
            50% {
                transform: scale(1.05);
            }
        }

        .expository-title {
            color: white;
            font-size: 2.5rem;
            font-weight: 700;
            margin-bottom: 1rem;
            position: relative;
            z-index: 1;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);
        }

        .expository-subtitle {
            color: rgba(255, 255, 255, 0.9);
            font-size: 1.1rem;
            font-weight: 400;
            position: relative;
            z-index: 1;
        }

        .quote-section {
            background: var(--bg-light);
            padding: 2rem;
            border-left: 4px solid var(--primary-color);
            margin: 2rem;
            border-radius: 10px;
            position: relative;
            transition: all 0.3s ease;
            cursor: pointer;
        }

        .quote-section:hover {
            transform: translateX(5px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .quote-icon {
            position: absolute;
            top: -15px;
            left: 20px;
            background: var(--primary-color);
            color: white;
            width: 40px;
            height: 40px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.2rem;
            box-shadow: 0 4px 10px rgba(102, 126, 234, 0.4);
        }

        .quote-text {
            font-style: italic;
            color: var(--text-dark);
            font-size: 1.1rem;
            line-height: 1.8;
            margin-bottom: 1rem;
            padding-left: 1rem;
        }

        .quote-author {
            color: var(--primary-color);
            font-weight: 600;
            font-size: 1rem;
            text-align: right;
            padding-right: 1rem;
        }

        .quote-author::before {
            content: 'â€” ';
        }

        .content-section {
            padding: 2rem;
        }

        .welcome-text {
            color: var(--text-light);
            font-size: 1.05rem;
            line-height: 1.8;
            margin-bottom: 2rem;
        }

        .features-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 1.5rem;
            margin-bottom: 2rem;
        }

        .feature-card {
            background: white;
            border: 2px solid var(--border-color);
            border-radius: 12px;
            padding: 1.5rem;
            transition: all 0.3s ease;
            cursor: pointer;
            position: relative;
            overflow: hidden;
        }

        .feature-card::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 4px;
            background: linear-gradient(90deg, var(--primary-color), var(--accent-color));
            transform: scaleX(0);
            transition: transform 0.3s ease;
        }

        .feature-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 25px rgba(0, 0, 0, 0.1);
            border-color: var(--primary-color);
        }

        .feature-card:hover::before {
            transform: scaleX(1);
        }

        .feature-icon {
            width: 50px;
            height: 50px;
            background: linear-gradient(135deg, var(--primary-color), var(--secondary-color));
            border-radius: 12px;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-size: 1.5rem;
            margin-bottom: 1rem;
            transition: transform 0.3s ease;
        }

        .feature-card:hover .feature-icon {
            transform: rotate(360deg);
        }

        .feature-title {
            color: var(--text-dark);
            font-size: 1.1rem;
            font-weight: 600;
            margin-bottom: 0.5rem;
        }

        .feature-description {
            color: var(--text-light);
            font-size: 0.95rem;
            line-height: 1.6;
        }

        .cta-section {
            background: linear-gradient(135deg, var(--primary-color) 0%, var(--secondary-color) 100%);
            padding: 2rem;
            text-align: center;
            border-radius: 12px;
            margin: 0 2rem 2rem 2rem;
        }

        .cta-button {
            background: white;
            color: var(--primary-color);
            padding: 1rem 2.5rem;
            border: none;
            border-radius: 50px;
            font-size: 1.1rem;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s ease;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
        }

        .cta-button:hover {
            transform: translateY(-3px);
            box-shadow: 0 6px 20px rgba(0, 0, 0, 0.3);
        }

        .cta-button:active {
            transform: translateY(-1px);
        }

        .tag-cloud {
        display: flex;
        flex-wrap: wrap;
        gap: 0.75rem;
        padding: 2rem;
        justify-content: center;
    }

    .tag {
        background: var(--accent-color);
        color: white;
        padding: 0.5rem 1rem;
        border-radius: 20px;
        font-size: 0.9rem;
        border: 1px solid var(--accent-color);
        transition: all 0.3s ease;
        cursor: pointer;
    }

    .tag:hover {
        background: var(--primary-color);
        color: white;
        transform: scale(1.1);
    }


        /* Expository Content List Styles */
        .expository-list {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.5s ease-out;
            margin: 0 2rem 2rem 2rem;
        }

        .expository-list.show {
            max-height: 50000px;
            transition: max-height 1s ease-in;
        }

        @media (max-width: 768px) {
            body {
                padding: 1rem;
            }

            .expository-title {
                font-size: 2rem;
            }

            .features-grid {
                grid-template-columns: 1fr;
            }

            .quote-section {
                margin: 1rem;
            }

            .content-section {
                padding: 1rem;
            }
        }

        .fade-in {
            animation: fadeIn 0.8s ease-out forwards;
            opacity: 0;
        }

        @keyframes fadeIn {
            to {
                opacity: 1;
            }
        }

        .delay-1 { animation-delay: 0.2s; }
        .delay-2 { animation-delay: 0.4s; }
        .delay-3 { animation-delay: 0.6s; }
        .delay-4 { animation-delay: 0.8s; }
    </style>

    <div class="main-wrapper">
        <div class="expository-container">
            <!-- Header Section -->
            <div class="expository-header">
                <div class="expository-badge">
                    <i class="fas fa-book-open"></i>
                    <span>Mathematical Expository</span>
                </div>
                <h1 class="expository-title">A Journey Through Mathematical Ideas</h1>
                <p class="expository-subtitle">An in-depth exploration of fundamental concepts and elegant proofs</p>
            </div>

            <!-- Quote Section -->
            <div class="quote-section" data-tooltip="Click to reveal another quote">
                <div class="quote-icon">
                    <i class="fas fa-quote-left"></i>
                </div>
                <p class="quote-text" id="quoteText">
                    "Mathematics is not about numbers, equations, computations, or algorithms: it is about understanding."
                </p>
                <p class="quote-author" id="quoteAuthor">William Paul Thurston</p>
            </div>

            <!-- Content Section -->
            <div class="content-section">
                <p class="welcome-text fade-in">
                    Welcome to this mathematical expository! This document is designed to guide you through complex mathematical concepts with clarity and precision. Whether you're a student, researcher, or enthusiast, you'll find rigorous proofs, illuminating examples, and intuitive explanations that bridge theory and application.
                </p>

                <!-- Features Grid -->
                <div class="features-grid">
                    <div class="feature-card fade-in delay-1">
                        <div class="feature-icon">
                            <i class="fas fa-book"></i>
                        </div>
                        <h3 class="feature-title">Comprehensive Theory</h3>
                        <p class="feature-description">
                            Detailed theoretical foundations with complete proofs and logical development.
                        </p>
                    </div>

                    <div class="feature-card fade-in delay-2">
                        <div class="feature-icon">
                            <i class="fas fa-lightbulb"></i>
                        </div>
                        <h3 class="feature-title">Worked Examples</h3>
                        <p class="feature-description">
                            Step-by-step solutions and computational techniques for practical understanding.
                        </p>
                    </div>

                    <div class="feature-card fade-in delay-3">
                        <div class="feature-icon">
                            <i class="fas fa-chart-line"></i>
                        </div>
                        <h3 class="feature-title">Visual Insights</h3>
                        <p class="feature-description">
                            Diagrams, graphs, and geometric interpretations to build intuition.
                        </p>
                    </div>

                    <div class="feature-card fade-in delay-4">
                        <div class="feature-icon">
                            <i class="fas fa-tasks"></i>
                        </div>
                        <h3 class="feature-title">Practice Exercises</h3>
                        <p class="feature-description">
                            Carefully selected problems to reinforce understanding and develop skills.
                        </p>
                    </div>
                </div>
            </div>

             <!-- Tag Cloud -->
            <div class="tag-cloud">
                <span class="tag">Theorems</span>
                <span class="tag">Proofs</span>
                <span class="tag">Examples</span>
                <span class="tag">Algorithms</span>
                <span class="tag">Applications</span>
                <span class="tag">Exercises</span>
                <span class="tag">Visualizations</span>
            </div>
        


            <!-- Call to Action -->
            <div class="cta-section">
                <h3 style="color: white; margin-bottom: 1rem; font-size: 1.5rem;">Ready to Begin?</h3>
                <button class="cta-button" onclick="toggleExpositoryList()">
                    <span id="btnText">Start Exploring</span>
                    <i class="fas fa-arrow-right" id="btnIcon"></i>
                </button>
            </div>

            <!-- Expository Content List (Initially Hidden) -->
            <div class="expository-list" id="expositoryList">
                <!-- Project Card 1: Transformation Monoids -->
             <div class="main-container">
        <div class="project-card">
            <div class="project-header" onclick="toggleContent('transformation-monoids')">
                <div class="project-meta">
                    <span class="meta-item">
                <i class="fas fa-calendar-alt"></i>
                     Added: September 28, 2025
                  </span>
                    <span class="meta-item">
                        <i class="fas fa-clock"></i>
                        2hr 15 min read
                    </span>
                    <span class="meta-item">
                        <i class="fas fa-tag"></i>
                        Semigroup & Algebraic Combinatorics
                    </span>
                </div>
                <h2 class="project-title">Transformation Monoids: Structure, Conjugacy, and Ideals</h2>
                <p class="project-subtitle">Author: Clinton Oluranran Kayoh</p>
                <div class="expand-btn" id="btn-transformation-monoids">
                    <i class="fas fa-plus"></i>
                </div>
            </div>
            <div class="project-content active" id="content-transformation-monoids">
                <div class="content-inner">
                    <!-- Abstract -->
                    <div class="math-content">
                        <h3>Abstract</h3>
                        <p>
                            This exposition provides a comprehensive introduction to the algebraic structure of transformation monoids, focusing on their classification, conjugacy relations, and ideal structure. We systematically develop the theory of full and partial transformation monoids on finite sets, emphasizing the interplay between combinatorial invariants (rank, image, kernel) and algebraic properties (conjugacy classes, ideals, Green's relations). Key submonoids including symmetric groups, order-preserving transformations, and contractions are examined in detail. Through worked examples and explicit computations, we illustrate how structural invariants such as index, period, and path decomposition characterize transformation behavior. The exposition balances rigorous definitions with intuitive geometric interpretations, making the rich theory accessible to graduate students in algebra, semigroup theory, and combinatorics.
                        </p>
                    </div>

                    <!-- Table of Contents -->
                    <div class="toc-container">
                        <h3>Table of Contents</h3>
                        <ul class="toc-list">
                            <li><a href="#section-introduction">1. Introduction</a></li>
                            <li><a href="#section-definitions">2. Basic Definitions and Classifications</a></li>
                            <li><a href="#section-invariants">3. Structural Invariants and Properties</a></li>
                            <li><a href="#section-conjugacy">4. Conjugacy and Classification</a></li>
                            <li><a href="#section-ideals">5. Ideals and Green's Relations</a></li>
                            <li><a href="#section-conclusion">6. Conclusion</a></li>
                            <li><a href="#section-appendix">Appendix: Worked Examples</a></li>
                            <li><a href="#section-references">References</a></li>
                        </ul>
                    </div>

                    <!-- Section 1: Introduction -->
                    <div class="math-content" id="section-introduction">
                        <h3>1. Introduction</h3>
                        
                        <h4>1.1 Motivation and Historical Context</h4>
                        <p>
                            A <strong>transformation monoid</strong> is a collection of self-maps on a set, closed under function composition. Formally, given a finite set \(X_n = \{1, 2, \ldots, n\}\), the <strong>full transformation monoid</strong> \(\mathcal{T}_n\) consists of all mappings \(\alpha: X_n \to X_n\) with the binary operation of function composition. This structure, equipped with the identity transformation, forms a monoid, one of the most fundamental objects in semigroup theory.
                        </p>
                        <p>
                            The study of transformation monoids traces back to Cayley's 1854 theorem, which showed that every group is isomorphic to a subgroup of a symmetric group (the group of bijective transformations). This universality extends beyond groups: every finite semigroup embeds into some transformation monoid \(\mathcal{T}_n\), making transformation monoids universal objects for studying finite algebraic structures (<em>Howie, 1995</em>).
                        </p>
                        <p>
                            Transformation monoids appear naturally across mathematics and computer science:
                        </p>
                        <ul>
                            <li><strong>Automata theory</strong>: State transitions in finite automata form transformation monoids</li>
                            <li><strong>Combinatorics</strong>: Counting problems involving functions and bijections</li>
                            <li><strong>Representation theory</strong>: Actions of semigroups on vector spaces</li>
                            <li><strong>Theoretical computer science</strong>: Complexity of computational problems on transformations</li>
                        </ul>
                        <p>
                            This document explores the algebraic structure of transformation monoids through several lenses: we classify important submonoids, introduce structural invariants (rank, index, period), study conjugacy classes, and analyze the ideal lattice. Throughout, we emphasize concrete examples and computational techniques.
                        </p>

                        <h4>1.2 Notation and Roadmap</h4>
                        
                        <table>
                            <caption>Table 1: Standard Notation for Transformation Monoids</caption>
                            <thead>
                                <tr>
                                    <th>Symbol</th>
                                    <th>Meaning</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr><td>\(\mathcal{T}_n\)</td><td>Full transformation monoid on \(X_n = \{1, \ldots, n\}\)</td></tr>
                                <tr><td>\(\mathcal{PT}_n\)</td><td>Partial transformation monoid (partial maps)</td></tr>
                                <tr><td>\(\mathcal{I}_n\)</td><td>Partial one-to-one transformation monoid (partial injections)</td></tr>
                                <tr><td>\(\mathcal{S}_n\)</td><td>Symmetric group (bijective transformations/permutations)</td></tr>
                                <tr><td>\(\mathcal{O}_n\)</td><td>Order-preserving transformation monoid</td></tr>
                                <tr><td>\(\mathcal{OR}_n\)</td><td>Order-preserving or order-reversing monoid</td></tr>
                                 <tr><td>\(\mathcal{OD}_n\)</td><td>Order-decreasing monoid</td></tr>
                                <tr><td>\(\mathcal{C}_n\)</td><td>Contraction monoid (Lipschitz maps with constant 1)</td></tr>
                                <tr><td>\(\text{im}(\alpha)\)</td><td>Image of transformation \(\alpha\)</td></tr>
                                <tr><td>\(\text{ker}(\alpha)\)</td><td>Kernel equivalence relation of \(\alpha\)</td></tr>
                                <tr><td>\(r(\alpha)\)</td><td>Rank of \(\alpha\): \(r(\alpha) = |\text{im}(\alpha)|\)</td></tr>
                                <tr><td>\(\text{ind}(\alpha)\)</td><td>Index of \(\alpha\) (pre-periodic length)</td></tr>
                                <tr><td>\(\text{per}(\alpha)\)</td><td>Period of \(\alpha\) (periodic length)</td></tr>
                            </tbody>
                        </table>

                        <p>
                            We represent transformations in <strong>two-line notation</strong>:
                        </p>
                        <div class="equation">
                            <div class="equation-content">$$\alpha = \begin{pmatrix} 1 & 2 & 3 & \cdots & n \\ \alpha(1) & \alpha(2) & \alpha(3) & \cdots & \alpha(n) \end{pmatrix}$$</div>
                            <div class="equation-number">(1)</div>
                        </div>

                        <p>
                            <strong>Roadmap</strong>: Section 2 defines fundamental transformation monoids and computes their orders. Section 3 introduces structural invariants: image, kernel, rank, index, and period. Section 4 studies conjugacy relations and classifies idempotent and nilpotent conjugacy classes. Section 5 examines the ideal structure and Green's relations. Section 6 concludes with connections to current research.
                        </p>
                    </div>

                    <!-- Section 2: Basic Definitions -->
                    <div class="math-content" id="section-definitions">
                        <h3>2. Basic Definitions and Classifications</h3>
                        
                        <h4>2.1 The Full Transformation Monoid</h4>
                        
                        <div class="definition">
                            <p class="definition-title">Definition 2.1 [Full Transformation Monoid]</p>
                            <p>
                                The <strong>full transformation monoid</strong> \(\mathcal{T}_n\) is the set of all mappings \(\alpha: X_n \to X_n\) where \(X_n = \{1, 2, \ldots, n\}\), equipped with function composition:
                                \[(\alpha \circ \beta)(x) = \alpha(\beta(x)).\]
                                The identity map \(\text{id}_n(x) = x\) serves as the identity element.
                            </p>
                        </div>

                        <div class="proposition">
                            <p class="proposition-title">Proposition 2.1</p>
                            <p>
                                \(|\mathcal{T}_n| = n^n\).
                            </p>
                            <p><strong>Proof:</strong> Each element can map to any of \(n\) values independently, giving \(n \times n \times \cdots \times n = n^n\) total transformations. â–¡</p>
                        </div>

                        <p>
                            For \(n = 3\), we have \(|\mathcal{T}_3| = 27\) transformations. An example:
                        </p>
                        <div class="equation">
                            <div class="equation-content">$$\alpha = \begin{pmatrix} 1 & 2 & 3 \\ 2 & 2 & 1 \end{pmatrix}, \quad \beta = \begin{pmatrix} 1 & 2 & 3 \\ 3 & 1 & 3 \end{pmatrix}$$</div>
                            <div class="equation-number">(2)</div>
                        </div>
                        <p>Composition gives:</p>
                        <div class="equation">
                            <div class="equation-content">$$\alpha \circ \beta = \begin{pmatrix} 1 & 2 & 3 \\ 1 & 2 & 1 \end{pmatrix}$$</div>
                            <div class="equation-number">(3)</div>
                        </div>

                        <h4>2.2 Other important Semigroups, Submonoids and Subsemigroups</h4>
                        
                        <p>We now survey other structures similar to \(\mathcal{T}_n\), each capturing specific properties  (<em>Ganyushkin & Mazorchuk, 2009</em>).</p>

                        <div class="definition">
                            <p class="definition-title">Definition 2.2 [Partial Transformation Monoid]</p>
                            <p>
                                The <strong>partial transformation monoid</strong> \(\mathcal{PT}_n\) consists of partial maps \(\alpha: \text{dom}(\alpha) \to X_n\) where \(\text{dom}(\alpha) \subseteq X_n\).
                            </p>
                        </div>

                        <div class="proposition">
                            <p class="proposition-title">Proposition 2.2</p>
                            <p>
                                \[|\mathcal{PT}_n| = \sum_{k=0}^{n} \binom{n}{k} k^n.\]
                                This counts choices of domain (\(\binom{n}{k}\) ways to select \(k\) elements) and image assignment (\(k^n\) functions on the domain).
                            </p>
                        </div>

                        <div class="definition">
                            <p class="definition-title">Definition 2.3 [Partial One-to-One Transformation Monoid]</p>
                            <p>
                                The <strong>partial one-to-one (injective) transformation monoid</strong> \(\mathcal{I}_n\) consists of partial injective maps.
                            </p>
                        </div>

                        <div class="proposition">
                            <p class="proposition-title">Proposition 2.3</p>
                            <p>
                                \[|\mathcal{I}_n| = \sum_{k=0}^{n} \binom{n}{k} \cdot k! = \sum_{k=0}^{n} \frac{n!}{(n-k)!}.\]
                            </p>
                        </div>

                        <div class="definition">
                            <p class="definition-title">Definition 2.4 [Symmetric Group]</p>
                            <p>
                                The <strong>symmetric group</strong> \(\mathcal{S}_n \subset \mathcal{T}_n\) consists of bijective transformations (permutations).
                            </p>
                        </div>

                        <p>As is well-known, \(|\mathcal{S}_n| = n!\). The symmetric group forms the unit group of \(\mathcal{T}_n\).</p>

                        <div class="definition">
                            <p class="definition-title">Definition 2.5 [Order-Preserving Transformations]</p>
                            <p>
                                Viewing \(X_n\) with the natural order \(1 < 2 < \cdots < n\), the <strong>order-preserving transformation monoid</strong> \(\mathcal{O}_n\) consists of maps \(\alpha\) satisfying:
                                \[i \leq j \implies \alpha(i) \leq \alpha(j).\]
                            </p>
                            <p class="reference-note">
                            <em>Reference: Ganyushkin & Mazorchuk (2009)</em>
                            </p>
                        </div>

                        <div class="proposition">
                            <p class="proposition-title">Proposition 2.4</p>
                            <p>
                                \[|\mathcal{O}_n| = \binom{2n-1}{n}.\]
                                This elegant formula connects to lattice path enumeration: order-preserving maps correspond to nondecreasing sequences, counted by central binomial coefficients.
                            </p>
                        </div>

                        <div class="example">
                            <p class="example-title">Example 2.1</p>
                            <p>
                                For \(n = 3\), \(|\mathcal{O}_3| = \binom{5}{3} = 10\). Examples include:
                            </p>
                            <div class="equation">
                                <div class="equation-content">$$\begin{pmatrix} 1 & 2 & 3 \\ 1 & 1 & 1 \end{pmatrix}, \quad \begin{pmatrix} 1 & 2 & 3 \\ 1 & 2 & 3 \end{pmatrix}, \quad \begin{pmatrix} 1 & 2 & 3 \\ 2 & 3 & 3 \end{pmatrix}$$</div>
                            </div>
                        </div>

                        <div class="definition">
    <p class="definition-title">Definition 2.6 [Order-Reversing Transformations]</p>
    <p>
        The <strong>order-reversing transformation monoid</strong> \(\mathcal{O}^r_n\) consists of maps satisfying:
    </p>
    <div class="equation">
        <div class="equation-content">$$i \leq j \implies \alpha(i) \geq \alpha(j).$$</div>
        <div class="equation-number">(4)</div>
    </div>
    <p>
        The union \(\mathcal{OR}_n = \mathcal{O}_n \cup \mathcal{O}^r_n\) forms the <strong>order-preserving or reversing monoid</strong>.
    </p>
</div>

<div class="definition">
    <p class="definition-title">Definition 2.7 [Order-Decreasing Transformations]</p>
    <p>
        The <strong>order-decreasing transformation monoid</strong> \(\mathcal{OD}_n\)  consists of maps satisfying \(\alpha(i) \leq i\) for all \(i \in X_n\).
    </p>
    <p class="reference-note">
        <em>Reference: Laradji & Umar (2004)</em>
    </p>
</div>

<p>
    These have applications in studying descents in permutations and fixed-point properties.
</p>

<div class="definition">
    <p class="definition-title">Definition 2.8 [Contraction Monoid]</p>
    <p>
        The <strong>contraction monoid</strong> \(\mathcal{C}_n\) consists of maps satisfying:
    </p>
    <div class="equation">
        <div class="equation-content">$$|\alpha(i) - \alpha(j)| \leq |i - j| \text{ for all } i, j \in X_n.$$</div>
        <div class="equation-number">(5)</div>
    </div>
    <p class="reference-note">
        <em>Reference: Reynolds & Sullivan (1985)</em>
    </p>
</div>

<p>
    Contractions are precisely the Lipschitz continuous maps with Lipschitz constant \(L = 1\), connecting transformation monoids to metric geometry.
</p>

                            </div>

                    <!-- Continue with remaining sections... -->
                    <!-- Section 3: Structural Invariants -->
                    <div class="math-content" id="section-invariants">
                        <h3>3. Structural Invariants and Properties</h3>
                        
                        <h4>3.1 Image, Kernel, and Rank</h4>
                        
                        <div class="definition">
                            <p class="definition-title">Definition 3.1 [Image and Rank]</p>
                            <p>
                                For \(\alpha \in \mathcal{T}_n\), the <strong>image</strong> is:
                                \[\text{im}(\alpha) = \{\alpha(x) : x \in X_n\} \subseteq X_n.\]
                                The <strong>rank</strong> of \(\alpha\) is \(r(\alpha) = |\text{im}(\alpha)|\).
                            </p>
                        </div>

                        <div class="definition">
                            <p class="definition-title">Definition 3.2 [Kernel]</p>
                            <p>
                                The <strong>kernel</strong> of \(\alpha\), denoted \(\text{ker}(\alpha)\), is the equivalence relation on \(X_n\) defined by:
                                \[x \equiv_\alpha y \iff \alpha(x) = \alpha(y).\]
                                This partitions \(X_n\) into equivalence classes.
                            </p>
                        </div>

                        <p>Rank and kernel are fundamental invariants determining much of a transformation's structure (<em>Howie, 1995</em>).</p>

                        <div class="example">
                            <p class="example-title">Example 3.1 [Computing Rank and Kernel]</p>
                            <p>
                                Consider \(\alpha = \begin{pmatrix} 1 & 2 & 3 & 4 \\ 2 & 3 & 3 & 2 \end{pmatrix}\).
                            </p>
                            <p>
                                <strong>Image</strong>: \(\text{im}(\alpha) = \{2, 3\}\), so \(r(\alpha) = 2\).
                            </p>
                            <p>
                                <strong>Kernel</strong>: Elements mapping to the same value are equivalent:
                                \[\text{ker}(\alpha) = \{\{1, 4\}, \{2, 3\}\}.\]
                                This is a partition of \(X_4\) into two blocks of sizes 2 and 2.
                            </p>
                        </div>

                        <p>The kernel structure encodes significant information. Two transformations are related by Green's \(\mathcal{R}\)-relation if and only if they have the same kernel (<em>Howie, 1995</em>).</p>

                        <div class="theorem">
                            <p class="theorem-title">Theorem 3.1 [Green's \(\mathcal{D}\)-Classes]</p>
                            <p>
                                Two transformations \(\alpha, \beta \in \mathcal{T}_n\) are \(\mathcal{D}\)-equivalent if and only if \(r(\alpha) = r(\beta)\).
                            </p>
                        </div>

                        <p>This partitions \(\mathcal{T}_n\) into \(n\) \(\mathcal{D}\)-classes \(D_1, D_2, \ldots, D_n\) where \(D_r = \{\alpha : r(\alpha) = r\}\).</p>

                        <h4>3.2 Index and Period</h4>
                        
                        <div class="definition">
                            <p class="definition-title">Definition 3.3 [Index and Period]</p>
                            <p>
                                For \(\alpha \in \mathcal{T}_n\), the <strong>index</strong> \(\text{ind}(\alpha)\) is the smallest non-negative integer \(m\) such that \(\alpha^m = \alpha^{m+p}\) for some positive integer \(p\). The smallest such \(p\) is the <strong>period</strong> \(\text{per}(\alpha)\).
                            </p>
                        </div>

                        <p>These invariants capture the eventual behavior of repeated application: after \(\text{ind}(\alpha)\) iterations, the transformation enters a cycle of length \(\text{per}(\alpha)\).</p>

                        <div class="proposition">
                            <p class="proposition-title">Proposition 3.1</p>
                            <p>
                                For any \(\alpha \in \mathcal{T}_n\), we have \(\text{ind}(\alpha) + \text{per}(\alpha) \leq n\).
                            </p>
                        </div>

                        <h4>3.3 Directed Graph Representation and Path Decomposition</h4>
                        
                        <p>
                            Every transformation \(\alpha \in \mathcal{T}_n\) corresponds to a directed graph \(G_\alpha\) with vertex set \(X_n\) and edge set \(\{(i, \alpha(i)) : i \in X_n\}\). Each vertex has out-degree exactly 1.
                        </p>

                        <p><strong>Structure</strong>: \(G_\alpha\) decomposes into connected components, each containing:</p>
                        <ul>
                            <li>A unique <strong>cycle</strong> (possibly a fixed point, i.e., a loop)</li>
                            <li><strong>Trees</strong> with roots on the cycle, representing elements eventually mapping into the cycle</li>
                        </ul>

                        <div class="example">
                            <p class="example-title">Example 3.2 [Computing Index and Period]</p>
                            <p>
                                Consider \(\alpha = \begin{pmatrix} 1 & 2 & 3 & 4 & 5 \\ 2 & 3 & 2 & 5 & 5 \end{pmatrix}\).
                            </p>
                            <p>
                                <strong>Graph</strong>: Edges are \(1 \to 2 \to 3 \to 2\) (cycle of length 2) and \(4 \to 5 \to 5\) (fixed point).
                            </p>
                            <p><strong>Computation</strong>:</p>
                            <div class="equation">
                                <div class="equation-content">
                                    \begin{align*}
                                    \alpha^1 &= \begin{pmatrix} 1 & 2 & 3 & 4 & 5 \\ 2 & 3 & 2 & 5 & 5 \end{pmatrix}, \\
                                    \alpha^2 &= \begin{pmatrix} 1 & 2 & 3 & 4 & 5 \\ 3 & 2 & 3 & 5 & 5 \end{pmatrix}, \\
                                    \alpha^3 &= \begin{pmatrix} 1 & 2 & 3 & 4 & 5 \\ 2 & 3 & 2 & 5 & 5 \end{pmatrix} = \alpha^1.
                                    \end{align*}
                                </div>
                            </div>
                            <p>
                                Thus \(\text{ind}(\alpha) = 1\) (element 1 enters the cycle after one step, and 4 enters the fixed point after one step) and \(\text{per}(\alpha) = 2\) (the longest cycle has length 2).
                            </p>
                        </div>

                        <p><strong>Algorithm</strong>: To compute \(\text{ind}(\alpha)\) and \(\text{per}(\alpha)\):</p>
                        <ol>
                            <li>Construct \(G_\alpha\) and identify all cycles</li>
                            <li>\(\text{per}(\alpha) = \text{lcm of all cycle lengths}\)</li>
                            <li>\(\text{ind}(\alpha) = \max\{\text{distance from } x \text{ to nearest cycle}\}\)</li>
                        </ol>
                    </div>

                    <!-- Section 4: Conjugacy -->
                    <div class="math-content" id="section-conjugacy">
                        <h3>4. Conjugacy and Classification</h3>
                        
                        <h4>4.1 Conjugacy in \(\mathcal{T}_n\)</h4>
                        
                        <div class="definition">
                            <p class="definition-title">Definition 4.1 [Conjugacy]</p>
                            <p>
                                Two transformations \(\alpha, \beta \in \mathcal{T}_n\) are <strong>conjugate</strong> if there exists \(\gamma \in \mathcal{S}_n\) such that:
                                \[\beta = \gamma^{-1} \alpha \gamma.\]
                            </p>
                        </div>

                        <p>Conjugacy is an equivalence relation partitioning \(\mathcal{T}_n\) into conjugacy classes. In the symmetric group \(\mathcal{S}_n\), conjugacy classes correspond to cycle types. For general transformations in \(\mathcal{T}_n\), the structure is richer.</p>

                        <p><strong>Conjugacy Invariants</strong>:</p>
                        <ul>
                            <li>Rank: \(r(\beta) = r(\alpha)\)</li>
                            <li>Image size and structure</li>
                            <li>Kernel partition type (sizes of equivalence classes)</li>
                            <li>Cycle structure when restricted to the image</li>
                        </ul>

                        <div class="proposition">
                            <p class="proposition-title">Proposition 4.1</p>
                            <p>
                                If \(\alpha\) and \(\beta\) are conjugate in \(\mathcal{T}_n\), then \(r(\alpha) = r(\beta)\) and their kernels have the same partition type.
                            </p>
                        </div>

                        <h4>4.2 Idempotent Elements and Their Classification</h4>
                        
                        <div class="definition">
                            <p class="definition-title">Definition 4.2 [Idempotent]</p>
                            <p>
                                A transformation \(\alpha \in \mathcal{T}_n\) is <strong>idempotent</strong> if \(\alpha^2 = \alpha\).
                            </p>
                        </div>

                        <div class="theorem">
                            <p class="theorem-title">Theorem 4.1 [Characterization of Idempotents]</p>
                            <p>
                                \(\alpha \in \mathcal{T}_n\) is idempotent if and only if \(\alpha\) fixes every element of its image: \(\alpha(y) = y\) for all \(y \in \text{im}(\alpha)\).
                            </p>
                        </div>

                        <div class="proof">
                            <p class="proof-title">Proof Sketch</p>
                            <p>
                                (\(\Rightarrow\)) If \(\alpha^2 = \alpha\), then for any \(y \in \text{im}(\alpha)\), we have \(y = \alpha(x)\) for some \(x\), so \(\alpha(y) = \alpha(\alpha(x)) = \alpha^2(x) = \alpha(x) = y\).
                            </p>
                            <p>
                                (\(\Leftarrow\)) If \(\alpha\) fixes its image, then \(\alpha^2(x) = \alpha(\alpha(x))\). Since \(\alpha(x) \in \text{im}(\alpha)\) and \(\alpha\) fixes its image, \(\alpha(\alpha(x)) = \alpha(x)\). â–¡
                            </p>
                        </div>

                        <div class="proposition">
                            <p class="proposition-title">Proposition 4.2</p>
                            <p>
                                The number of idempotents in \(\mathcal{T}_n\) is:
                                \[\sum_{k=0}^{n} \binom{n}{k} k^{n-k}.\]
                            </p>
                            <p class="reference-note">
                         <em>Reference: Higgins (1992)</em>
                           </p>
                        </div>

                        <p>This counts choosing an image of size \(k\) (\(\binom{n}{k}\) ways) and assigning the remaining \(n-k\) elements to image elements (\(k^{n-k}\) ways), with the image elements fixed.</p>

                        <div class="example">
                            <p class="example-title">Example 4.1 [Idempotent Conjugacy Class]</p>
                            <p>
                                Consider the idempotent \(\epsilon = \begin{pmatrix} 1 & 2 & 3 & 4 \\ 1 & 3 & 3 & 1 \end{pmatrix}\).
                            </p>
                            <p>
                                <strong>Image</strong>: \(\text{im}(\epsilon) = \{1, 3\}\) with \(r(\epsilon) = 2\).
                            </p>
                            <p>
                                <strong>Verification</strong>: \(\epsilon(1) = 1\) and \(\epsilon(3) = 3\), so the image is fixed pointwise.
                            </p>
                            <p>
                                <strong>Kernel</strong>: \(\text{ker}(\epsilon) = \{\{1, 4\}, \{2, 3\}\}\).
                            </p>
                            <p>
                                <strong>Conjugacy Class</strong>: Idempotents of rank 2 are parameterized by:
                            </p>
                            <ul>
                                <li>Choice of 2-element image: \(\binom{4}{2} = 6\) ways</li>
                                <li>Partition of remaining 2 elements into blocks: For each block, assign to an image element. With partition \(\{1, 1\}\), there are \(2^2 = 4\) assignments.</li>
                            </ul>
                            <p>This gives multiple conjugacy classes within rank 2 idempotents, distinguished by kernel structure.</p>
                        </div>

                        <div class="theorem">
                            <p class="theorem-title">Theorem 4.2 [Idempotent Conjugacy Classes]</p>
                            <p>
                                Idempotent conjugacy classes in \(\mathcal{T}_n\) are parameterized by pairs \((r, \lambda)\) where:
                            </p>
                            <ul>
                                <li>\(r\) is the rank (\(1 \leq r \leq n\))</li>
                                <li>\(\lambda\) is a partition of \(n - r\) representing the kernel structure</li>
                            </ul>
                        </div>

                        <h4>4.3 Nilpotent Conjugacy Classes</h4>
                        
                        <div class="definition">
                            <p class="definition-title">Definition 4.3 [Nilpotent]</p>
                            <p>
                                A transformation \(\alpha \in \mathcal{T}_n\) is <strong>nilpotent</strong> if \(\alpha^k\) is a constant map for some \(k \geq 1\).
                            </p>
                        </div>

                        <p>Nilpotent transformations have all elements eventually mapping to a single fixed point. Their directed graphs are trees rooted at this point.</p>

                        <div class="proposition">
                            <p class="proposition-title">Proposition 4.3</p>
                            <p>
                                Nilpotent conjugacy classes in \(\mathcal{T}_n\) correspond to unlabeled rooted trees on \(n\) vertices.
                            </p>
                            <p class="reference-note">
                            <em>Reference: Mora & Kemprasit (2003)</em>
                             </p>
                        </div>

                        <p>The number of such classes equals the number of rooted trees on \(n\) vertices, which by Cayley's formula is \(n^{n-1}\).</p>

                        <div class="example">
                            <p class="example-title">Example 4.2 [Nilpotent Transformation]</p>
                            <p>
                                For \(n = 4\), the nilpotent \(\alpha = \begin{pmatrix} 1 & 2 & 3 & 4 \\ 2 & 2 & 2 & 2 \end{pmatrix}\) has all elements mapping to the constant 2. Its graph is a star with center 2, representing the tree structure.
                            </p>
                        </div>
                    </div>

                    <!-- Section 5: Ideals -->
                    <div class="math-content" id="section-ideals">
                        <h3>5. Ideals and Green's Relations</h3>
                        
                        <h4>5.1 Rank-Based Ideals</h4>
                        
                        <div class="definition">
                            <p class="definition-title">Definition 5.1 [Rank-Based Ideals]</p>
                            <p>
                                For \(1 \leq r \leq n\), define:
                                \[I_r = \{\alpha \in \mathcal{T}_n : r(\alpha) \leq r\}.\]
                            </p>
                        </div>

                        <div class="proposition">
                            <p class="proposition-title">Proposition 5.1</p>
                            <p>
                                Each \(I_r\) is a two-sided ideal of \(\mathcal{T}_n\).
                            </p>
                        </div>

                        <div class="proof">
                            <p class="proof-title">Proof</p>
                            <p>
                                We must show \(\mathcal{T}_n I_r \mathcal{T}_n \subseteq I_r\). If \(r(\alpha) \leq r\), then for any \(\beta, \gamma \in \mathcal{T}_n\):
                                \[r(\beta \alpha \gamma) \leq r(\alpha \gamma) \leq r(\alpha) \leq r,\]
                                since composition cannot increase rank. â–¡
                            </p>
                        </div>

                        <p>These ideals form a chain:</p>
                        <div class="equation">
                            <div class="equation-content">$$I_1 \subseteq I_2 \subseteq \cdots \subseteq I_{n-1} \subseteq I_n = \mathcal{T}_n.$$</div>
                            <div class="equation-number">(6)</div>
                        </div>

                        <div class="theorem">
                            <p class="theorem-title">Theorem 5.1</p>
                            <p>
                                \(I_{n-1}\) is the unique maximal ideal of \(\mathcal{T}_n\).
                            </p>
                            <p class="reference-note">
                            <em>Reference: Howie (1995)</em>
                            </p>
                        </div>

                        <div class="proof">
                            <p class="proof-title">Proof Sketch</p>
                            <p>
                                The complement \(\mathcal{T}_n \setminus I_{n-1} = \{\alpha : r(\alpha) = n\} = \mathcal{S}_n\) is precisely the group of units. Any proper ideal cannot contain units, so must be contained in \(I_{n-1}\). â–¡
                            </p>
                        </div>

                        <h4>5.2 Green's Relations</h4>
                        
                        <p>Green's relations provide a fundamental classification of elements in semigroups (<em>Howie, 1995</em>).</p>

                        <div class="definition">
                            <p class="definition-title">Definition 5.2 [Green's Relations]</p>
                            <p>For \(\alpha, \beta \in \mathcal{T}_n\):</p>
                            <ul>
                                <li>\(\alpha \mathrel{\mathcal{L}} \beta\) if \(\mathcal{T}_n^1 \alpha = \mathcal{T}_n^1 \beta\) (same left ideal)</li>
                                <li>\(\alpha \mathrel{\mathcal{R}} \beta\) if \(\alpha \mathcal{T}_n^1 = \beta \mathcal{T}_n^1\) (same right ideal)</li>
                                <li>\(\alpha \mathrel{\mathcal{J}} \beta\) if \(\mathcal{T}_n^1 \alpha \mathcal{T}_n^1 = \mathcal{T}_n^1 \beta \mathcal{T}_n^1\) (same two-sided ideal)</li>
                                <li>\(\mathcal{D} = \mathcal{L} \vee \mathcal{R}\) (join of \(\mathcal{L}\) and \(\mathcal{R}\))</li>
                            </ul>
                        </div>

                        <div class="theorem">
                            <p class="theorem-title">Theorem 5.2 [Green's Classes in \(\mathcal{T}_n\)]</p>
                            <ol>
                                <li>\(\alpha \mathrel{\mathcal{L}} \beta \iff \text{im}(\alpha) = \text{im}(\beta)\)</li>
                                <li>\(\alpha \mathrel{\mathcal{R}} \beta \iff \text{ker}(\alpha) = \text{ker}(\beta)\)</li>
                                <li>\(\alpha \mathrel{\mathcal{D}} \beta \iff r(\alpha) = r(\beta)\)</li>
                                <li>\(\mathcal{D} = \mathcal{J}\) in \(\mathcal{T}_n\)</li>
                            </ol>
                        </div>

                        <p>These relations partition \(\mathcal{T}_n\) into a hierarchy of equivalence classes, with \(\mathcal{D}\)-classes determined solely by rank.</p>

                       
    <h4>5.3 Additional Structural Invariants</h4>
    
    <div class="definition">
        <p class="definition-title">Definition 5.3 [Breadth/Width]</p>
        <p>
            The <strong>breadth</strong> or <strong>width</strong> of \(\alpha\) is denoted and defined by:
        </p>
        <div class="equation">
            <div class="equation-content">$$b(\alpha) = |\text{Dom}\,\alpha|$$</div>
            <div class="equation-number">(7)</div>
        </div>
        <p>
            This measures the size of the domain of \(\alpha\).
        </p>
    </div>

    <div class="definition">
        <p class="definition-title">Definition 5.4 [Height]</p>
        <p>
            The <strong>height</strong> of \(\alpha\) is denoted and defined by:
        </p>
        <div class="equation">
            <div class="equation-content">$$h(\alpha) = |\text{Im}\,\alpha|$$</div>
            <div class="equation-number">(8)</div>
        </div>
        <p>
            This measures the size of the image (range) of \(\alpha\). Note that \(h(\alpha) = r(\alpha)\), the rank of \(\alpha\).
        </p>
    </div>

    <div class="definition">
        <p class="definition-title">Definition 5.5 [Right and Left Waist]</p>
        <p>
            The <strong>right waist</strong> of \(\alpha\) is denoted and defined by:
        </p>
        <div class="equation">
            <div class="equation-content">$$w^+(\alpha) = \max(\text{Im}\,\alpha)$$</div>
            <div class="equation-number">(9)</div>
        </div>
        <p>
            The <strong>left waist</strong> of \(\alpha\) is denoted and defined by:
        </p>
        <div class="equation">
            <div class="equation-content">$$w^-(\alpha) = \min(\text{Im}\,\alpha)$$</div>
            <div class="equation-number">(10)</div>
        </div>
        <p>
            These measure the maximum and minimum elements in the image of \(\alpha\), respectively.
        </p>
    </div>

    <div class="definition">
        <p class="definition-title">Definition 5.6 [Collapse]</p>
        <p>
            The <strong>collapse</strong> of \(\alpha\) is denoted and defined by:
        </p>
        <div class="equation">
            <div class="equation-content">$$c(\alpha) = \left|\bigcup\{t\alpha^{-1} : t \in \text{Im}\,\alpha \text{ and } |t\alpha^{-1}| \geq 2\}\right|$$</div>
            <div class="equation-number">(11)</div>
        </div>
        <p>
            This counts the number of elements in the domain that map to the same image point (i.e., elements in non-singleton preimages). It measures how much "collapsing" or "merging" occurs under \(\alpha\).
        </p>
    </div>

    <div class="definition">
        <p class="definition-title">Definition 5.7 [Fix]</p>
        <p>
            The <strong>fix</strong> of \(\alpha\) is denoted and defined by:
        </p>
        <div class="equation">
            <div class="equation-content">$$f(\alpha) = |F(\alpha)| = |\{x \in \text{Dom}\,\alpha : x\alpha = x\}|$$</div>
            <div class="equation-number">(12)</div>
        </div>
        <p>
            This counts the number of fixed points of \(\alpha\), i.e., elements that map to themselves.
        </p>
    </div>

    <div class="example">
        <p class="example-title">Example [Computing All Invariants]</p>
        <p>
            Consider the transformation \(\alpha = \begin{pmatrix} 1 & 2 & 3 & 4 & 5 & 6 \\ 2 & 4 & 4 & 4 & 5 & 1 \end{pmatrix}\) on \(X_6 = \{1, 2, 3, 4, 5, 6\}\).
        </p>

        <p><strong>Step 1: Compute Domain and Image</strong></p>
        <ul>
            <li>Domain: \(\text{Dom}\,\alpha = \{1, 2, 3, 4, 5, 6\}\)</li>
            <li>Image: \(\text{Im}\,\alpha = \{1, 2, 4, 5\}\)</li>
        </ul>

        <p><strong>Step 2: Breadth (Width)</strong></p>
        <div class="equation">
            <div class="equation-content">$$b(\alpha) = |\text{Dom}\,\alpha| = |\{1, 2, 3, 4, 5, 6\}| = 6$$</div>
        </div>

        <p><strong>Step 3: Height</strong></p>
        <div class="equation">
            <div class="equation-content">$$h(\alpha) = |\text{Im}\,\alpha| = |\{1, 2, 4, 5\}| = 4$$</div>
        </div>

        <p><strong>Step 4: Right Waist</strong></p>
        <div class="equation">
            <div class="equation-content">$$w^+(\alpha) = \max(\text{Im}\,\alpha) = \max\{1, 2, 4, 5\} = 5$$</div>
        </div>

        <p><strong>Step 5: Left Waist</strong></p>
        <div class="equation">
            <div class="equation-content">$$w^-(\alpha) = \min(\text{Im}\,\alpha) = \min\{1, 2, 4, 5\} = 1$$</div>
        </div>

        <p><strong>Step 6: Collapse</strong></p>
        <p>Identify preimages for each element in the image:</p>
        <ul>
            <li>\(1\alpha^{-1} = \{6\}\) (singleton, \(|1\alpha^{-1}| = 1\))</li>
            <li>\(2\alpha^{-1} = \{1\}\) (singleton, \(|2\alpha^{-1}| = 1\))</li>
            <li>\(4\alpha^{-1} = \{2, 3, 4\}\) (non-singleton, \(|4\alpha^{-1}| = 3 \geq 2\))</li>
            <li>\(5\alpha^{-1} = \{5\}\) (singleton, \(|5\alpha^{-1}| = 1\))</li>
        </ul>
        <p>Only element 4 has a preimage with 2 or more elements. Thus:</p>
        <div class="equation">
            <div class="equation-content">$$c(\alpha) = |4\alpha^{-1}| = |\{2, 3, 4\}| = 3$$</div>
        </div>
        <p>The collapse counts elements 2, 3, and 4 which all "collapse" to the single image point 4.</p>

        <p><strong>Step 7: Fix</strong></p>
        <p>Find fixed points where \(x\alpha = x\):</p>
        <ul>
            <li>\(1\alpha = 2 \neq 1\) (not fixed)</li>
            <li>\(2\alpha = 4 \neq 2\) (not fixed)</li>
            <li>\(3\alpha = 4 \neq 3\) (not fixed)</li>
            <li>\(4\alpha = 4 = 4\) <strong>(fixed!)</strong></li>
            <li>\(5\alpha = 5 = 5\) <strong>(fixed!)</strong></li>
            <li>\(6\alpha = 1 \neq 6\) (not fixed)</li>
        </ul>
        <div class="equation">
            <div class="equation-content">$$f(\alpha) = |F(\alpha)| = |\{4, 5\}| = 2$$</div>
        </div>

        <p><strong>Summary Table:</strong></p>
        <table>
            <thead>
                <tr>
                    <th>Invariant</th>
                    <th>Notation</th>
                    <th>Value</th>
                    <th>Interpretation</th>
                </tr>
            </thead>
            <tbody>
                <tr><td>Breadth</td><td>\(b(\alpha)\)</td><td>6</td><td>Full domain size</td></tr>
                <tr><td>Height</td><td>\(h(\alpha)\)</td><td>4</td><td>Image has 4 elements</td></tr>
                <tr><td>Right Waist</td><td>\(w^+(\alpha)\)</td><td>5</td><td>Largest value in image</td></tr>
                <tr><td>Left Waist</td><td>\(w^-(\alpha)\)</td><td>1</td><td>Smallest value in image</td></tr>
                <tr><td>Collapse</td><td>\(c(\alpha)\)</td><td>3</td><td>3 elements collapse to 4</td></tr>
                <tr><td>Fix</td><td>\(f(\alpha)\)</td><td>2</td><td>Elements 4 and 5 are fixed</td></tr>
            </tbody>
        </table>
    </div>

    <div class="example">
        <p class="example-title">Example [Partial Transformation]</p>
        <p>
            Consider the partial transformation \(\beta\) on \(X_5 = \{1, 2, 3, 4, 5\}\) with:
            \[\beta = \begin{pmatrix} 1 & 2 & 3 & 4 & 5 \\ 3 & \text{â€”} & 3 & 1 & 1 \end{pmatrix}\]
            where "â€”" indicates 2 is not in the domain.
        </p>

        <p><strong>Computation:</strong></p>
        <ul>
            <li><strong>Domain:</strong> \(\text{Dom}\,\beta = \{1, 3, 4, 5\}\)</li>
            <li><strong>Image:</strong> \(\text{Im}\,\beta = \{1, 3\}\)</li>
            <li><strong>Breadth:</strong> \(b(\beta) = 4\)</li>
            <li><strong>Height:</strong> \(h(\beta) = 2\)</li>
            <li><strong>Right Waist:</strong> \(w^+(\beta) = 3\)</li>
            <li><strong>Left Waist:</strong> \(w^-(\beta) = 1\)</li>
            <li><strong>Preimages:</strong>
                <ul>
                    <li>\(1\beta^{-1} = \{4, 5\}\) (size 2)</li>
                    <li>\(3\beta^{-1} = \{1, 3\}\) (size 2)</li>
                </ul>
            </li>
            <li><strong>Collapse:</strong> \(c(\beta) = |\{4, 5\} \cup \{1, 3\}| = |\{1, 3, 4, 5\}| = 4\)</li>
            <li><strong>Fixed Points:</strong> None (1 maps to 3, 3 maps to 3... wait, 3 IS fixed!)</li>
            <li><strong>Fix:</strong> \(f(\beta) = |\{3\}| = 1\)</li>
        </ul>
    </div>

    <div class="remark">
        <p class="remark-title">Remark [Relationships Between Invariants]</p>
        <ul>
            <li>For full transformations: \(b(\alpha) = n\) always</li>
            <li>Height equals rank: \(h(\alpha) = r(\alpha)\)</li>
            <li>Always: \(c(\alpha) \leq b(\alpha)\) and \(f(\alpha) \leq b(\alpha)\)</li>
            <li>If \(\alpha\) is injective: \(c(\alpha) = 0\) (no collapse)</li>
            <li>If \(\alpha\) is the identity: \(f(\alpha) = b(\alpha) = n\)</li>
            <li>For idempotents: \(f(\alpha) = h(\alpha)\) (all image elements are fixed)</li>
        </ul>
    </div>
</div>

                    <!-- Section 6: Conclusion -->
                    <div class="math-content" id="section-conclusion">
                        <h3>6. Conclusion</h3>
                        
                        <h4>6.1 Summary</h4>
                        
                        <p>We have explored the rich algebraic landscape of transformation monoids, revealing deep connections between combinatorial properties and algebraic structure. Key insights include:</p>

                        <ul>
                            <li><strong>Universality</strong>: Every finite semigroup embeds in some \(\mathcal{T}_n\), making these monoids fundamental</li>
                            <li><strong>Structural Invariants</strong>: Rank, image, kernel, index, and period provide complete characterizations of transformation behavior</li>
                            <li><strong>Conjugacy</strong>: Classification of idempotent and nilpotent conjugacy classes connects to partition theory and tree enumeration</li>
                            <li><strong>Ideals</strong>: The rank-based ideal structure provides a natural hierarchy, with \(I_{n-1}\) as the unique maximal ideal</li>
                            <li><strong>Green's Relations</strong>: These partition \(\mathcal{T}_n\) into classes determined by image structure (\(\mathcal{L}\)), kernel structure (\(\mathcal{R}\)), and rank (\(\mathcal{D} = \mathcal{J}\))</li>
                        </ul>

                        <p>The interplay between geometric intuition (directed graphs, trees, cycles) and algebraic formalism (ideals, conjugacy, monoid structure) makes transformation monoids both accessible and profound.</p>

                        <h4>6.2 Advanced Topics and Further Directions</h4>
                        
                        <p>Contemporary research explores several active areas:</p>

                        <p><strong>Computational Complexity</strong>: Decision problems on transformation monoids (membership testing, ideal generation, conjugacy determination) have varying complexity. Understanding these boundaries connects semigroup theory to theoretical computer science (<em>East, 2011</em>).</p>

                        <p><strong>Representations</strong>: Linear representations of transformation monoids over fields provide connections to representation theory of algebras and quivers.</p>

                        <p><strong>Automata Theory</strong>: The syntactic monoid of a regular language is a quotient of a transformation monoid, linking formal language theory to algebraic structures (<em>Ganyushkin & Mazorchuk, 2009</em>).</p>

                        <p><strong>Asymptotic Properties</strong>: For large \(n\), statistical properties of random transformations (distribution of ranks, cycle lengths, index/period) exhibit interesting limiting behavior (<em>Gray & Ruskuc, 2009</em>).</p>

                        <p><strong>Generalizations</strong>: Infinite transformation monoids, transformations on partial orders beyond linear orders, and transformations preserving other structures (graphs, metrics) extend the theory in diverse directions.</p>

                        <p><strong>Specific Submonoids</strong>: Recent work characterizes nilpotent-generated subsemigroups, minimal generating sets for specific transformation monoids, and counting conjugacy classes in various restricted settings (<em>Garba, 1994</em>).</p>

                        <p>The field remains vibrant, with transformation monoids serving as a testing ground for general semigroup theory while maintaining rich connections to combinatorics, computer science, and beyond.</p>
                    </div>

                    <!-- Appendix -->
                    <div class="math-content" id="section-appendix">
                        <h3>Appendix: Worked Examples</h3>
                        
                        <h4>Example A: Complete Structural Analysis</h4>
                        
                        <p><strong>Problem</strong>: For \(\alpha = \begin{pmatrix} 1 & 2 & 3 & 4 & 5 & 6 \\ 3 & 1 & 3 & 5 & 5 & 4 \end{pmatrix}\), compute all structural invariants.</p>

                        <p><strong>Solution</strong>:</p>

                        <p><strong>Step 1: Image and Rank</strong></p>
                        <p>\(\text{im}(\alpha) = \{1, 3, 4, 5\}\), \(r(\alpha) = 4\).</p>

                        <p><strong>Step 2: Kernel</strong></p>
                        <p>Equivalence classes:</p>
                        <ul>
                            <li>Elements mapping to 1: \(\{2\}\)</li>
                            <li>Elements mapping to 3: \(\{1, 3\}\)</li>
                            <li>Elements mapping to 4: \(\{6\}\)</li>
                            <li>Elements mapping to 5: \(\{4, 5\}\)</li>
                        </ul>
                        <p>\(\text{ker}(\alpha) = \{\{2\}, \{1, 3\}, \{6\}, \{4, 5\}\}\).</p>
                        <p>This is a partition with type \((1, 1, 2, 2)\) (two singleton blocks, two blocks of size 2).</p>

                        <p><strong>Step 3: Directed Graph</strong></p>
                        <p>Edges: \(1 \to 3 \to 3\) (3 is a fixed point), \(2 \to 1 \to 3\), \(4 \to 5 \to 5\) (5 is a fixed point), \(6 \to 4 \to 5\).</p>
                        <p>Components:</p>
                        <ul>
                            <li>Component 1: Tree rooted at fixed point 3, containing path \(2 \to 1 \to 3\) and loop \(3 \to 3\)</li>
                            <li>Component 2: Tree rooted at fixed point 5, containing path \(6 \to 4 \to 5\) and loop \(5 \to 5\)</li>
                        </ul>

                        <p><strong>Step 4: Index and Period</strong></p>
                        <p>Compute powers:</p>
                        <div class="equation">
                            <div class="equation-content">
                                \begin{align*}
                                \alpha^2 &= \begin{pmatrix} 1 & 2 & 3 & 4 & 5 & 6 \\ 3 & 3 & 3 & 5 & 5 & 5 \end{pmatrix}, \\
                                \alpha^3 &= \begin{pmatrix} 1 & 2 & 3 & 4 & 5 & 6 \\ 3 & 3 & 3 & 5 & 5 & 5 \end{pmatrix} = \alpha^2.
                                \end{align*}
                            </div>
                        </div>
                        <p>Thus \(\text{ind}(\alpha) = 2\) (after 2 iterations, behavior stabilizes) and \(\text{per}(\alpha) = 1\) (all cycles have length 1, i.e., fixed points).</p>

                        <p><strong>Step 5: Green's Classes</strong></p>
                        <p>Since \(r(\alpha) = 4\), we have \(\alpha \in D_4\) (the \(\mathcal{D}\)-class of rank 4 transformations).</p>
                        <p>Within \(D_4\):</p>
                        <ul>
                            <li>\(\mathcal{L}\)-class determined by image \(\{1, 3, 4, 5\}\)</li>
                            <li>\(\mathcal{R}\)-class determined by kernel partition type \((1, 1, 2, 2)\)</li>
                        </ul>

                        <h4>Example B: Idempotent Conjugacy Class</h4>
                        
                        <p><strong>Problem</strong>: Classify the conjugacy class of \(\epsilon = \begin{pmatrix} 1 & 2 & 3 & 4 & 5 \\ 2 & 2 & 3 & 3 & 5 \end{pmatrix}\).</p>

                        <p><strong>Solution</strong>:</p>

                        <p><strong>Step 1: Verify Idempotent</strong></p>
                        <p>Check \(\epsilon^2 = \epsilon\): \(\epsilon^2 = \begin{pmatrix} 1 & 2 & 3 & 4 & 5 \\ 2 & 2 & 3 & 3 & 5 \end{pmatrix}\). Yes, \(\epsilon\) is idempotent.</p>

                        <p><strong>Step 2: Image</strong></p>
                        <p>\(\text{im}(\epsilon) = \{2, 3, 5\}\) with \(r(\epsilon) = 3\).</p>
                        <p>Verify fixed: \(\epsilon(2) = 2\), \(\epsilon(3) = 3\), \(\epsilon(5) = 5\). âœ“</p>

                        <p><strong>Step 3: Kernel Structure</strong></p>
                        <p>Partition of non-image elements \(\{1, 4\}\):</p>
                        <ul>
                            <li>1 maps to 2</li>
                            <li>4 maps to 3</li>
                        </ul>
                        <p>Kernel: \(\{\{1, 2\}, \{3, 4\}, \{5\}\}\) with type \((2, 2, 1)\).</p>

                        <p><strong>Step 4: Conjugacy Class Parameters</strong></p>
                        <p>The conjugacy class is characterized by:</p>
                        <ul>
                            <li>Rank: \(r = 3\)</li>
                            <li>Non-image partition: \(5 - 3 = 2\) elements partitioned into 2 singletons (trivial partition)</li>
                        </ul>

                        <p><strong>Step 5: Count Elements in Class</strong></p>
                        <p>Number of conjugates:</p>
                        <ul>
                            <li>Choose 3-element image from 5 elements: \(\binom{5}{3} = 10\)</li>
                            <li>Assign 2 non-image elements to image elements: Each of 2 elements assigned independently to one of 3 image elements: \(3^2 = 9\)</li>
                        </ul>
                        <p>Total: \(10 \times 9 = 90\) idempotents in this conjugacy class.</p>

                        <h4>Example C: Ideal Membership</h4>
                        
                        <p><strong>Problem</strong>: Show that \(\alpha = \begin{pmatrix} 1 & 2 & 3 & 4 \\ 2 & 2 & 1 & 1 \end{pmatrix}\) belongs to \(I_2\) but not \(I_1\).</p>

                        <p><strong>Solution</strong>:</p>

                        <p><strong>Step 1: Compute Rank</strong></p>
                        <p>\(\text{im}(\alpha) = \{1, 2\}\), so \(r(\alpha) = 2\).</p>

                        <p><strong>Step 2: Membership in \(I_2\)</strong></p>
                        <p>By definition, \(I_2 = \{\beta \in \mathcal{T}_4 : r(\beta) \leq 2\}\). Since \(r(\alpha) = 2\), we have \(\alpha \in I_2\).</p>

                        <p><strong>Step 3: Non-membership in \(I_1\)</strong></p>
                        <p>\(I_1 = \{\beta \in \mathcal{T}_4 : r(\beta) \leq 1\}\) consists of constant maps. Since \(r(\alpha) = 2 > 1\), we have \(\alpha \notin I_1\).</p>

                        <p><strong>Step 4: Verify Ideal Property</strong></p>
                        <p>To confirm \(I_2\) is an ideal, check closure under left and right multiplication. For any \(\beta \in \mathcal{T}_4\):</p>
                        <ul>
                            <li>\(r(\beta \alpha) \leq r(\alpha) = 2\) (composition can't increase rank)</li>
                            <li>\(r(\alpha \beta) \leq \min(r(\alpha), r(\beta)) \leq 2\)</li>
                        </ul>
                        <p>Thus \(\mathcal{T}_4 \alpha \mathcal{T}_4 \subseteq I_2\), confirming \(I_2\) is a two-sided ideal.</p>

                        <h4>Further Exercises</h4>
                        
                        <p><strong>Exercise 1</strong>: Prove that the intersection \(\mathcal{O}_n \cap \mathcal{S}_n\) (order-preserving permutations) contains only the identity transformation.</p>

                        <p><strong>Exercise 2</strong>: For the transformation \(\beta = \begin{pmatrix} 1 & 2 & 3 \\ 2 & 3 & 1 \end{pmatrix}\), compute \(\text{ind}(\beta)\) and \(\text{per}(\beta)\). Show that \(\beta \in \mathcal{S}_3\) and relate the period to the cycle structure.</p>

                        <p><strong>Exercise 3</strong>: Determine all idempotents in \(\mathcal{T}_3\) and classify them by rank and kernel structure.</p>

                        <p><strong>Exercise 4</strong>: Show that \(|\mathcal{O}_3| = \binom{5}{3} = 10\) by explicitly listing all order-preserving transformations on \(X_3 = \{1, 2, 3\}\).</p>

                        <p><strong>Exercise 5</strong>: Prove that every nilpotent transformation in \(\mathcal{T}_n\) has index at most \(n - 1\).</p>

                        <p><strong>Exercise 6</strong>: For the contraction monoid \(\mathcal{C}_n\), show that the identity and all constant maps are contractions. Are all permutations contractions?</p>

                        <p><strong>Exercise 7</strong>: Find the left and right waist of \(\alpha = \begin{pmatrix} 1 & 2 & 3 & 4 \\ 3 & 3 & 2 & 2 \end{pmatrix}\).</p>

                        <p><strong>Exercise 8</strong>: Prove that the number of \(\mathcal{D}\)-classes in \(\mathcal{T}_n\) equals \(n\).</p>

                        <h4>Visual Representations</h4>
                        <div class="figure-container">
                        <img src="transformation_monoids_figures.png" alt="Monoids" class="figure-image" style="width: 80%; max-width: 800px; max-height: 1000px;">
                        <div class="figure-caption">
                        <strong>Figure 1:</strong> Directed graph (Digraph), Ideal Lattice and Transformation Representation.
                         </div>
                        </div>
                        <p><strong>(A) (Directed Graph)</strong>: For \(\alpha = \begin{pmatrix} 1 & 2 & 3 & 4 & 5 \\ 2 & 3 & 3 & 5 & 4 \end{pmatrix}\), the directed graph has:</p>
                        <ul>
                            <li>Edges: \(1 \to 2 \to 3 \to 3\) (cycle of length 1 at 3, with incoming tree)</li>
                            <li>Edges: \(4 \to 5 \to 4\) (cycle of length 2)</li>
                        </ul>
                        <p>This illustrates the tree-cycle decomposition: element 1 is in a tree leading to the fixed point 3, while elements 4 and 5 form a 2-cycle.</p>

                        <p><strong>(B) (Ideal Lattice)</strong>: The Hasse diagram for ideals in \(\mathcal{T}_4\) by rank shows the chain \(I_1 \subseteq I_2 \subseteq I_3 \subseteq I_4 = \mathcal{T}_4\). This chain shows the unique maximal ideal \(I_3\) containing all non-bijective transformations.</p>

                        <p><strong>(C) (Transformation Representation)</strong>: The transformation \(\alpha = \begin{pmatrix} 1 & 2 & 3 & 4 \\ 2 & 2 & 3 & 1 \end{pmatrix}\) with \(\text{im}(\alpha) = \{1, 2, 3\}\) (highlighted) and \(\text{ker}(\alpha) = \{\{1, 2\}, \{3\}, \{4\}\}\) showing elements 1 and 2 in the same kernel class (both map to 2).</p>
                    </div>

                    <!-- References -->
                    <div class="math-content" id="section-references">
                        <h3>References</h3>
                        
                        <div class="reference-list">
                            <div class="reference-item">
                                <span class="reference-citation">[1]</span>
                                <div class="reference-content">
                                    <p>A.H. Clifford and G.B. Preston, <em>The Algebraic Theory of Semigroups, Volume I</em>, Mathematical Surveys <strong>7</strong>, American Mathematical Society, Providence, 1961.</p>
                                </div>
                            </div>

                            <div class="reference-item">
                                <span class="reference-citation">[2]</span>
                                <div class="reference-content">
                                    <p>A.H. Clifford and G.B. Preston, <em>The Algebraic Theory of Semigroups, Volume II</em>, Mathematical Surveys <strong>7</strong>, American Mathematical Society, Providence, 1967.</p>
                                </div>
                            </div>

                            <div class="reference-item">
                                <span class="reference-citation">[3]</span>
                                <div class="reference-content">
                                    <p>J. East, <em>Generators and relations for partition monoids and algebras</em>, J. Algebra <strong>339</strong> (2011), 1â€“26.</p>
                                </div>
                            </div>

                            <div class="reference-item">
                                <span class="reference-citation">[4]</span>
                                <div class="reference-content">
                                    <p>G.U. Garba, <em>On the idempotent ranks of certain semigroups of order-preserving transformations</em>, Port. Math. <strong>51</strong> (1994), 185â€“204.</p>
                                </div>
                            </div>

                            <div class="reference-item">
                                <span class="reference-citation">[5]</span>
                                <div class="reference-content">
                                    <p>O. Ganyushkin and V. Mazorchuk, <em>Classical Finite Transformation Semigroups: An Introduction</em>, Algebra and Applications <strong>9</strong>, Springer, London, 2009.</p>
                                </div>
                            </div>

                            <div class="reference-item">
                                <span class="reference-citation">[6]</span>
                                <div class="reference-content">
                                    <p>R. Gray and N. RuÅ¡kuc, <em>Generating sets of completely 0-simple semigroups</em>, Comm. Algebra <strong>33</strong> (2005), 4657-4678.</p>
                                </div>
                            </div>

                            <div class="reference-item">
                                <span class="reference-citation">[7]</span>
                                <div class="reference-content">
                                    <p>P.M. Higgins, <em>Techniques of Semigroup Theory</em>, Oxford University Press, Oxford, 1992.</p>
                                </div>
                            </div>

                            <div class="reference-item">
                                <span class="reference-citation">[8]</span>
                                <div class="reference-content">
                                    <p>J.M. Howie, <em>Fundamentals of Semigroup Theory</em>, London Mathematical Society Monographs, Oxford University Press, Oxford, 1995.</p>
                                </div>
                            </div>

                            <div class="reference-item">
                                <span class="reference-citation">[9]</span>
                                <div class="reference-content">
                                    <p>A. Laradji and A. Umar, <em>Combinatorial results for semigroups of order-preserving partial transformations</em>, J. Algebra <strong>278</strong> (2004), 342â€“359.</p>
                                </div>
                            </div>

                            <div class="reference-item">
                                <span class="reference-citation">[10]</span>
                                <div class="reference-content">
                                    <p>W. Mora and Y. Kemprasit, <em>Regular elements of some order-preserving transformation semigroups.</em>,  Int J Algebra,<strong>4</strong> (2010), 631-641.</p>
                                </div>
                            </div>

                            <div class="reference-item">
                                <span class="reference-citation">[11]</span>
                                <div class="reference-content">
                                    <p>M.A. Reynolds and R.P. Sullivan, <em>Products of idempotent linear transformations</em>, Proc. Roy. Soc. Edinburgh Sect. A <strong>100</strong> (1985), 123â€“138.</p>
                                </div>
                            </div>

                            <div class="reference-item">
                                <span class="reference-citation">[12]</span>
                                <div class="reference-content">
                                    <p>A. Umar, <em>On the semigroups of order-decreasing finite full transformations</em>, Proc. Roy. Soc. Edinburgh Sect. A <strong>120</strong> (1992), 129-142. .</p>
                                </div>
                            </div>

                            <div class="reference-item">
                                <span class="reference-citation">[13]</span>
                                <div class="reference-content">
                                    <p>J. East, <em>Presentations for singular subsemigroups of the partial transformation semigroup</em>, Internat. J. Algebra Comput. <strong>20</strong> (2010), 1â€“25.</p>
                                </div>
                            </div>

                            <div class="reference-item">
                                <span class="reference-citation">[14]</span>
                                <div class="reference-content">
                                    <p>V.H. Fernandes, G.M. Gomes, and M.M. Jesus, <em>Presentations for some monoids of partial transformations on a finite chain</em>, Comm. Algebra <strong>33</strong> (2005), 587â€“604.</p>
                                </div>
                            </div>
                        </div>
                    </div>

                    <!-- Additional Resources -->
                    <div class="math-content">
                        <h3>Additional Resources</h3>
                        
                        <h4>Glossary of Key Terms</h4>
                        
                        <dl class="glossary-list">
                            <dt>Contraction Monoid</dt>
                            <dd>The monoid \(\mathcal{C}_n\) of Lipschitz maps with constant 1, satisfying \(|\alpha(i) - \alpha(j)| \leq |i - j|\).</dd>
                            
                            <dt>Green's Relations</dt>
                            <dd>Equivalence relations \(\mathcal{L}\), \(\mathcal{R}\), \(\mathcal{J}\), and \(\mathcal{D}\) that classify elements of semigroups by their ideal-theoretic properties.</dd>
                            
                            <dt>Idempotent</dt>
                            <dd>An element \(\alpha\) satisfying \(\alpha^2 = \alpha\).</dd>
                            
                            <dt>Image</dt>
                            <dd>For transformation \(\alpha\), the set \(\text{im}(\alpha) = \{\alpha(x) : x \in X_n\}\).</dd>
                            
                            <dt>Index</dt>
                            <dd>The smallest \(m\) such that \(\alpha^m = \alpha^{m+p}\) for some \(p > 0\).</dd>
                            
                            <dt>Kernel</dt>
                            <dd>The equivalence relation \(\text{ker}(\alpha)\) where \(x \equiv y\) if \(\alpha(x) = \alpha(y)\).</dd>
                            
                            <dt>Nilpotent</dt>
                            <dd>A transformation whose some power is a constant map.</dd>
                            
                            <dt>Order-Preserving</dt>
                            <dd>A transformation satisfying \(i \leq j \implies \alpha(i) \leq \alpha(j)\).</dd>
                            
                            <dt>Period</dt>
                            <dd>The smallest \(p > 0\) such that \(\alpha^{\text{ind}(\alpha)} = \alpha^{\text{ind}(\alpha) + p}\).</dd>
                            
                            <dt>Rank</dt>
                            <dd>The cardinality of the image: \(r(\alpha) = |\text{im}(\alpha)|\).</dd>
                            
                            <dt>Symmetric Group</dt>
                            <dd>The group \(\mathcal{S}_n\) of bijective transformations (permutations).</dd>
                            
                            <dt>Transformation Monoid</dt>
                            <dd>The set \(\mathcal{T}_n\) of all self-maps on \(X_n = \{1, \ldots, n\}\) with composition.</dd>
                        </dl>

                        <h4>Notation Summary</h4>
                        
                        <table>
                            <caption>Other Notation Reference</caption>
                            <thead>
                                <tr>
                                    <th>Notation</th>
                                    <th>Description</th>
                                    <th>Example/Notes</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr><td>\(X_n\)</td><td>Finite set \(\{1, 2, \ldots, n\}\)</td><td>Domain and codomain</td></tr>
                                <tr><td>\(\mathcal{T}_n\)</td><td>Full transformation monoid</td><td>\(|\mathcal{T}_n| = n^n\)</td></tr>
                                <tr><td>\(\mathcal{S}_n\)</td><td>Symmetric group</td><td>\(|\mathcal{S}_n| = n!\)</td></tr>
                                <tr><td>\(\mathcal{O}_n\)</td><td>Order-preserving transformations</td><td>\(|\mathcal{O}_n| = \binom{2n-1}{n}\)</td></tr>
                                <tr><td>\(r(\alpha)\)</td><td>Rank of \(\alpha\)</td><td>Size of image</td></tr>
                                <tr><td>\(\text{im}(\alpha)\)</td><td>Image of \(\alpha\)</td><td>Range of the function</td></tr>
                                <tr><td>\(\text{ker}(\alpha)\)</td><td>Kernel of \(\alpha\)</td><td>Partition of domain</td></tr>
                                <tr><td>\(I_r\)</td><td>Rank-based ideal</td><td>\(\{\alpha : r(\alpha) \leq r\}\)</td></tr>
                                <tr><td>\(\mathcal{L}, \mathcal{R}, \mathcal{D}, \mathcal{J}\)</td><td>Green's relations</td><td>Classification relations</td></tr>
                            </tbody>
                        </table>

                        <h4>Key Theorems at a Glance</h4>
                        
                        <table>
                            <caption>Summary of Major Results</caption>
                            <thead>
                                <tr>
                                    <th>Theorem</th>
                                    <th>Statement</th>
                                    <th>Significance</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Cardinality</td>
                                    <td>\(|\mathcal{T}_n| = n^n\)</td>
                                    <td>Basic counting</td>
                                </tr>
                                <tr>
                                    <td>Idempotent Characterization</td>
                                    <td>\(\alpha^2 = \alpha \iff \alpha\) fixes its image</td>
                                    <td>Structural property</td>
                                </tr>
                                <tr>
                                    <td>Green's \(\mathcal{D}\)-Classes</td>
                                    <td>\(\alpha \mathrel{\mathcal{D}} \beta \iff r(\alpha) = r(\beta)\)</td>
                                    <td>Classification by rank</td>
                                </tr>
                                <tr>
                                    <td>Maximal Ideal</td>
                                    <td>\(I_{n-1}\) is unique maximal ideal</td>
                                    <td>Ideal structure</td>
                                </tr>
                                <tr>
                                    <td>Nilpotent Classes</td>
                                    <td>Correspond to rooted trees</td>
                                    <td>Combinatorial connection</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <!-- Footer -->
                    <div class="math-content">
                        <h3>About This Document</h3>
                        <p>
                            This exposition was prepared as course material for students in semigroup theory and algebraic combinatorics. It emphasizes computational techniques and explicit examples alongside rigorous theoretical development. For additional exercises, proofs, and applications, consult the references listed above, particularly the comprehensive texts by Howie (1995) and Ganyushkin & Mazorchuk (2009).
                        </p>
                        <p>
                            <em>Last Updated: 2025</em>
                        </p>
                    </div>

                </div>
            </div>
        </div>
              </div>

              <div class="main-container">   
<div class="project-card">
    <div class="project-header" onclick="toggleContent('fixed-points-project')">
        <div class="project-meta">
            <span class="meta-item">
                <i class="fas fa-calendar-alt"></i>
                Added: July 25, 2025
            </span>
            <span class="meta-item">
                <i class="fas fa-clock"></i>
                1hr 55 min read
            </span>
            <span class="meta-item">
                <i class="fas fa-tag"></i>
                Functional Analysis
            </span>
        </div>
        <h2 class="project-title">Finding Common Fixed Points in Normed Linear Spaces: A Journey Through Iterative Schemes</h2>
        <p class="project-subtitle">Author: Clinton Oluranran Kayoh</p>
        <div class="expand-btn" id="btn-fixed-points-project">
            <i class="fas fa-plus"></i>
        </div>
    </div>
    <div class="project-content" id="content-fixed-points-project">
        <div class="content-inner">
            <!-- Abstract -->
            <div class="math-content">
                <h3>Abstract</h3>
                <p>
                    This exposition explores the theory and practice of finding common fixed points for families of mappings in normed linear spaces, with particular emphasis on Banach and Hilbert spaces. We systematically develop classical iteration schemes including Picard, Mann, Ishikawa, Noor, and S-iteration, analyzing their convergence properties and practical implementation. Each method is illustrated with concrete examples and visualizations that illuminate the geometric intuition behind convergence. We progress from the foundational contraction mapping principle through sophisticated multi-step iterations, emphasizing the interplay between geometric properties of spaces, operator conditions, and iteration design. Applications to equilibrium problems, optimization, and operator equations demonstrate the practical relevance of these theoretical tools. This treatment assumes familiarity with basic real analysis and linear algebra, introducing functional analytic concepts as needed for accessibility to graduate students in applied mathematics, computational science, and engineering.
                </p>
            </div>

            <!-- Table of Contents -->
            <div class="toc-container">
                <h3>Table of Contents</h3>
                <ul class="toc-list">
                    <li><a href="#section-introduction">1. Introduction: The Quest for Common Fixed Points</a></li>
                    <li><a href="#section-foundations">2. Foundational Theory: When Do Fixed Points Exist?</a></li>
                    <li><a href="#section-classical">3. Classical Iteration Schemes</a></li>
                    <li><a href="#section-hilbert">4. Convergence in Hilbert Spaces</a></li>
                    <li><a href="#section-fixed_point">5. Finding Common Fixed Points: Advanced Schemes</a></li>
                    <li><a href="#section-applications">6. Applications and Examples</a></li>
                    <li><a href="#section-comparison">7. Comparison and Parameter Selection</a></li>
                    <li><a href="#section-modern_extension">8. Modern Extensions and Open problems</a></li>
                    <li><a href="#section-conclusion">9. Conclusion</a></li>
                </ul>
            </div>

            <!-- Introduction -->
            <div class="math-content" id="section-introduction">
                <h3>1. Introduction: The Quest for Common Fixed Points</h3>
                
                <h4>1.1 Motivation and the Fixed Point Problem</h4>
                <p>
                    The search for fixed points (points that remain invariant under a given transformation) represents one of the most fundamental problems in mathematics. When a function \(T: X \to X\) satisfies \(T(x^*) = x^*\), we call \(x^*\) a <em>fixed point</em> of \(T\). This deceptively simple concept underlies solutions to differential equations, equilibrium states in dynamical systems, Nash equilibria in game theory, and optimal solutions in convex optimization.
                </p>
                <p>
                    But nature rarely presents us with a single transformation. More often, we encounter <em>families</em> of operators \(\{T_i\}_{i=1}^N\) and seek points that are simultaneously fixed by all members: points \(x^*\) satisfying \(T_i(x^*) = x^*\) for all \(i\). This is the <strong>common fixed point problem</strong>, and it emerges in:
                </p>
                <ul>
                    <li><strong>Convex feasibility problems</strong>: Finding points in the intersection of convex sets \(C_1 \cap C_2 \cap \cdots \cap C_N\) by iterating projection operators</li>
                    <li><strong>Nash equilibria</strong>: Multi-player games where each player's best-response operator must share a fixed point</li>
                    <li><strong>Image reconstruction</strong>: Medical imaging (CT, MRI) combines multiple constraint operators</li>
                    <li><strong>Variational inequalities</strong>: Traffic equilibrium and economic models</li>
                </ul>
                <p>
                    The challenge is profound: even if each operator has fixed points individually, there's no guarantee a common fixed point exists. And even when existence is assured, how do we <em>find</em> it computationally?
                </p>

                <h4>1.2 The Iterative Philosophy</h4>
                <p>
                    Our strategy is iterative: starting from an initial guess \(x_0\), we generate a sequence \(\{x_n\}\) that (hopefully) converges to a common fixed point. The art lies in designing the iteration scheme. Should we apply operators sequentially or in parallel? Should we use convex combinations? Do we need memory of previous iterates?
                </p>
                <p>
                    This document explores a hierarchy of iteration schemes, each building on its predecessors:
                </p>
                <ol>
                    <li><strong>Picard iteration</strong> (1890): The simplest scheme, \(x_{n+1} = Tx_n\)</li>
                    <li><strong>Mann iteration</strong> (1953): Convex averaging, \(x_{n+1} = (1-\alpha_n)x_n + \alpha_n Tx_n\)</li>
                    <li><strong>Ishikawa iteration</strong> (1974): Two-step process with double averaging</li>
                    <li><strong>Noor iteration</strong> (1982): Three-step generalization</li>
                    <li><strong>S-iteration</strong> (2007): Simplified two-step scheme</li>
                    <li><strong>Modern variants</strong>: Adaptive and accelerated schemes</li>
                </ol>
                <p>
                    Each scheme enlarges the class of operators for which convergence can be guaranteed, trading simplicity for generality.
                </p>

                <h4>1.3 The Spaces We Work In</h4>
                <p>
                    We operate primarily in three nested categories of spaces, each with richer structure:
                </p>

                <div class="definition">
                    <p class="definition-title">Definition [Normed Linear Space]</p>
                    <p>
                        A <strong>normed linear space</strong> \((X, \|\cdot\|)\) is a vector space \(X\) over \(\mathbb{R}\) or \(\mathbb{C}\) equipped with a norm \(\|\cdot\|: X \to [0, \infty)\) satisfying:
                    </p>
                    <ol>
                        <li>\(\|x\| = 0 \iff x = 0\)</li>
                        <li>\(\|\alpha x\| = |\alpha| \|x\|\) for all scalars \(\alpha\)</li>
                        <li>\(\|x + y\| \leq \|x\| + \|y\|\) (triangle inequality)</li>
                    </ol>
                </div>

                <div class="definition">
                    <p class="definition-title">Definition [Banach Space]</p>
                    <p>
                        A <strong>Banach space</strong> is a normed linear space that is complete with respect to the metric induced by its norm: every Cauchy sequence converges.
                    </p>
                </div>

                <div class="definition">
                    <p class="definition-title">Definition [Hilbert Space]</p>
                    <p>
                        A <strong>Hilbert space</strong> \(H\) is a complete inner product space, with inner product \(\langle \cdot, \cdot \rangle\) inducing the norm \(\|x\| = \sqrt{\langle x, x \rangle}\).
                    </p>
                </div>

                <p>
                    Completeness (the Banach property) is crucial: it ensures that convergent-looking sequences actually converge to points in the space. Hilbert spaces add geometric richness through the inner product, enabling orthogonality and powerful projection theorems.
                </p>

                <h4>1.4 Roadmap</h4>
                <p>
                    We begin with foundational theory: the contraction mapping principle and existence theorems. We then systematically develop each iteration scheme, providing:
                </p>
                <ul>
                    <li>Formal algorithm statements</li>
                    <li>Convergence theorems with complete proofs or detailed sketches</li>
                    <li>Worked numerical examples with iteration tables</li>
                    <li>Geometric visualizations showing convergence behavior</li>
                    <li>Implementation considerations and parameter selection</li>
                </ul>
                <p>
                    The narrative flows from simple to complex, from concrete to abstract, ensuring accessibility while building mathematical sophistication. Let us begin our journey.
                </p>
            </div>

            <!-- Foundational Theory -->
            <div class="math-content" id="section-foundations">
                <h3>2. Foundational Theory: When Do Fixed Points Exist?</h3>
                
                <h4>2.1 The Contraction Mapping Principle</h4>
                <p>
                    The bedrock of fixed point theory is Banach's 1922 result:
                </p>

                <div class="theorem">
                    <p class="theorem-title">Theorem [Banach Fixed Point Theorem]</p>
                    <p>
                        Let \((X, d)\) be a complete metric space and \(T: X \to X\) a contraction mapping, i.e., there exists \(L \in [0, 1)\) such that
                    </p>
                    <div class="equation">
                        <div class="equation-content">$$d(Tx, Ty) \leq L \cdot d(x, y) \quad \text{for all } x, y \in X.$$</div>
                        <div class="equation-number">(1)</div>
                    </div>
                    <p>
                        Then:
                    </p>
                    <ol>
                        <li>\(T\) has a unique fixed point \(x^* \in X\)</li>
                        <li>For any \(x_0 \in X\), the Picard iteration \(x_{n+1} = Tx_n\) converges to \(x^*\)</li>
                        <li>The error satisfies \(d(x_n, x^*) \leq \frac{L^n}{1-L} d(x_1, x_0)\)</li>
                    </ol>
                </div>

                <div class="proof">
                    <p class="proof-title">Proof Sketch</p>
                    <p>
                        Define \(x_{n+1} = Tx_n\) for \(n \geq 0\). Then
                    </p>
                    <div class="equation">
                        <div class="equation-content">$$d(x_{n+1}, x_n) = d(Tx_n, Tx_{n-1}) \leq L \cdot d(x_n, x_{n-1}) \leq L^2 d(x_{n-1}, x_{n-2}) \leq \cdots \leq L^n d(x_1, x_0).$$</div>
                        <div class="equation-number">(2)</div>
                    </div>
                    <p>
                        For \(m > n\), triangle inequality gives
                    </p>
                    <div class="equation">
                        <div class="equation-content">$$d(x_m, x_n) \leq \sum_{k=n}^{m-1} d(x_{k+1}, x_k) \leq d(x_1, x_0) \sum_{k=n}^{m-1} L^k = d(x_1, x_0) \frac{L^n - L^m}{1 - L} \leq \frac{L^n}{1-L} d(x_1, x_0).$$</div>
                        <div class="equation-number">(3)</div>
                    </div>
                    <p>
                        Since \(L < 1\), this tends to 0 as \(n \to \infty\), so \(\{x_n\}\) is Cauchy. By completeness, \(x_n \to x^*\) for some \(x^* \in X\). Continuity of \(T\) (which follows from the contraction property) gives
                    </p>
                    <div class="equation">
                        <div class="equation-content">$$x^* = \lim_{n \to \infty} x_n = \lim_{n \to \infty} Tx_{n-1} = T(x^*).$$</div>
                        <div class="equation-number">(4)</div>
                    </div>
                    <p>
                        Uniqueness: if \(Ty = y\), then \(d(x^*, y) = d(Tx^*, Ty) \leq L \cdot d(x^*, y)\), implying \((1-L)d(x^*, y) \leq 0\), hence \(x^* = y\).
                    </p>
                </div>

                <p>
                    This theorem is remarkable: it guarantees existence, uniqueness, and constructibility via iteration, all from a single contraction condition.
                </p>

                <h4>2.2 Beyond Contractions: Nonexpansive Mappings</h4>
                <p>
                    Contractions are rare in applications. Consider projection operators onto convex sets in Hilbert spaces, these are <em>nonexpansive</em> (Lipschitz constant \(L = 1\)) but not contractive. We need weaker conditions.
                </p>

                <div class="definition">
                    <p class="definition-title">Definition [Nonexpansive Mapping]</p>
                    <p>
                        A mapping \(T: X \to X\) on a normed space is <strong>nonexpansive</strong> if
                    </p>
                    <div class="equation">
                        <div class="equation-content">$$\|Tx - Ty\| \leq \|x - y\| \quad \text{for all } x, y \in X.$$</div>
                        <div class="equation-number">(5)</div>
                    </div>
                </div>

                <p>
                    Picard iteration \(x_{n+1} = Tx_n\) need not converge for nonexpansive \(T\), even if fixed points exist. Consider \(T: \mathbb{R}^2 \to \mathbb{R}^2\) given by \(T(x, y) = (-x, -y)\)â€”this is nonexpansive with unique fixed point \((0, 0)\), but starting from \((1, 0)\) produces the sequence \((1,0), (-1,0), (1,0), \ldots\) which oscillates.
                </p>
                <p>
                    This failure motivates Mann iteration, which averages the operator with the identity to ensure convergence.
                </p>

                <h4>2.3 Operator Classes</h4>
                <p>
                    We categorize operators by their Lipschitz behavior and fixed point structure:
                </p>

                <div class="definition">
                    <p class="definition-title">Definition [Pseudocontractive Mapping]</p>
                    <p>
                        \(T: X \to X\) is <strong>pseudocontractive</strong> if for all \(x, y \in X\) there exists \(j(x-y) \in J(x-y)\) (the normalized duality mapping) such that
                    </p>
                    <div class="equation">
                        <div class="equation-content">$$\langle Tx - Ty, j(x-y) \rangle \leq \|x - y\|^2.$$</div>
                        <div class="equation-number">(6)</div>
                    </div>
                    <p>
                        In Hilbert spaces with \(\langle \cdot, \cdot \rangle\), this becomes
                    </p>
                    <div class="equation">
                        <div class="equation-content">$$\langle Tx - Ty, x - y \rangle \leq \|x - y\|^2.$$</div>
                        <div class="equation-number">(7)</div>
                    </div>
                </div>

                <p>
                    The hierarchy is: Contractive \(\subsetneq\) Nonexpansive \(\subsetneq\) Pseudocontractive.
                </p>

                <div class="definition">
                    <p class="definition-title">Definition [Demicompact Operator]</p>
                    <p>
                        \(T\) is <strong>demicompact</strong> if whenever \(\{x_n\}\) is bounded and \(\|x_n - Tx_n\| \to 0\), then \(\{x_n\}\) has a convergent subsequence.
                    </p>
                </div>

                <p>
                    Demicompactness is crucial for convergence in infinite-dimensional spaces where Bolzano-Weierstrass fails.
                </p>
            </div>

            <!-- Classical Iteration Schemes -->
            <div class="math-content" id="section-classical">
                <h3>3. Classical Iteration Schemes</h3>
                
                <h4>3.1 Picard Iteration: The Foundation</h4>

                <div class="algorithm">
                    <p class="algorithm-title">Algorithm 1: Picard Iteration</p>
                    <div class="algorithm-content">
                        <p><strong>Input:</strong> Initial point \(x_0 \in X\), operator \(T: X \to X\), tolerance \(\epsilon > 0\), maximum iterations \(N\)</p>
                        <p><strong>Output:</strong> Approximate fixed point \(x^*\)</p>
                        <p>\(n \gets 0\)</p>
                        <p><strong>while</strong> \(n < N\) and \(\|x_n - Tx_n\| > \epsilon\) <strong>do</strong></p>
                        <p style="margin-left: 2em;">\(x_{n+1} \gets Tx_n\)</p>
                        <p style="margin-left: 2em;">\(n \gets n + 1\)</p>
                        <p><strong>end while</strong></p>
                        <p><strong>return</strong> \(x_n\)</p>
                    </div>
                </div>

                <div class="theorem">
                    <p class="theorem-title">Theorem [Picard Convergence]</p>
                    <p>
                        Let \(T: X \to X\) be a contraction mapping on a Banach space with constant \(L < 1\). Then for any \(x_0 \in X\), the Picard sequence \(x_{n+1} = Tx_n\) converges to the unique fixed point \(x^*\), and
                    </p>
                    <div class="equation">
                        <div class="equation-content">$$\|x_n - x^*\| \leq \frac{L^n}{1-L} \|x_1 - x_0\|.$$</div>
                        <div class="equation-number">(8)</div>
                    </div>
                    <p>
                        The convergence is linear with rate \(L\).
                    </p>
                </div>

                <div class="example">
                    <p class="example-title">Example [Computing Square Roots]</p>
                    <p>
                        Consider finding \(\sqrt{2}\) by solving \(x^2 = 2\). Define \(T(x) = \frac{1}{2}(x + \frac{2}{x})\) (the Babylonian method). For \(x \in [1, 2]\), we can verify \(T\) maps \([1,2]\) to itself and is contractive. Starting with \(x_0 = 1\):
                    </p>
                    
                    <table class="result-table">
                        <thead>
                            <tr>
                                <th>\(n\)</th>
                                <th>\(x_n\)</th>
                                <th>\(Tx_n\)</th>
                                <th>\(|x_n - \sqrt{2}|\)</th>
                                <th>\(|x_n - x_{n-1}|\)</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr><td>0</td><td>1.000000</td><td>1.500000</td><td>0.414214</td><td>---</td></tr>
                            <tr><td>1</td><td>1.500000</td><td>1.416667</td><td>0.085786</td><td>0.500000</td></tr>
                            <tr><td>2</td><td>1.416667</td><td>1.414216</td><td>0.002453</td><td>0.083333</td></tr>
                            <tr><td>3</td><td>1.414216</td><td>1.414214</td><td>0.000002</td><td>0.002451</td></tr>
                            <tr><td>4</td><td>1.414214</td><td>1.414214</td><td>\(<10^{-9}\)</td><td>0.000002</td></tr>
                        </tbody>
                    </table>
                    
                    <p>
                        Convergence is rapid due to the small contraction constant in the interval.
                    </p>
            <div class="figure-container">
            <img src="cobweb_babylonian.png" alt="Cobweb Baby" class="figure-image" style="width: 80%; max-width: 800px; max-height: 900px;">
            <img src="cobweb_multiple.png" alt="Cobweb Multi" class="figure-image">
            <div class="figure-caption">
                <strong>Figure 1:</strong> Picard iteration for \( \sqrt{2} \): cobweb diagram showing rapid convergence to the fixed point where \( T(x) = x \) intersects \( y = x \) and the cobweb diagram from different starting point.
            </div>
            </div>
                </div>

                <h4>3.2 Mann Iteration: Introducing Damping</h4>
                <p>
                    Mann iteration, introduced in 1953, adds a damping parameter to control convergence for nonexpansive operators.
                </p>

                <div class="algorithm">
                    <p class="algorithm-title">Algorithm 2: Mann Iteration</p>
                    <div class="algorithm-content">
                        <p><strong>Input:</strong> Initial point \(x_0 \in X\), operator \(T: X \to X\), sequence \(\{\alpha_n\} \subset (0,1)\), tolerance \(\epsilon > 0\)</p>
                        <p><strong>Output:</strong> Approximate fixed point \(x^*\)</p>
                        <p>\(n \gets 0\)</p>
                        <p><strong>while</strong> \(\|x_n - Tx_n\| > \epsilon\) <strong>do</strong></p>
                        <p style="margin-left: 2em;">\(x_{n+1} \gets (1 - \alpha_n) x_n + \alpha_n T x_n\)</p>
                        <p style="margin-left: 2em;">\(n \gets n + 1\)</p>
                        <p><strong>end while</strong></p>
                        <p><strong>return</strong> \(x_n\)</p>
                    </div>
                </div>

                <p>
                    The iteration can be written as
                </p>
                <div class="equation">
                    <div class="equation-content">$$x_{n+1} = x_n + \alpha_n (Tx_n - x_n), \quad n \geq 0.$$</div>
                    <div class="equation-number">(9)</div>
                </div>
                <p>
                    This is a gradient-like step toward \(Tx_n\) with step size \(\alpha_n\).
                </p>

                <div class="theorem">
                    <p class="theorem-title">Theorem [Mann Convergence]</p>
                    <p>
                        Let \(X\) be a Banach space, \(K \subset X\) a nonempty closed convex subset, and \(T: K \to K\) a nonexpansive mapping with \(\text{Fix}(T) \neq \emptyset\). Suppose \(\{\alpha_n\} \subset [a, b] \subset (0, 1)\) satisfies \(\sum_{n=0}^\infty \alpha_n = \infty\). If \(T\) is demicompact, then the Mann sequence converges strongly to a fixed point of \(T\).
                    </p>
                </div>

                <div class="remark">
                    <p class="remark-title">Remark</p>
                    <p>
                        The condition \(\sum \alpha_n = \infty\) ensures the iteration doesn't stall, while boundedness prevents overshooting. Common choices: \(\alpha_n = \frac{1}{n+1}\), \(\alpha_n = 0.5\), or \(\alpha_n = \frac{n}{n+2}\).
                    </p>
                </div>

                <div class="example">
                    <p class="example-title">Example [Projection onto Interval]</p>
                    <p>
                        Let \(X = \mathbb{R}\) and \(T: [0, 3] \to [0, 3]\) be \(T(x) = 3 - x\). This is nonexpansive with fixed point \(x^* = 1.5\). Picard iteration oscillates: starting at \(x_0 = 0\) gives \(0, 3, 0, 3, \ldots\)
                    </p>
                    <p>
                        Apply Mann iteration with \(\alpha_n = 0.5\):
                    </p>
                    
                    <table class="result-table">
                        <thead>
                            <tr>
                                <th>\(n\)</th>
                                <th>\(x_n\)</th>
                                <th>\(Tx_n\)</th>
                                <th>\(|x_n - 1.5|\)</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr><td>0</td><td>0.000</td><td>3.000</td><td>1.500</td></tr>
                            <tr><td>1</td><td>1.500</td><td>1.500</td><td>0.000</td></tr>
                        </tbody>
                    </table>
                    
                    <p>
                        With \(\alpha_n = 0.3\):
                    </p>
                    
                    <table class="result-table">
                        <thead>
                            <tr>
                                <th>\(n\)</th>
                                <th>\(x_n\)</th>
                                <th>\(Tx_n\)</th>
                                <th>\(|x_n - 1.5|\)</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr><td>0</td><td>0.000</td><td>3.000</td><td>1.500</td></tr>
                            <tr><td>1</td><td>0.900</td><td>2.100</td><td>0.600</td></tr>
                            <tr><td>2</td><td>1.260</td><td>1.740</td><td>0.240</td></tr>
                            <tr><td>3</td><td>1.404</td><td>1.596</td><td>0.096</td></tr>
                            <tr><td>4</td><td>1.462</td><td>1.538</td><td>0.038</td></tr>
                            <tr><td>5</td><td>1.485</td><td>1.515</td><td>0.015</td></tr>
                        </tbody>
                    </table>
                    
                    <p>
                        The convex averaging eliminates oscillation.
                    </p>
                    <div class="figure-container">
                    <img src="PicardvsMann.png" alt="PicardvsMann" class="figure-image">
                     <div class="figure-caption">
                    <strong>Figure 2:</strong> Comparison of Picard (oscillating) vs Mann iteration for \( T(x) = 3 - x \) on \([0,3] \).
                    </div>
                    </div>
                </div>

                <h4>3.3 Ishikawa Iteration: Two-Step Refinement</h4>
                <p>
                    Ishikawa iteration (1974) extends Mann by introducing an intermediate step, enabling convergence for Lipschitz pseudocontractive mappings.
                </p>

                <div class="algorithm">
                    <p class="algorithm-title">Algorithm 3: Ishikawa Iteration</p>
                    <div class="algorithm-content">
                        <p><strong>Input:</strong> Initial \(x_0 \in X\), operator \(T: X \to X\), sequences \(\{\alpha_n\}, \{\beta_n\} \subset (0,1)\)</p>
                        <p><strong>Output:</strong> Approximate fixed point</p>
                        <p>\(n \gets 0\)</p>
                        <p><strong>while</strong> not converged <strong>do</strong></p>
                        <p style="margin-left: 2em;">\(y_n \gets (1 - \beta_n) x_n + \beta_n T x_n\)</p>
                        <p style="margin-left: 2em;">\(x_{n+1} \gets (1 - \alpha_n) x_n + \alpha_n T y_n\)</p>
                        <p style="margin-left: 2em;">\(n \gets n + 1\)</p>
                        <p><strong>end while</strong></p>
                        <p><strong>return</strong> \(x_n\)</p>
                    </div>
                </div>

                <p>
                    The iteration is:
                </p>
                <div class="equation">
                    <div class="equation-content">$$y_n = (1 - \beta_n) x_n + \beta_n T x_n,$$</div>
                    <div class="equation-number">(10)</div>
                </div>
                <div class="equation">
                    <div class="equation-content">$$x_{n+1} = (1 - \alpha_n) x_n + \alpha_n T y_n.$$</div>
                    <div class="equation-number">(11)</div>
                </div>

                <div class="theorem">
                    <p class="theorem-title">Theorem [Ishikawa Convergence]</p>
                    <p>
                        Let \(X\) be a real Banach space, \(K \subset X\) closed convex, and \(T: K \to K\) Lipschitz pseudocontractive with \(\text{Fix}(T) \neq \emptyset\). Suppose:
                    </p>
                    <ul>
                        <li>\(\{\alpha_n\}, \{\beta_n\} \subset [0, 1]\) with \(\sum \alpha_n = \infty\), \(\sum \alpha_n^2 < \infty\), \(\sum \beta_n^2 < \infty\)</li>
                        <li>\(T\) is demicompact</li>
                    </ul>
                    <p>
                        Then the Ishikawa sequence converges strongly to a fixed point.
                    </p>
                </div>

                <p>
                    The second step \(Ty_n\) uses information from both \(x_n\) and \(Tx_n\), providing a form of momentum that stabilizes convergence.
                </p>

                <div class="example">
                    <p class="example-title">Example [Pseudocontractive Mapping]</p>
                    <p>
                        Consider \(X = \mathbb{R}\) with \(T: [0, 4] \to [0, 4]\) defined by \(T(x) = x - \frac{x^2}{4} + 1\). This is pseudocontractive with fixed point \(x^* = 2\). Starting at \(x_0 = 0.5\) with \(\alpha_n = \beta_n = \frac{1}{n+2}\):
                    </p>
                    
                    <table class="result-table">
                        <thead>
                            <tr>
                                <th>\(n\)</th>
                                <th>\(x_n\)</th>
                                <th>\(y_n\)</th>
                                <th>\(Ty_n\)</th>
                                <th>\(|x_n - 2|\)</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr><td>0</td><td>0.500</td><td>0.781</td><td>1.848</td><td>1.500</td></tr>
                            <tr><td>1</td><td>0.949</td><td>1.281</td><td>1.873</td><td>1.051</td></tr>
                            <tr><td>2</td><td>1.180</td><td>1.458</td><td>1.919</td><td>0.820</td></tr>
                            <tr><td>3</td><td>1.327</td><td>1.556</td><td>1.945</td><td>0.673</td></tr>
                            <tr><td>4</td><td>1.430</td><td>1.620</td><td>1.963</td><td>0.570</td></tr>
                            <tr><td>5</td><td>1.506</td><td>1.666</td><td>1.976</td><td>0.494</td></tr>
                            <tr><td>10</td><td>1.743</td><td>1.807</td><td>1.995</td><td>0.257</td></tr>
                            <tr><td>20</td><td>1.898</td><td>1.920</td><td>1.999</td><td>0.102</td></tr>
                        </tbody>
                    </table>
                    
                    <p>
                        The two-step process provides smoother convergence than Mann for this operator class.
                    </p>
                    <div class="figure-container">
                   <img src="IshikawavsMann.png" alt="IshikawavsMann" class="figure-image">
                   <div class="figure-caption">
                   <strong>Figure 3:</strong> Logarithmic error plot showing Ishikawa iteration converging faster than Mann for a pseudocontractive operator.
                  </div>
                  </div>

                </div>

                <h4>3.4 Noor Iteration: Three-Step Extension</h4>
                <p>
                    Noor iteration (2000) adds a third step, further expanding the class of operators:
                </p>
                <div class="equation">
                    <div class="equation-content">$$z_n = (1 - \gamma_n) x_n + \gamma_n T x_n,$$</div>
                    <div class="equation-number">(12)</div>
                </div>
                <div class="equation">
                    <div class="equation-content">$$y_n = (1 - \beta_n) x_n + \beta_n T z_n,$$</div>
                    <div class="equation-number">(13)</div>
                </div>
                <div class="equation">
                    <div class="equation-content">$$x_{n+1} = (1 - \alpha_n) x_n + \alpha_n T y_n.$$</div>
                    <div class="equation-number">(14)</div>
                </div>

                <div class="theorem">
                    <p class="theorem-title">Theorem [Noor Convergence]</p>
                    <p>
                        Under similar conditions to Ishikawa (with three parameter sequences \(\{\alpha_n\}, \{\beta_n\}, \{\gamma_n\}\) satisfying appropriate summability conditions), Noor iteration converges to a fixed point for Lipschitz strongly pseudocontractive mappings.
                    </p>
                </div>

                <p>
                    The additional flexibility allows handling more general operator classes but at increased computational cost per iteration.
                </p>

                <h4>3.5 S-Iteration: Simplified Alternative</h4>
                <p>
                    The S-iteration (Agarwal et al., 2007) simplifies Ishikawa while maintaining similar convergence properties:
                </p>
                <div class="equation">
                    <div class="equation-content">$$y_n = (1 - \beta_n) x_n + \beta_n T x_n,$$</div>
                    <div class="equation-number">(15)</div>
                </div>
                <div class="equation">
                    <div class="equation-content">$$x_{n+1} = (1 - \alpha_n) T y_n + \alpha_n T x_n.$$</div>
                    <div class="equation-number">(16)</div>
                </div>

                <p>
                    Notice that unlike Ishikawa, \(x_{n+1}\) doesn't involve \(x_n\) directly, only through the operator applications.
                </p>

                <div class="proposition">
                    <p class="proposition-title">Proposition [S-iteration Properties]</p>
                    <p>
                        For nonexpansive \(T\) with fixed points, under standard conditions on \(\{\alpha_n\}, \{\beta_n\}\):
                    </p>
                    <ol>
                        <li>S-iteration converges to a fixed point</li>
                        <li>Convergence rate is comparable to Ishikawa</li>
                        <li>Requires one fewer evaluation of \(T\) per iteration in some implementations</li>
                    </ol>
                </div>

                <div class="example">
                    <p class="example-title">Example [Comparative Study]</p>
                    <p>
                        Consider \(T: [-1, 1] \to [-1, 1]\) with \(T(x) = \frac{x}{2} \cos(x)\) (nonexpansive, fixed point \(x^* = 0\)). Starting at \(x_0 = 0.8\) with \(\alpha_n = \beta_n = 0.5\):
                    </p>
                    
                    <table class="result-table">
                        <thead>
                            <tr>
                                <th>\(n\)</th>
                                <th>Mann</th>
                                <th>Ishikawa</th>
                                <th>S-iteration</th>
                                <th>Noor</th>
                                <th>Picard</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr><td>0</td><td>0.8000</td><td>0.8000</td><td>0.8000</td><td>0.8000</td><td>0.8000</td></tr>
                            <tr><td>2</td><td>0.4123</td><td>0.3256</td><td>0.3189</td><td>0.2987</td><td>0.2788</td></tr>
                            <tr><td>4</td><td>0.2234</td><td>0.1389</td><td>0.1301</td><td>0.1087</td><td>0.0773</td></tr>
                            <tr><td>6</td><td>0.1245</td><td>0.0638</td><td>0.0571</td><td>0.0429</td><td>0.0214</td></tr>
                            <tr><td>8</td><td>0.0712</td><td>0.0312</td><td>0.0265</td><td>0.0181</td><td>0.0059</td></tr>
                            <tr><td>10</td><td>0.0414</td><td>0.0156</td><td>0.0127</td><td>0.0078</td><td>0.0016</td></tr>
                        </tbody>
                    </table>
                    
                    <p>
                        Picard converges fastest (being a contraction), but multi-step methods provide more stable convergence paths.
                    </p>
                   <div class="figure-container">
                    <img src="Generalized_FPI.png" alt="Generalized FPI" class="figure-image">
                   <div class="figure-caption">
                    <strong>Figure 4:</strong> Comparison of convergence rates for different iteration schemes applied to\(  T(x) = \frac{x}{2}\cos(x) \).
                     </div>
                    </div>

                </div>
            </div>

            <!-- Convergence in Hilbert Spaces -->
            <div class="math-content" id="section-hilbert">
                <h3>4. Convergence in Hilbert Spaces</h3>
                
                <p>
                    Hilbert spaces possess additional geometric structure through the inner product, enabling stronger convergence results and new iteration strategies.
                </p>

                <h4>4.1 Weak and Strong Convergence</h4>

                <div class="definition">
                    <p class="definition-title">Definition [Strong and Weak Convergence]</p>
                    <p>
                        Let \(H\) be a Hilbert space and \(\{x_n\}\) a sequence in \(H\).
                    </p>
                    <ul>
                        <li>\(\{x_n\}\) converges <strong>strongly</strong> to \(x\) if \(\|x_n - x\| \to 0\), written \(x_n \to x\).</li>
                        <li>\(\{x_n\}\) converges <strong>weakly</strong> to \(x\) if \(\langle x_n, y \rangle \to \langle x, y \rangle\) for all \(y \in H\), written \(x_n \rightharpoonup x\).</li>
                    </ul>
                </div>

                <p>
                    Strong convergence implies weak convergence, but not conversely. In infinite-dimensional spaces, bounded sequences may have no strongly convergent subsequences but always have weakly convergent ones (by weak compactness of closed balls).
                </p>

                <div class="theorem">
                    <p class="theorem-title">Theorem [Opial's Condition]</p>
                    <p>
                        A Hilbert space \(H\) satisfies <strong>Opial's condition</strong>: if \(x_n \rightharpoonup x\), then
                        \(\liminf_{n \to \infty} \|x_n - x\| < \liminf_{n \to \infty} \|x_n - y\|\)
                        for all \(y \neq x\).
                    </p>
                </div>

                <p>
                    This condition is crucial for proving weak convergence of iteration sequences to fixed points.
                </p>

                <h4>4.2 Projection Operators</h4>

                <div class="definition">
                    <p class="definition-title">Definition [Metric Projection]</p>
                    <p>
                        Let \(C \subset H\) be nonempty, closed, and convex. The <strong>metric projection</strong> \(P_C: H \to C\) is defined by
                        \(P_C(x) = \arg\min_{y \in C} \|x - y\|.\)
                    </p>
                </div>

                <div class="theorem">
                    <p class="theorem-title">Theorem [Projection Properties]</p>
                    <p>
                        The metric projection \(P_C\) is:
                    </p>
                    <ol>
                        <li>Well-defined and unique</li>
                        <li>Nonexpansive: \(\|P_C(x) - P_C(y)\| \leq \|x - y\|\)</li>
                        <li>Characterized by: \(z = P_C(x) \iff \langle x - z, y - z \rangle \leq 0\) for all \(y \in C\)</li>
                    </ol>
                </div>

                <p>
                    Projections are the prototypical nonexpansive operators, and finding common fixed points of projections solves convex feasibility problems.
                </p>

                <h4>4.3 Firmly Nonexpansive Operators</h4>

                <div class="definition">
                    <p class="definition-title">Definition [Firmly Nonexpansive]</p>
                    <p>
                        \(T: H \to H\) is <strong>firmly nonexpansive</strong> if
                        \(\|Tx - Ty\|^2 + \|(I - T)x - (I - T)y\|^2 \leq \|x - y\|^2\)
                        for all \(x, y \in H\). Equivalently,
                        \(\langle Tx - Ty, x - y \rangle \geq \|Tx - Ty\|^2.\)
                    </p>
                </div>

                <p>
                    Projections are firmly nonexpansive. Firmly nonexpansive operators form a subclass of nonexpansive operators with better convergence properties.
                </p>

                <div class="theorem">
                    <p class="theorem-title">Theorem [Krasnoselskii-Mann for Firmly Nonexpansive]</p>
                    <p>
                        Let \(T: H \to H\) be firmly nonexpansive with \(\text{Fix}(T) \neq \emptyset\). The Mann iteration
                        \(x_{n+1} = (1 - \alpha_n) x_n + \alpha_n T x_n\)
                        with \(\alpha_n \in [\epsilon, 1 - \epsilon]\) for some \(\epsilon > 0\) converges weakly to a fixed point.
                    </p>
                </div>

                <h4>4.4 Common Fixed Points via Alternating Projections</h4>
                <p>
                    The <strong>method of alternating projections</strong> finds points in \(C_1 \cap C_2\) by iterating:
                    \(x_{n+1} = P_{C_1} P_{C_2}(x_n).\)
                </p>

                <div class="theorem">
                    <p class="theorem-title">Theorem [Von Neumann's Alternating Projection]</p>
                    <p>
                        If \(C_1, C_2 \subset H\) are closed convex subspaces with \(C_1 \cap C_2 \neq \emptyset\), then alternating projections converge strongly to \(P_{C_1 \cap C_2}(x_0)\).
                    </p>
                </div>

                <p>
                    For general closed convex sets (not subspaces), convergence is weak, not strong.
                </p>

                <div class="example">
                    <p class="example-title">Example [Intersection of Line and Circle in \(\mathbb{R}^2\)]</p>
                    <p>
                        Let \(C_1 = \{(x, y) : y = 0\}\) (horizontal axis) and \(C_2 = \{(x, y) : x^2 + y^2 = 1\}\) (unit circle). Their intersection is \(\{(-1, 0), (1, 0)\}\). Starting at \(x_0 = (0, 2)\):
                    </p>
                    
                    <table class="result-table">
                        <thead>
                            <tr>
                                <th>\(n\)</th>
                                <th>\(x_n\)</th>
                                <th>\(P_{C_2}(x_n)\)</th>
                                <th>\(P_{C_1}P_{C_2}(x_n)\)</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr><td>0</td><td>(0.000, 2.000)</td><td>(0.000, 1.000)</td><td>(0.000, 0.000)</td></tr>
                            <tr><td>1</td><td>(0.000, 0.000)</td><td>(0.000, 0.000)</td><td>(0.000, 0.000)</td></tr>
                        </tbody>
                    </table>
                    
                    <p>
                        Converges to \((0, 0)\) which is \(P_{C_1 \cap C_2}(x_0)\) (the origin is closest point in intersection to \((0, 2)\)).
                    </p>
                    <div class="figure-container">
            <img src="Maps_FPI.png" alt="Maps_FPI" class="figure-image">
            <div class="figure-caption">
                <strong>Figure 5:</strong> Alternating projections between horizontal axis \( C_1 \) and unit circle \( C_2 \), converging to \( (0, 0) \in C_1 \cap C_2 \).
            </div>
    </div>

                </div>
            </div>

            <!-- Section 1: Finding Common Fixed Points -->
            <div class="math-content" id="fixed_point">
                <h3>5. Finding Common Fixed Points: Advanced Schemes</h3>
                
                <h4>5.1 Parallel and Cyclic Algorithms</h4>
                <p>
                    For a family \( \{T_i\}_{i=1}^N \) of nonexpansive operators, we seek common fixed points: \( x^* \in \bigcap_{i=1}^N \text{Fix}(T_i) \).
                </p>
                 
                <h5>Parallel Scheme</h5>
                <div class="equation">
                    <div class="equation-content">$$x_{n+1} = \sum_{i=1}^N w_i T_i x_n, \quad \sum_{i=1}^N w_i = 1, \; w_i > 0.$$</div>
                    <div class="equation-number">(17)</div>
                </div>
                <p>This averages all operators at each step.</p>

                <h5>Cyclic Scheme</h5>
                <div class="equation">
                    <div class="equation-content">$$x_{n+1} = T_{n \bmod N + 1} x_n.$$</div>
                    <div class="equation-number">(18)</div>
                </div>
                <p>Apply operators sequentially in a cycle.</p>

                <div class="theorem">
                    <p class="theorem-title">Theorem [Common Fixed Point Convergence]</p>
                    <p>Let \( H \) be a Hilbert space, \( \{T_i\}_{i=1}^N \) nonexpansive with \( \bigcap_{i=1}^N \text{Fix}(T_i) \neq \emptyset \). Then:</p>
                    <ol>
                        <li>The parallel scheme with convex weights converges weakly to a common fixed point</li>
                        <li>The cyclic scheme converges weakly to a common fixed point</li>
                    </ol>
                </div>
                <h4>5.2 Viscosity Approximation Methods</h4>
                
                <p>
                    To ensure strong convergence (not just weak), we add a contraction term:
                </p>
                
                <div class="equation">
                    <div class="equation-content">$$x_{n+1} = \alpha_n f(x_n) + (1 - \alpha_n) T x_n,$$</div>
                    <div class="equation-number">(19)</div>
                </div>
                
                <p>
                    where \( f: H \to H \) is a contraction and \( \alpha_n \to 0 \) slowly (e.g., \( \alpha_n = \frac{1}{n} \)).
                </p>

                <div class="theorem">
                    <p class="theorem-title">Theorem [Moudafi Viscosity]</p>
                    <p>
                        If \( T \) is nonexpansive with \( \text{Fix}(T) \neq \emptyset \), \( f \) is a contraction, and \( \sum \alpha_n = \infty \), \( \sum \alpha_n^2 < \infty \), then \( x_n \to x^* \) strongly, where \( x^* \) is the unique solution to
                        \( \langle (I - f)x^*, x^* - y \rangle \leq 0 \quad \text{for all } y \in \text{Fix}(T). \)
                    </p>
                </div>
                <h4>5.3 Hybrid and Shrinking Projection Methods</h4>
                
                <p>
                    These methods project onto half-spaces defined by previous iterates to guarantee strong convergence.
                </p>

                <div class="algorithm">
                    <p class="algorithm-title">Algorithm: Hybrid (CQ) Algorithm</p>
                    <div class="algorithm-content">
                        <p><strong>Input:</strong> Initial \( x_0 = x \in H \), nonexpansive \( T \)</p>
                        <p>\( C_0 \gets H \), \( Q_0 \gets H \)</p>
                        <p><strong>For</strong> \( n = 0, 1, 2, \ldots \):</p>
                        <p style="margin-left: 2rem;">\( y_n \gets (1 - \alpha_n)x_n + \alpha_n T x_n \)</p>
                        <p style="margin-left: 2rem;">\( C_{n+1} \gets \{z \in C_n : \|y_n - z\| \leq \|x_n - z\|\} \)</p>
                        <p style="margin-left: 2rem;">\( Q_{n+1} \gets \{z \in Q_n : \langle x_n - z, x_0 - x_n \rangle \geq 0\} \)</p>
                        <p style="margin-left: 2rem;">\( x_{n+1} \gets P_{C_{n+1} \cap Q_{n+1}}(x_0) \)</p>
                        <p><strong>End For</strong></p>
                    </div>
                </div>

                <p>
                    This constructs shrinking sets \( C_n, Q_n \) containing all fixed points, ensuring \( x_n \to x^* \) strongly.
                </p>
            </div>

            <!-- Section 6: Applications and Examples -->
            <div class="math-content" id="section-applications">
                <h3>6. Applications and Examples</h3>
                
                <h4>6.1 Split Feasibility Problem</h4>
                <p>
                    Find \( x \in C \) such that \( Ax \in Q \), where \( C \subset H_1 \), \( Q \subset H_2 \) are convex, and \( A: H_1 \to H_2 \) is linear.
                </p>
                
                <p><strong>Solution:</strong> Iterate</p>
                <div class="equation">
                    <div class="equation-content">$$x_{n+1} = P_C(x_n - \gamma A^*(I - P_Q)Ax_n),$$</div>
                    <div class="equation-number">(20)</div>
                </div>
                <p>where \( \gamma \in (0, 2/\|A\|^2) \). This is a fixed point iteration for a nonexpansive operator.</p>

                <div class="example">
                    <p class="example-title">Example [Image Recovery]</p>
                    <p>
                        In CT reconstruction, \( C \) represents image constraints (non-negativity, bounded support), \( Q \) represents projection data constraints, and \( A \) is the Radon transform. The algorithm reconstructs images satisfying both types of constraints.
                    </p>
                </div>

                <h4>6.2 Variational Inequalities</h4>
                <p>
                    Find \( x^* \in C \) such that
                    \( \langle F(x^*), y - x^* \rangle \geq 0 \quad \text{for all } y \in C. \)
                </p>
                
                <p>
                    This is equivalent to finding \( x^* = P_C(x^* - \lambda F(x^*)) \) for any \( \lambda > 0 \). Apply Mann or Ishikawa iteration to the operator \( T(x) = P_C(x - \lambda F(x)) \).
                </p>

                <h4>6.3 Nash Equilibrium Computation</h4>
                <p>
                    In an \( N \)-player game, each player \( i \) has best-response operator \( B_i: \prod_{j=1}^N S_j \to S_i \). A Nash equilibrium is a common fixed point of the composite operator \( B = (B_1, \ldots, B_N) \).
                </p>
                
                <p>
                    Apply cyclic or parallel schemes to find equilibria when \( B_i \) are nonexpansive (satisfied for convex games).
                </p>

                <h4>6.4 Numerical Example: Convex Feasibility</h4>
                <p>
                    Find a point in \( C_1 \cap C_2 \cap C_3 \subset \mathbb{R}^2 \) where:
                </p>
                <div class="equation">
                    <div class="equation-content">$$C_1 = \{(x, y) : x^2 + y^2 \leq 4\},$$</div>
                    <div class="equation-number">(21)</div>
                </div>
                <div class="equation">
                    <div class="equation-content">$$C_2 = \{(x, y) : (x-2)^2 + (y-1)^2 \leq 2\},$$</div>
                    <div class="equation-number">(22)</div>
                </div>
                <div class="equation">
                    <div class="equation-content">$$C_3 = \{(x, y) : y \geq 0\}.$$</div>
                    <div class="equation-number">(23)</div>
                </div>
                
                <p>
                    Apply parallel projections: \( x_{n+1} = \frac{1}{3}(P_{C_1}(x_n) + P_{C_2}(x_n) + P_{C_3}(x_n)) \) starting at \( x_0 = (3, 2) \):
                </p>

                <div class="table-container">
                    <table class="data-table">
                        <thead>
                            <tr>
                                <th>\( n \)</th>
                                <th>\( x_n \)</th>
                                <th>\( P_{C_1}(x_n) \)</th>
                                <th>\( P_{C_2}(x_n) \)</th>
                                <th>\( P_{C_3}(x_n) \)</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr><td>0</td><td>(3.000, 2.000)</td><td>(1.664, 1.109)</td><td>(2.207, 1.483)</td><td>(3.000, 2.000)</td></tr>
                            <tr><td>1</td><td>(2.290, 1.531)</td><td>(1.710, 1.143)</td><td>(2.093, 1.406)</td><td>(2.290, 1.531)</td></tr>
                            <tr><td>2</td><td>(2.031, 1.360)</td><td>(1.711, 1.146)</td><td>(2.011, 1.351)</td><td>(2.031, 1.360)</td></tr>
                            <tr><td>3</td><td>(1.918, 1.286)</td><td>(1.709, 1.145)</td><td>(1.935, 1.299)</td><td>(1.918, 1.286)</td></tr>
                            <tr><td>5</td><td>(1.802, 1.209)</td><td>(1.707, 1.145)</td><td>(1.818, 1.221)</td><td>(1.802, 1.209)</td></tr>
                            <tr><td>10</td><td>(1.742, 1.169)</td><td>(1.706, 1.144)</td><td>(1.746, 1.172)</td><td>(1.742, 1.169)</td></tr>
                        </tbody>
                    </table>
                </div>
                
                <p>
                    Converges to approximately \( (1.71, 1.15) \in C_1 \cap C_2 \cap C_3 \).
                </p>

            <div class="figure-container">
            <img src="Parallel_projection.png" alt="Parallel_projection" class="figure-image">
            <div class="figure-caption">
                <strong>Figure 6:</strong> Parallel projection algorithm converging to a point in \( C_1 \cap C_2 \cap C_3 \). The shaded region shows the feasible set. The algorithm starts at \( x_0 = (3, 2) \) and converges to the intersection point \( x^* \approx (1.71, 1.15) \).
                    </div>
                </div>
            

            <!-- Section 7: Comparison and Parameter Selection -->
            <div class="math-content" id="section-comparison">
                <h3>7. Comparison and Parameter Selection</h3>
                <h4>7.1 Parameter Selection Guidelines</h4>
                
                <p>The Mann iteration method uses different schemes for the iteration parameter \(\alpha_n\). 
Common choices include fixed \(\alpha_n = 0.5\), which is simple and robust; diminishing 
\(\alpha_n = \frac{1}{n+1}\), which ensures the sum of \(\alpha_n\) diverges and thus convergence; 
and bounded \(\alpha_n = \frac{n}{n+2}\), which stays within [0,1) and typically converges faster 
than diminishing sequences. The Ishikawa iteration involves two parameters \(\alpha_n\) and \(\beta_n\) 
with conditions such that \(\sum \alpha_n = \infty\), \(\sum \alpha_n^2 < \infty\), and \(\sum \beta_n^2 < \infty\). 
Example parameters satisfying these include \(\alpha_n = \frac{1}{n+1}\) and \(\beta_n = \frac{1}{(n+1)^2}\), 
but practical fixed choices like \(\alpha_n = 0.6\) and \(\beta_n = 0.4\) also work well in applications.</p>

<p>In general, a larger \(\alpha_n\) leads to faster initial progress but may cause instability, while smaller 
\(\alpha_n\) values improve stability but slow down convergence. The Mann iteration is simpler with a single parameter
 controlling the iteration, whereas the Ishikawa iteration uses two parameters to more flexibly approach fixed points,
 often requiring specific summability conditions to guarantee convergence. The choice between these parameters depends
 on the desired trade-off between speed and stability in the iteration process.</p>
                
                
                <div class="table-container">
                    <table class="comparison-table">
                        <caption>Comparison of Fixed Point Iteration Schemes</caption>
                        <thead>
                            <tr>
                                <th>Method</th>
                                <th>Steps</th>
                                <th>Operator Class</th>
                                <th>Convergence</th>
                                <th>Parameters</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr><td>Picard</td><td>1</td><td>Contraction</td><td>Strong, Linear</td><td>None</td></tr>
                            <tr><td>Mann</td><td>1</td><td>Nonexpansive</td><td>Weak</td><td>\( \{\alpha_n\} \)</td></tr>
                            <tr><td>Ishikawa</td><td>2</td><td>Lipschitz Pseudo.</td><td>Strong/Weak</td><td>\( \{\alpha_n, \beta_n\} \)</td></tr>
                            <tr><td>Noor</td><td>3</td><td>Strongly Pseudo.</td><td>Strong/Weak</td><td>\( \{\alpha_n, \beta_n, \gamma_n\} \)</td></tr>
                            <tr><td>S-iteration</td><td>2</td><td>Nonexpansive</td><td>Weak</td><td>\( \{\alpha_n, \beta_n\} \)</td></tr>
                            <tr><td>Hybrid</td><td>Variable</td><td>Nonexpansive</td><td>Strong</td><td>\( \{\alpha_n\} \) + projection</td></tr>
                        </tbody>
                    </table>
                </div>

                

                <h4>7.2 Computational Cost Analysis</h4>
                 <p>Multi-step methods such as Ishikawa and Noor iterations require more operator evaluations per iteration compared to simpler methods like Mann iteration. However, these multi-step methods may converge in fewer iterations, especially for difficult problems. The choice between these methods depends on several factors, including the cost of operator evaluation, the difficulty of achieving convergence, and implementation simplicity.</p> 

<p>If the operator \( T \) is expensive to evaluate, for example involving solving partial differential equations (PDEs), the Mann iteration is preferred due to its lower per-iteration computational cost. For nearly nonexpansive operators where the Lipschitz constant \( L \approx 1 \), multi-step methods such as Ishikawa or Noor iterations may be necessary to ensure convergence. Additionally, Mann and Picard iterations are easier to implement and tune, making them attractive choices when simplicity is important. Ultimately, the trade-off between computational cost and convergence speed guides the method selection.</p>

                <div class="table-container">
                    <table class="cost-table">
                        <caption>Per-Iteration Cost for Different Schemes</caption>
                        <thead>
                            <tr>
                                <th>Method</th>
                                <th>Operator Evaluations</th>
                                <th>Additional Operations</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr><td>Picard</td><td>1</td><td>None</td></tr>
                            <tr><td>Mann</td><td>1</td><td>1 convex combination</td></tr>
                            <tr><td>Ishikawa</td><td>2</td><td>2 convex combinations</td></tr>
                            <tr><td>Noor</td><td>3</td><td>3 convex combinations</td></tr>
                            <tr><td>S-iteration</td><td>2</td><td>1 convex combination</td></tr>
                            <tr><td>Parallel (N ops)</td><td>N</td><td>N-way averaging</td></tr>
                            <tr><td>Cyclic (N ops)</td><td>1</td><td>Indexing</td></tr>
                        </tbody>
                    </table>
                </div>
                
                
            </div>

            <!-- Section 8: Modern Extensions and Open Problems -->
            <div class="math-content" id="section-modern_extension">
                <h3>8. Modern Extensions and Open Problems</h3>
                
                <h4>8.1 Inertial Methods</h4>
                <p>
                    Add momentum term for acceleration:
                </p>
                <div class="equation">
                    <div class="equation-content">$$x_{n+1} = (1 - \alpha_n) T y_n + \alpha_n T x_n + \theta_n (x_n - x_{n-1}),$$</div>
                    <div class="equation-number">(24)</div>
                </div>
                <p>
                    where \( \theta_n \in [0, 1) \) is the inertial parameter. This can significantly speed convergence for slowly varying operators.
                </p>

                <h4>8.2 Stochastic Fixed Point Iteration</h4>
                <p>
                    For stochastic operators \( T_\omega \), use stochastic approximation:
                </p>
                <div class="equation">
                    <div class="equation-content">$$x_{n+1} = x_n + \alpha_n (T_{\omega_n}(x_n) - x_n),$$</div>
                    <div class="equation-number">(25)</div>
                </div>
                <p>
                    where \( \omega_n \) are random samples. Applications include reinforcement learning (finding fixed points of Bellman operators) and stochastic optimization.
                </p>

                <h4>8.3 Fixed Points in Machine Learning</h4>
                
                <p><strong>Implicit layers</strong>: Neural networks with layers defined by fixed point equations:</p>
                <div class="equation">
                    <div class="equation-content">$$z^* = \sigma(Wz^* + Ux + b).$$</div>
                    <div class="equation-number">(26)</div>
                </div>
                <p>Training requires finding \( z^* \) via fixed point iteration at each forward pass.</p>
                
                <p><strong>Equilibrium models</strong>: Deep equilibrium networks replace depth with fixed point convergence, reducing memory in training.</p>

                <h4>8.4 Open Questions</h4>
                
                <ol>
                    <li><strong>Optimal parameter sequences</strong>: What choice of \( \{\alpha_n\} \) minimizes total iterations for a given operator class?</li>
                    <li><strong>Finite-time convergence</strong>: Under what conditions do iterations converge in finitely many steps?</li>
                    <li><strong>Acceleration limits</strong>: Can we beat the golden ratio convergence rate of the secant method for general nonexpansive operators?</li>
                    <li><strong>Distributed algorithms</strong>: How to efficiently find common fixed points when operators are distributed across multiple processors with limited communication?</li>
                </ol>
            </div>

            <!-- Section 7: Conclusion -->
            <div class="math-content" id="section-conclusion">
                <h3>9. Conclusion</h3>
                
                <p>
                    We have journeyed through the landscape of fixed point iteration methods in normed linear spaces, progressing from the foundational Picard iteration to sophisticated multi-step schemes. Several key insights emerge:
                </p>
                
                <p><strong>Geometric intuition matters</strong>: Fixed point iteration is fundamentally about repeated geometric transformations. Understanding how operators behave geometrically, whether they contract, preserve distances, or mildly expand guides algorithm design.</p>
                
                <p><strong>Space structure enables convergence</strong>: The progression from normed spaces to Banach spaces (completeness) to Hilbert spaces (inner products) mirrors increasing power of convergence theorems. Completeness ensures sequences converge; inner products enable projections and weak convergence.</p>
                
                <p><strong>Iteration design is an art</strong>: There's no universally best scheme. Picard is simplest but most restrictive. Mann adds flexibility for nonexpansive operators. Ishikawa, Noor, and S-iteration handle progressively more general operator classes but at increased per-iteration cost. The practitioner must balance theoretical guarantees against computational resources.</p>
                
                <p><strong>Parameters control behavior</strong>: Proper choice of \( \{\alpha_n\}, \{\beta_n\} \) sequences is crucial. Too aggressive (large \( \alpha \)) risks instability; too conservative (small \( \alpha \)) wastes iterations. Classical choices like \( \alpha_n = 1/(n+1) \) satisfy theoretical conditions but may be suboptimal in practice.</p>
                
                <p><strong>Applications are ubiquitous</strong>: From solving nonlinear equations to computing Nash equilibria, from image reconstruction to training neural networks, fixed point problems appear throughout applied mathematics and computational science. The methods we've studied provide a unified framework for these diverse applications.</p>
                
                <p><strong>Looking forward</strong>: Modern research focuses on acceleration (inertial methods), adaptation (choosing parameters automatically), and scale (distributed and stochastic algorithms for big data). The interplay between classical fixed point theory and contemporary machine learning opens exciting new directions.</p>
                
                <p>
                    The beauty of fixed point theory lies in its blend of deep mathematical structure and practical utility. Whether you're a graduate student beginning research or a practitioner solving real-world problems, these iteration schemes provide powerful, well-understood tools for finding solutions to challenging nonlinear problems. May your iterations converge swiftly and your fixed points be stable!
                </p>
            </div>

            <!-- Acknowledgments -->
            <div class="math-content">
                <h3>Acknowledgments</h3>
                <p>
                    This exposition draws on the rich literature of fixed point theory, from Banach's 1922 work through contemporary research. I thank the mathematical community for decades of insight into these beautiful methods.
                </p>
            </div>

             <!-- References -->
            <div class="math-content" id="section-references">
                <h3>References</h3>
                <div class="reference-list">
                    <div class="reference-item">
                        <span class="reference-citation">[banach1922]</span>
                        <p class="reference-content">
                            S. Banach, <em>Sur les opÃ©rations dans les ensembles abstraits et leur application aux Ã©quations intÃ©grales</em>, Fund. Math. <strong>3</strong> (1922), 133â€“181.
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[mann1953]</span>
                        <p class="reference-content">
                            W.R. Mann, <em>Mean value methods in iteration</em>, Proc. Amer. Math. Soc. <strong>4</strong> (1953), 506â€“510.
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[ishikawa1974]</span>
                        <p class="reference-content">
                            S. Ishikawa, <em>Fixed points by a new iteration method</em>, Proc. Amer. Math. Soc. <strong>44</strong> (1974), 147â€“150.
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[noor2000]</span>
                        <p class="reference-content">
                            M.A. Noor, <em>New approximation schemes for general variational inequalities</em>, J. Math. Anal. Appl. <strong>251</strong> (2000), 217â€“229.
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[agarwal2007]</span>
                        <p class="reference-content">
                            R.P. Agarwal, D. O'Regan, and D.R. Sahu, <em>Iterative construction of fixed points of nearly asymptotically nonexpansive mappings</em>, J. Nonlinear Convex Anal. <strong>8</strong> (2007), 61â€“79.
                        </p>
                    </div>

                    <!-- Continue with all other references... -->
                    <div class="reference-item">
                        <span class="reference-citation">[bai2021]</span>
                        <p class="reference-content">
                            S. Bai, J. Z. Kolter, and V. Koltun, <em>Deep equilibrium models</em>, in: Advances in Neural Information Processing Systems (2019), pp. 688-699.
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[chidume2009]</span>
                        <p class="reference-content">
                            C.E. Chidume, <em>Geometric Properties of Banach Spaces and Nonlinear Iterations</em>, Lecture Notes in Mathematics <strong>1965</strong>, Springer, London, 2009.
                        </p>
                    </div>
                </div>
            </div>

         <!-- Notation Reference -->
            <div class="math-content">
                <h3>Notation Reference</h3>
                
                <div class="table-container">
                    <table class="notation-table">
                        <caption>Mathematical Notation Used Throughout</caption>
                        <thead>
                            <tr>
                                <th>Symbol</th>
                                <th>Meaning</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr><td>\(X, H\)</td><td>Normed linear space, Hilbert space</td></tr>
                            <tr><td>\(\|\cdot\|\)</td><td>Norm on a normed space</td></tr>
                            <tr><td>\(\langle \cdot, \cdot \rangle\)</td><td>Inner product in Hilbert space</td></tr>
                            <tr><td>\(d(x, y)\)</td><td>Metric/distance function</td></tr>
                            <tr><td>\(T: X \to X\)</td><td>Operator/mapping from \(X\) to itself</td></tr>
                            <tr><td>\(\text{Fix}(T)\)</td><td>Set of fixed points of \(T\): \(\{x : Tx = x\}\)</td></tr>
                            <tr><td>\(x_n \to x\)</td><td>Strong convergence: \(\|x_n - x\| \to 0\)</td></tr>
                            <tr><td>\(x_n \rightharpoonup x\)</td><td>Weak convergence</td></tr>
                            <tr><td>\(P_C(x)\)</td><td>Metric projection of \(x\) onto set \(C\)</td></tr>
                            <tr><td>\(L\)</td><td>Lipschitz constant</td></tr>
                            <tr><td>\(\alpha_n, \beta_n, \gamma_n\)</td><td>Parameter sequences in iterations</td></tr>
                            <tr><td>\(J(x)\)</td><td>Normalized duality mapping</td></tr>
                            <tr><td>\(\rho(A)\)</td><td>Spectral radius of operator \(A\)</td></tr>
                            <tr><td>\(I\)</td><td>Identity operator</td></tr>
                            <tr><td>\(A^*\)</td><td>Adjoint of operator \(A\)</td></tr>
                        </tbody>
                    </table>
                </div>
            </div>

            <!-- Implementation Guide -->
            <div class="math-content">
                <h3>Implementation Guide</h3>
                
                <h4>Python Implementation: Mann Iteration</h4>
                
                <div class="code-container">
                    <pre><code class="language-python">import numpy as np

def mann_iteration(T, x0, alpha_seq, tol=1e-6, max_iter=1000):
    """
    Mann iteration for finding fixed points.
    
    Parameters:
    -----------
    T : callable
        Operator T: R^n -> R^n
    x0 : ndarray
        Initial point
    alpha_seq : callable or float
        Parameter sequence alpha_n or constant
    tol : float
        Convergence tolerance
    max_iter : int
        Maximum iterations
    
    Returns:
    --------
    x : ndarray
        Approximate fixed point
    history : list
        Convergence history
    """
    x = x0.copy()
    history = [x.copy()]
    
    for n in range(max_iter):
        # Get alpha_n
        alpha_n = alpha_seq(n) if callable(alpha_seq) else alpha_seq
        
        # Compute T(x_n)
        Tx = T(x)
        
        # Mann update
        x_new = (1 - alpha_n) * x + alpha_n * Tx
        
        # Check convergence
        if np.linalg.norm(x_new - x) < tol:
            print(f"Converged in {n+1} iterations")
            return x_new, history
        
        x = x_new
        history.append(x.copy())
    
    print(f"Warning: Maximum iterations {max_iter} reached")
    return x, history

# Example usage
def T(x):
    """Example: T(x) = (x + 2/x) / 2 for sqrt(2)"""
    return 0.5 * (x + 2.0 / x)

x0 = np.array([1.0])
alpha = 0.5  # constant parameter
x_star, hist = mann_iteration(T, x0, alpha)
print(f"Fixed point: {x_star[0]:.10f}")
print(f"sqrt(2) = {np.sqrt(2):.10f}")</code></pre>
                </div>

                <h4>MATLAB Implementation: Ishikawa Iteration</h4>
                
                <div class="code-container">
                    <pre><code class="language-matlab">function [x, history] = ishikawa_iteration(T, x0, alpha_seq, ...
    beta_seq, tol, max_iter)
% ISHIKAWA_ITERATION  Two-step iteration for fixed points
%
% Inputs:
%   T - function handle for operator
%   x0 - initial point (column vector)
%   alpha_seq, beta_seq - parameter sequences (function handles)
%   tol - convergence tolerance
%   max_iter - maximum iterations
%
% Outputs:
%   x - approximate fixed point
%   history - struct with iteration history

x = x0;
history.x = x0;
history.error = [];

for n = 1:max_iter
    % Get parameters
    alpha_n = alpha_seq(n);
    beta_n = beta_seq(n);
    
    % First step: y_n = (1-beta_n)*x_n + beta_n*T(x_n)
    Tx = T(x);
    y = (1 - beta_n) * x + beta_n * Tx;
    
    % Second step: x_{n+1} = (1-alpha_n)*x_n + alpha_n*T(y_n)
    Ty = T(y);
    x_new = (1 - alpha_n) * x + alpha_n * Ty;
    
    % Check convergence
    err = norm(x_new - x);
    history.error = [history.error; err];
    history.x = [history.x, x_new];
    
    if err < tol
        fprintf('Converged in %d iterations\n', n);
        x = x_new;
        return;
    end
    
    x = x_new;
end

fprintf('Warning: Maximum iterations reached\n');
end</code></pre>
                </div>

                <h4>Practical Tips for Implementation</h4>
                
                <ol>
                    <li><strong>Numerical stability</strong>: When computing norms, use relative error \(\|x_{n+1} - x_n\| / \max(\|x_n\|, 1)\) to avoid issues near zero.</li>
                    
                    <li><strong>Parameter adaptation</strong>: If convergence is slow, try adapting \(\alpha_n\) based on progress:
                        <div class="code-container">
                            <pre><code>if norm(x_new - x) > norm(x - x_prev):
    alpha_n = alpha_n * 0.8  # reduce step size</code></pre>
                        </div>
                    </li>
                    
                    <li><strong>Monitoring convergence</strong>: Track both \(\|x_n - x_{n-1}\|\) and \(\|Tx_n - x_n\|\) (residual). The latter is often more reliable.</li>
                    
                    <li><strong>Projection onto convex sets</strong>: For common sets:
                        <ul>
                            <li>Hyperplane \(\{x : \langle a, x \rangle = b\}\): \(P(x) = x - \frac{\langle a, x \rangle - b}{\|a\|^2} a\)</li>
                            <li>Ball \(\{x : \|x - c\| \leq r\}\): \(P(x) = c + r \frac{x - c}{\max(\|x - c\|, r)}\)</li>
                            <li>Halfspace \(\{x : \langle a, x \rangle \leq b\}\): \(P(x) = x - \max(0, \langle a, x \rangle - b) \frac{a}{\|a\|^2}\)</li>
                        </ul>
                    </li>
                    
                    <li><strong>Parallelization</strong>: For parallel schemes with multiple operators, evaluate all \(T_i(x_n)\) simultaneously using parallel computing frameworks (e.g., MATLAB's <code>parfor</code>, Python's <code>multiprocessing</code>).</li>
                    
                    <li><strong>Warm starting</strong>: When solving a sequence of related problems, use the solution of problem \(k\) as initial guess for problem \(k+1\).</li>
                </ol>
            </div>

            <!-- Exercises and Problems -->
            <div class="math-content">
                <h3>Exercises and Problems</h3>
                
                <h4>Basic Exercises</h4>
                
                <ol>
                    <li>Show that the operator \(T: [0, 1] \to [0, 1]\) defined by \(T(x) = x^2\) has two fixed points. Is \(T\) a contraction? Apply Picard iteration starting from \(x_0 = 0.5\) and \(x_0 = 1.5\). What do you observe?</li>
                    
                    <li>For \(T(x) = \frac{1}{2}(x + \frac{a}{x})\) on \((0, \infty)\) with \(a > 0\), show that:
                        <ul>
                            <li>(a) \(T\) has a unique fixed point at \(x^* = \sqrt{a}\)</li>
                            <li>(b) \(|T'(x^*)| = 0\), implying quadratic convergence</li>
                            <li>(c) Picard iteration converges from any \(x_0 > 0\)</li>
                        </ul>
                    </li>
                    
                    <li>Implement Mann iteration for \(T: \mathbb{R}^2 \to \mathbb{R}^2\) given by \(T(x, y) = (y/2, x/2)\). This operator has unique fixed point \((0, 0)\). Compare convergence for different \(\alpha_n\) choices.</li>
                    
                    <li>Prove that if \(T: H \to H\) is firmly nonexpansive, then \(I - T\) is also firmly nonexpansive.</li>
                </ol>

                <h4>Intermediate Problems</h4>
                
                <ol>
                    <li>Consider finding the intersection of two circles in \(\mathbb{R}^2\):
                        <div class="equation">
                            <div class="equation-content">$$C_1 = \{(x,y) : x^2 + y^2 = 1\}, \quad C_2 = \{(x,y) : (x-1)^2 + y^2 = 1\}.$$</div>
                            <div class="equation-number">(27)</div>
                        </div>
                        <ul>
                            <li>(a) Implement alternating projections</li>
                            <li>(b) Compare with parallel projections</li>
                            <li>(c) Verify convergence to \((0.5, \pm \sqrt{3}/2)\)</li>
                        </ul>
                    </li>
                    
                    <li>For the split feasibility problem with \(C = [0, 1]^2\), \(Q = \{y \in \mathbb{R}^2 : y_1 + y_2 = 1\}\), and \(A = I\) (identity), implement the algorithm and find a solution.</li>
                    
                    <li>Show that Ishikawa iteration with \(\beta_n = 0\) reduces to Mann iteration. Experimentally determine when Ishikawa outperforms Mann (hint: consider Lipschitz pseudocontractive operators).</li>
                </ol>

                <h4>Advanced Research Problems</h4>
                
                <ol>
                    <li><strong>Optimal parameter selection</strong>: For a given class of nonexpansive operators, determine the parameter sequence \(\{\alpha_n\}\) that minimizes expected number of iterations to reach tolerance \(\epsilon\).</li>
                    
                    <li><strong>Stochastic extension</strong>: Extend Mann iteration to the setting where \(T_\omega\) is a random operator (e.g., stochastic gradient). Prove almost-sure convergence under appropriate conditions.</li>
                    
                    <li><strong>Acceleration</strong>: Combine Ishikawa iteration with Nesterov-type momentum. Can you achieve faster than linear convergence for certain operator classes?</li>
                    
                    <li><strong>Infinite families</strong>: Extend cyclic iteration to countably infinite families \(\{T_i\}_{i=1}^\infty\). Under what conditions does cycling through operators yield convergence?</li>
                </ol>
            </div>

            <!-- Glossary -->
            <div class="math-content">
                <h3>Glossary of Key Terms</h3>
                
                <dl class="glossary-list">
                    <dt>Banach Space</dt>
                    <dd>A complete normed vector space where every Cauchy sequence converges.</dd>
                    
                    <dt>Contraction Mapping</dt>
                    <dd>An operator \(T\) satisfying \(d(Tx, Ty) \leq Ld(x,y)\) for some \(L < 1\).</dd>
                    
                    <dt>Demicompact Operator</dt>
                    <dd>An operator for which bounded sequences with \(\|x_n - Tx_n\| \to 0\) have convergent subsequences.</dd>
                    
                    <dt>Firmly Nonexpansive</dt>
                    <dd>An operator satisfying \(\|Tx-Ty\|^2 + \|(I-T)x-(I-T)y\|^2 \leq \|x-y\|^2\).</dd>
                    
                    <dt>Fixed Point</dt>
                    <dd>A point \(x^*\) satisfying \(T(x^*) = x^*\).</dd>
                    
                    <dt>Hilbert Space</dt>
                    <dd>A complete inner product space.</dd>
                    
                    <dt>Lipschitz Continuous</dt>
                    <dd>An operator satisfying \(\|Tx - Ty\| \leq L\|x - y\|\) for some \(L \geq 0\).</dd>
                    
                    <dt>Nonexpansive Mapping</dt>
                    <dd>A Lipschitz continuous operator with \(L = 1\).</dd>
                    
                    <dt>Pseudocontractive</dt>
                    <dd>An operator satisfying \(\langle Tx-Ty, x-y \rangle \leq \|x-y\|^2\) in Hilbert spaces.</dd>
                    
                    <dt>Strong Convergence</dt>
                    <p>Convergence in norm: \(\|x_n - x\| \to 0\).</p>
                    
                    <dt>Weak Convergence</dt>
                    <dd>Convergence of inner products: \(\langle x_n, y \rangle \to \langle x, y \rangle\) for all \(y\).</dd>
                </dl>
            </div>

            <!-- Historical Notes -->
            <div class="math-content">
                <h3>Historical Notes</h3>
                
                <p>The development of fixed point iteration methods spans over a century:</p>
                
                <ul class="historical-timeline">
                    <li><strong>1890</strong>: Ã‰mile Picard uses successive approximations to prove existence theorems for ODEs, introducing what we now call Picard iteration.</li>
                    
                    <li><strong>1912</strong>: L.E.J. Brouwer proves his fixed point theorem for continuous functions on compact convex sets, a cornerstone of topology.</li>
                    
                    <li><strong>1922</strong>: Stefan Banach publishes the contraction mapping principle in his thesis, revolutionizing analysis.</li>
                    
                    <li><strong>1953</strong>: W.R. Mann introduces averaged iteration to handle nonexpansive operators, opening fixed point theory to a much wider class of problems.</li>
                    
                    <li><strong>1965</strong>: Felix Browder and others develop the theory of nonexpansive mappings in Banach spaces, proving fundamental convergence results.</li>
                    
                    <li><strong>1967</strong>: ZdzisÅ‚aw Opial proves weak convergence results using his ingenious condition, now standard in Hilbert space theory.</li>
                    
                    <li><strong>1974</strong>: Shiro Ishikawa extends Mann iteration to two-step form, enabling convergence for Lipschitz pseudocontractive operators.</li>
                    
                    <li><strong>1980s-1990s</strong>: Reich, Xu, and others develop sophisticated convergence theory, parameter conditions, and applications to variational inequalities.</li>
                    
                    <li><strong>2000s</strong>: Noor and others introduce multi-step iterations; hybrid and shrinking projection methods guarantee strong convergence.</li>
                    
                    <li><strong>2007</strong>: Agarwal, O'Regan, and Sahu introduce S-iteration as simpler alternative to Ishikawa.</li>
                    
                    <li><strong>2010s-present</strong>: Applications to machine learning (deep equilibrium models), large-scale optimization (proximal algorithms), and image processing drive renewed interest and new algorithmic variants.</li>
                </ul>
                
                <div class="quote">
                    <p><em>"The principle of contraction mappings is one of the great unifying ideas of analysis. It appears in different guises and different levels of abstraction and generality throughout mathematics."</em></p>
                    <p class="quote-author">â€” John L. Kelley</p>
                </div>
            </div>
        </div>
    </div>
</div>
</div>
              </div>

                <!-- Project Card 2: Example - Add More Expositions Here -->
<div class="main-container">
        <!-- Project 2 -->
    <div class="project-card">
    <div class="project-header" onclick="toggleContent('mfa-network')">
        <div class="project-meta">
            <span class="meta-item">
                <i class="fas fa-calendar-alt"></i>
                Added: March 15, 2025
            </span>
            <span class="meta-item">
                <i class="fas fa-clock"></i>
                2hr 25 min read
            </span>
            <span class="meta-item">
                <i class="fas fa-tag"></i>
                Network Science & Dynamical Systems
            </span>
        </div>
        <h2 class="project-title">Mean Field Approximation in Network Science: Theory, Applications, and Limitations</h2>
        <p class="project-subtitle">Author: Clinton Oluranran Kayoh</p>
        <div class="expand-btn" id="btn-mfa-network">
            <i class="fas fa-plus"></i>
        </div>
    </div>
    <div class="project-content" id="content-mfa-network">
        <div class="content-inner">
            <!-- Abstract -->
            <div class="math-content">
                <h3>Abstract</h3>
                <p>
                    Mean field approximation (MFA) is a powerful analytical technique borrowed from statistical physics that has become indispensable in network science. By replacing complex heterogeneous interactions with effective average fields, MFA enables tractable analysis of dynamical processes on networks, including epidemic spreading, opinion formation, and synchronization phenomena. This exposition provides a comprehensive introduction to MFA in network science, covering its mathematical foundations, key applications, and fundamental limitations. We derive mean field equations from first principles, discuss extensions to heterogeneous networks through degree-based approximations, and examine when MFA predictions deviate from reality. This work is intended for graduate students and researchers seeking a rigorous yet accessible entry point into mean field methods for complex networks.
                </p>
            </div>

            <!-- Introduction -->
            <div class="math-content">
                <h3>1. Introduction</h3>
                
                <h4>1.1 Historical Context and Motivation</h4>
                <p>
                    Mean field approximation (MFA) originated in statistical physics as a method to study systems with many interacting particles [Kadanoff2000]. The core insight is elegant: instead of tracking all microscopic interactions between individual components, we approximate the effect of all neighbors on a given component by an average or "mean" field. This dramatic simplification transforms intractable many-body problems into manageable effective single-body problems.
                </p>
                <p>
                    In network science, nodes represent entities (individuals, neurons, computers) and edges represent interactions (contacts, synapses, connections). Dynamical processes on networks such as disease transmission, information diffusion, or neuronal firing; involve local interactions that can exhibit rich collective behavior [Newman2018, BarratBook2008]. However, the combinatorial complexity of heterogeneous network structures makes exact analysis nearly impossible for realistic systems. Here, MFA provides an analytical framework that captures essential features while remaining mathematically tractable.
                </p>

                <h4>1.2 Why MFA in Network Science?</h4>
                <p>The relevance of MFA in network science stems from three fundamental challenges:</p>
                <ul>
                    <li><strong>High dimensionality</strong>: Networks with \(N\) nodes can have states described by \(2^N\) or more configurations, making exact enumeration infeasible.</li>
                    <li><strong>Structural heterogeneity</strong>: Real networks exhibit diverse topological features degree distributions, clustering, community structure that influence dynamics in non-trivial ways.</li>
                    <li><strong>Nonlinear dynamics</strong>: Many network processes involve nonlinear interactions (e.g., threshold dynamics, multiplicative spreading), complicating analytical treatment.</li>
                </ul>
                <p>
                    MFA addresses these challenges by assuming that each node experiences an average effect from its neighbors, effectively decoupling individual node dynamics. This homogeneous mixing assumption, while approximate, often captures phase transitions, critical thresholds, and steady-state behavior remarkably well [PastorSatorras2001].
                </p>

                <h4>1.3 Intuitive Picture</h4>
                <p>
                    Consider an epidemic spreading on a social network. Each susceptible individual's infection probability depends on their specific infected contacts. Tracking all possible infection pathways quickly becomes intractable. MFA simplifies this by asking: "What if each susceptible experiences an average infection pressure from the population?" This replaces \(N\) coupled stochastic processes with a single deterministic equation describing the average infected fraction over time.
                </p>
                <p>
                    While this approximation neglects local fluctuations and specific network structure details, it provides invaluable insight into global behavior, particularly near critical transitions where mean field predictions often become asymptotically exact in the thermodynamic limit [Dorogovtsev2008].
                </p>
            </div>

            <!-- Mathematical Foundation -->
            <div class="math-content">
                <h3>2. Mathematical Foundation</h3>
                
                <h4>2.1 Formal Definition</h4>
                <div class="definition">
                    <p class="definition-title">Definition 1 (Mean Field Approximation)</p>
                    <p>
                        For a network system with \(N\) nodes where node \(i\) has state \(x_i(t)\) evolving according to:
                    <div class="equation">
                    <div class="equation-content">
                        $$\frac{dx_i}{dt} = f_i\left(x_i, \{x_j\}_{j \in \mathcal{N}_i}\right),$$
                    </div>
                    <div class="equation-number">(1)</div>
                </div>
                        where \(\mathcal{N}_i\) denotes the neighborhood of node \(i\), the mean field approximation replaces the heterogeneous neighbor states with their average:
                        <div class="equation">
                    <div class="equation-content">
                        $$\frac{dx_i}{dt} \approx f_i\left(x_i, \langle x \rangle\right),$$
                        </div>
                    <div class="equation-number">(2)</div>
                </div>
                        where \(\langle x \rangle = \frac{1}{N}\sum_{j=1}^N x_j\) is the mean field.
                    </p>
                </div>
                <p>
                    In the thermodynamic limit (\(N \to \infty\)), we often assume all nodes behave identically under MFA, yielding a single equation:
                    <div class="equation">
                    <div class="equation-content">
                    $$\frac{d\langle x \rangle}{dt} = f(\langle x \rangle).$$
                    </div>
                    <div class="equation-number">(3)</div>
                </div>
                </p>

                <h4>2.2 The Homogeneous Mixing Assumption</h4>
                <p>The validity of MFA rests on the <em>homogeneous mixing assumption</em>: each node interacts with a representative sample of the population, making specific neighbor identities irrelevant. Mathematically, this requires:</p>
                <ul>
                    <li><strong>Weak correlations</strong>: Node states are approximately independent, i.e., \(\langle x_i x_j \rangle \approx \langle x_i \rangle \langle x_j \rangle\).</li>
                    <li><strong>Large coordination number</strong>: Each node has many neighbors, so fluctuations average out.</li>
                    <li><strong>Structural uniformity</strong>: The network is sufficiently random that local structure doesn't dominate.</li>
                </ul>
                <p>
                    These conditions are satisfied in complete graphs (all-to-all coupling) and well-mixed populations, but are violated in networks with strong clustering, degree correlations, or spatial structure [Newman2002Structure].
                </p>

                Here is the rewritten passage with the display equations enclosed in the requested HTML format:

<h4>2.3 Derivation from First Principles</h4>
<p>
    Consider a stochastic process where each node \(i\) transitions between states with rates depending on neighbor states. Let \(P(\{x_i\})\) be the probability distribution over all network configurations. The master equation governing this distribution is:
</p>
<div class="equation">
    <div class="equation-content">$$\frac{dP(\{x_i\})}{dt} = \sum_{i=1}^N \left[W_{i}^{-}P(\ldots, x_i', \ldots) - W_i^{+}P(\{x_i\})\right],$$</div>
    <div class="equation-number">(4)</div>
</div>
<p>
    where \(W_i^{\pm}\) are transition rates depending on \(\{x_j\}_{j \in \mathcal{N}_i}\).
</p>
<p>
    To derive the mean-field approximation (MFA), we make a factorization ansatz:
</p>
<div class="equation">
    <div class="equation-content">$$P(\{x_i\}) \approx \prod_{i=1}^N P_i(x_i),$$</div>
    <div class="equation-number">(5)</div>
</div>
<p>
    assuming independence of node states. The evolution of single-node marginals \(P_i(x_i)\) then becomes:
</p>
<div class="equation">
    <div class="equation-content">$$\frac{dP_i(x_i)}{dt} = \sum_{x_i'} \left[W_i^{-}(x_i', \{\langle x_j \rangle\})P_i(x_i') - W_i^{+}(x_i, \{\langle x_j \rangle\})P_i(x_i)\right].$$</div>
    <div class="equation-number">(6)</div>
</div>
<p>
    For deterministic dynamics, we track mean values \(\rho_i = \langle x_i \rangle\):
</p>
<div class="equation">
    <div class="equation-content">$$\frac{d\rho_i}{dt} = \int x_i \frac{dP_i(x_i)}{dt} dx_i \approx f_i(\rho_i, \{\rho_j\}_{j \in \mathcal{N}_i}).$$</div>
    <div class="equation-number">(7)</div>
</div>
                <p>
                    Under homogeneous mixing, \(\rho_i \approx \rho\) for all \(i\), yielding the basic MFA equation.
                </p>

                <div class="remark">
                    <p class="remark-title">Remark</p>
                    <p>The factorization ansatz is the crucial approximation. It neglects correlations that arise from network structure, leading to MFA's primary limitation.</p>
                </div>

                <h4>2.4 Validity Conditions</h4>
                <p>MFA becomes exact in several limiting cases:</p>
                <ul>
                    <li><strong>Infinite network limit</strong>: As \(N \to \infty\) with fixed average degree, fluctuations vanish by the law of large numbers [Dorogovtsev2008].</li>
                    <li><strong>Complete graph</strong>: When every node connects to every other, mean field is exact at finite \(N\).</li>
                    <li><strong>Annealed networks</strong>: If connections rewire faster than dynamics evolve, structure effectively averages out [PastorSatorras2015].</li>
                </ul>
                <p>Conversely, MFA fails when:</p>
                <ul>
                    <li>Networks have high clustering coefficients \(C \gg 0\)</li>
                    <li>Strong degree-degree correlations exist</li>
                    <li>Dynamics occur on timescales comparable to structural evolution</li>
                    <li>Finite-size effects dominate (\(N\) small or dynamics localized)</li>
                </ul>
            </div>

            <!-- Applications -->
            <div class="math-content">
                <h3>3. Applications in Network Dynamics</h3>
                
                <h4>3.1 Epidemic Spreading Models</h4>

<h5>SIS Model</h5>
<p>
    The Susceptible-Infected-Susceptible (SIS) model describes diseases where recovery confers no immunity [PastorSatorras2001]. Each node is either susceptible (\(S\)) or infected (\(I\)). The microscopic dynamics are:
</p>
<div class="equation">
    <div class="equation-content">
        $$\begin{aligned}
        S + I &\xrightarrow{\beta} I + I \quad \text{(infection)}, \\
        I &\xrightarrow{\gamma} S \quad \text{(recovery)}.
        \end{aligned}$$
    </div>
    <div class="equation-number">(8)</div>
</div>
<p>
    Let \(\rho_i(t)\) be the probability node \(i\) is infected. The exact equation is:
</p>
<div class="equation">
    <div class="equation-content">$$\frac{d\rho_i}{dt} = -\gamma \rho_i + \beta (1-\rho_i) \sum_{j \in \mathcal{N}_i} \rho_j.$$</div>
    <div class="equation-number">(9)</div>
</div>
<p>
    <strong>MFA Derivation</strong>: Assume \(\rho_i \approx \rho\) (homogeneous infection) and replace the sum by average degree \(\langle k \rangle\) times mean infection \(\rho\):
</p>
<div class="equation">
    <div class="equation-content">$$\frac{d\rho}{dt} = -\gamma \rho + \beta \langle k \rangle (1-\rho) \rho.$$</div>
    <div class="equation-number">(10)</div>
</div>
<p>
    Setting \(d\rho/dt = 0\), we find steady states:
</p>
<div class="equation">
    <div class="equation-content">$$\rho^* = 0 \quad \text{or} \quad \rho^* = 1 - \frac{\gamma}{\beta \langle k \rangle}.$$</div>
    <div class="equation-number">(11)</div>
</div>
<p>
    The epidemic threshold is:
</p>
<div class="equation">
    <div class="equation-content">$$\lambda_c = \frac{\beta}{\gamma} = \frac{1}{\langle k \rangle}.$$</div>
    <div class="equation-number">(12)</div>
</div>
<p>
    For \(\lambda < \lambda_c\), the disease dies out; for \(\lambda > \lambda_c\), an endemic state persists.
</p>
<p>
    <strong>Predictions vs. Simulations</strong>: On ErdÅ‘s-RÃ©nyi random graphs, MFA predicts the threshold accurately. However, on scale-free networks with \(P(k) \sim k^{-\gamma}\) (\(\gamma < 3\)), MFA predicts \(\lambda_c \to 0\) as \(N \to \infty\) (vanishing threshold) [also see Figure 1 and 2], which simulations confirm [PastorSatorras2001].
</p>

<h5>SIR Model</h5>
<p>
    The Susceptible-Infected-Recovered (SIR) model includes permanent immunity:
</p>
<div class="equation">
    <div class="equation-content">
        $$\begin{aligned}
        S + I &\xrightarrow{\beta} I + I, \\
        I &\xrightarrow{\gamma} R.
        \end{aligned}$$
    </div>
    <div class="equation-number">(13)</div>
</div>
<p>
    Let \(s(t), i(t), r(t)\) be fractions of \(S, I, R\) nodes. MFA yields:
</p>
<div class="equation">
    <div class="equation-content">
        $$\begin{aligned}
        \frac{ds}{dt} &= -\beta \langle k \rangle s i, \\
        \frac{di}{dt} &= \beta \langle k \rangle s i - \gamma i, \\
        \frac{dr}{dt} &= \gamma i.
        \end{aligned}$$
    </div>
    <div class="equation-number">(14)</div>
</div>
<p>
    The basic reproduction number is:
</p>
<div class="equation">
    <div class="equation-content">$$R_0 = \frac{\beta \langle k \rangle}{\gamma}.$$</div>
    <div class="equation-number">(15)</div>
</div>
<p>
    An epidemic outbreak occurs if \(R_0 > 1\), determining whether a significant fraction becomes infected.
</p>

<h4>3.2 Opinion Dynamics: Voter Model</h4>
<p>
    In the voter model, agents adopt opinions of randomly chosen neighbors [Sood2005]. Each node holds opinion \(\sigma_i \in \{-1, +1\}\). At rate \(1\), node \(i\) copies a neighbor's opinion.
</p>
<p>
    Let \(m = \frac{1}{N}\sum_i \sigma_i\) be the magnetization. Under MFA:
</p>
<div class="equation">
    <div class="equation-content">$$\frac{dm}{dt} = 0,$$</div>
    <div class="equation-number">(16)</div>
</div>
<p>
    indicating \(m\) is conserved on average, this result is quite trivial. However, fluctuations drive consensus formation. Including fluctuations via Langevin dynamics:
</p>
<div class="equation">
    <div class="equation-content">$$\frac{dm}{dt} = \eta(t),$$</div>
    <div class="equation-number">(17)</div>
</div>
<p>
    where \(\langle \eta(t) \rangle = 0\), \(\langle \eta(t)\eta(t') \rangle = \frac{2}{N}\delta(t-t')\). This predicts consensus time \(\tau \sim N\) on complete graphs, matching simulations [Redner2019].
</p>
<div class="remark">
    <p class="remark-title">Remark</p>
    <p>MFA captures mean behavior but misses fluctuation-driven dynamics crucial in finite systems.</p>
</div>

<h4>3.3 Synchronization: Kuramoto Model</h4>
<p>
    The Kuramoto model describes coupled oscillators [Kuramoto1975, Strogatz2000]:
</p>
<div class="equation">
    <div class="equation-content">$$\frac{d\theta_i}{dt} = \omega_i + \frac{K}{N} \sum_{j=1}^N \sin(\theta_j - \theta_i),$$</div>
    <div class="equation-number">(18)</div>
</div>
<p>
    where \(\theta_i\) is the phase of oscillator \(i\), \(\omega_i\) its natural frequency, and \(K\) the coupling strength.
</p>
<p>
    Define the order parameter:
</p>
<div class="equation">
    <div class="equation-content">$$r e^{i\psi} = \frac{1}{N}\sum_{j=1}^N e^{i\theta_j},$$</div>
    <div class="equation-number">(19)</div>
</div>
<p>
    where \(r \in [0,1]\) measures synchronization. Under MFA, assuming \(\omega_i\) drawn from \(g(\omega)\):
</p>
<div class="equation">
    <div class="equation-content">$$\frac{d\theta_i}{dt} = \omega_i + Kr\sin(\psi - \theta_i).$$</div>
    <div class="equation-number">(20)</div>
</div>
<p>
    For unimodal symmetric \(g(\omega)\), there exists a critical coupling:
</p>
<div class="equation">
    <div class="equation-content">$$K_c = \frac{2}{\pi g(0)},$$</div>
    <div class="equation-number">(21)</div>
</div>
<p>
    below which \(r = 0\) (incoherence) and above which \(r > 0\) (partial synchronization). This transition is continuous (second-order phase transition) [Strogatz2000].
</p>
<p>
    <strong>Network Extension</strong>: On networks with adjacency matrix \(A_{ij}\):
</p>
<div class="equation">
    <div class="equation-content">$$\frac{d\theta_i}{dt} = \omega_i + \frac{K}{k_i} \sum_{j=1}^N A_{ij}\sin(\theta_j - \theta_i).$$</div>
    <div class="equation-number">(22)</div>
</div>
<p>
    MFA predicts \(K_c \propto \langle k \rangle / \langle k^2 \rangle\) for heterogeneous networks, showing high-degree hubs facilitate synchronization [Restrepo2005].
</p>
            <!-- Limitations -->
            <div class="math-content">
                <h3>5. Limitations and Beyond MFA</h3>
                
                <h4>5.1 When MFA Fails</h4>
                
                <h5>Clustering</h5>
                <p>
                    Networks with high clustering coefficient \(C\) exhibit triangles and closed loops, violating tree-like assumptions (see Figure 3). Infections can propagate through multiple short paths, accelerating spread beyond MFA predictions [Newman2003]. For small-world networks (high \(C\), short path length), MFA underestimates infection rates.
                </p>

                <h5>Degree Correlations</h5>
                <p>
                    Assortative mixing (high-degree nodes connect preferentially) or disassortative mixing affects epidemic thresholds. MFA assumes random mixing conditional on degree, missing these correlations [Newman2002Assortative].
                </p>

                <h5>Finite-Size Effects</h5>
                <p>
                    In small networks (\(N \sim 10^2\)), fluctuations dominate. Stochastic simulations show extinctions and re-emergences absent in deterministic MFA. Epidemic thresholds smear into crossover regions rather than sharp transitions.
                </p>

                <h4>5.2 Improved Approximations</h4>
                
                <h5>Pair Approximation</h5>
                <p>
                    Pair approximation (PA) tracks correlations between neighboring nodes [Keeling1999]. For SIS, we evolve densities \(\rho\) (infected nodes) and \(\rho_{SI}\) (S-I pairs):
                    <div class="equation">
          <div class="equation-content">
        $$
        \begin{aligned}
        \frac{d\rho}{dt} &= -\gamma \rho + \beta \rho_{SI}, \\
        \frac{d\rho_{SI}}{dt} &= -\gamma \rho_{SI} + \beta\left[(2\rho_{SI})\frac{\rho_{II}}{\rho} - \rho_{SI}\right] + \text{closure}.
        \end{aligned}
        $$
    </div>
    <div class="equation-number">(23)</div>
</div>
                </p>
                <p>
                    Closure approximates triples (e.g., \(\rho_{SII}\)) in terms of pairs. PA captures clustering effects but requires solving \(O(k^2)\) equations for degree-\(k\) networks.
                </p>

                <h5>Moment Closure Techniques</h5>
                <p>
                    Higher-order moment equations form an infinite hierarchy. Closure schemes (e.g., second-order closure) truncate this by expressing \(\langle x_i x_j x_k \rangle\) via lower moments. Advanced closures balance accuracy and complexity [KeelingBook2008].
                </p>

                <h4>5.3 Comparison with Simulations</h4>
                <p>
                    Figure 1, illustrates SIS prevalence \(\rho^*\) vs. \(\lambda\) on an ErdÅ‘s-RÃ©nyi network (\(C \approx 0\)) and a Watts-Strogatz small-world network (high \(C\)). MFA matches ER simulations closely but underestimates \(\rho^*\) for small-world networks, with PA providing better agreement.
            <div class="figure-container">
            <img src="SIS_prevalence.png" alt="SIS Prevalence" class="figure-image">
            <div class="figure-caption">
                <strong>Figure 1:</strong>  SIS epidemic prevalence \( \rho^* \) versus spreading rate \( \lambda = \beta/\gamma \) for networks with average degree \( \langle k \rangle = 6 \). The solid blue line shows the MFA prediction from Eq.~(12), which matches ErdÅ‘s-RÃ©nyi simulations (red circles) closely. Watts-Strogatz small-world simulations (green squares) show higher prevalence due to clustering effects that MFA neglects. The dashed vertical line marks the epidemic threshold \( \lambda_c = 1/\langle k \rangle \approx 0.167 \).
            </div>
        </div>
                </p>
                <p>
            Figure 2, depicts the phase diagrams for heterogeneous networks, showing MFA's qualitative accuracy for threshold location but quantitative deviations in finite systems.
            <div class="figure-container">
            <img src="Phase_Diagram.png" alt="Phase Diagram" class="figure-image">
            <div class="figure-caption">
                <strong>Figure 2:</strong>  Phase diagram showing epidemic threshold \( \lambda_c\) versus power-law exponent \( \gamma \) for scale-free networks. The horizontal dashed line represents homogeneous MFA (constant threshold). The solid red curve shows degree-based MFA prediction with \(\lambda_c \to 0 \) as \( \gamma \to 2 \). Simulation data (green points) confirms the vanishing threshold for \( \gamma < 3 \). The vertical dashed line at \( \gamma = 3 \) marks the boundary between vanishing (\( \gamma < 3 \)) and finite (\( \gamma > 3 \)) threshold regimes.
            </div>
            </div>
                </p>

                <div class="remark">
                    <p class="remark-title">Remark</p>
                    <p>Despite limitations, MFA remains invaluable for analytical insight, especially identifying scaling laws and critical exponents that simulations alone cannot easily reveal.</p>
                </div>
            </div>

            <!-- Conclusion -->
            <div class="math-content">
                <h3>6. Conclusion</h3>
                <p>
                    Mean field approximation provides a foundational analytical tool for understanding dynamical processes on networks. By replacing heterogeneous local interactions with effective average fields, MFA enables tractable analysis of epidemic spreading, opinion dynamics, synchronization, and beyond. We have seen how MFA:
                </p>
                <ul>
                    <li>Reduces high-dimensional stochastic dynamics to low-dimensional deterministic equations.</li>
                    <li>Identifies critical thresholds and phase transitions (e.g., epidemic thresholds, synchronization transitions).</li>
                    <li>Extends naturally to heterogeneous networks via degree-based stratification.</li>
                    <li>Becomes asymptotically exact on unclustered, locally tree-like networks in the thermodynamic limit.</li>
                </ul>
                <p>
                    However, MFA's limitations neglect of clustering, correlations, and finite-size effects necessitate caution. Improved methods like pair approximation and moment closure offer enhanced accuracy at the cost of complexity.
            <div class="figure-container">
            <img src="Cluster.png" alt="Cluster" class="figure-image">
            <div class="figure-caption">
                <strong>Figure 3:</strong> Comparison of network structures affecting MFA accuracy. (a) A locally tree-like random network where a central node (blue) connects to neighbors (red) that are themselves disconnected, with distinct second-order neighbors (gray). This structure satisfies MFA assumptions. (b) A highly clustered network where the central node's neighbors form triangles (blue edges) and share connections, creating multiple correlated pathways. Clustering violates MFA's independence assumption, leading to prediction errors.
            </div>
            </div>
                </p>

                <h4>Current Research Directions</h4>
                <p>Contemporary research explores:</p>
                <ul>
                    <li><strong>Adaptive networks</strong>: Co-evolution of network structure and node dynamics [Gross2008].</li>
                    <li><strong>Multiplex networks</strong>: MFA on interconnected layers with distinct interaction types [Gomez2013].</li>
                    <li><strong>Non-Markovian processes</strong>: Memory effects violating Markov assumptions [Barrat2013].</li>
                    <li><strong>Machine learning integration</strong>: Data-driven corrections to MFA using neural networks to capture residuals.</li>
                </ul>
                <p>
                    As network science matures, MFA remains a cornerstone balancing mathematical elegance with practical utility, guiding intuition while revealing the rich interplay between structure and dynamics in complex systems.
                </p>
            </div>

            <!-- Notation Summary -->
            <div class="math-content">
                <h3>Notation Summary</h3>
                <table class="notation-table">
                    <thead>
                        <tr>
                            <th>Symbol</th>
                            <th>Meaning</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr><td>\(N\)</td><td>Number of nodes</td></tr>
                        <tr><td>\(x_i(t)\)</td><td>State of node \(i\) at time \(t\)</td></tr>
                        <tr><td>\(\mathcal{N}_i\)</td><td>Neighborhood (neighbors) of node \(i\)</td></tr>
                        <tr><td>\(\langle x \rangle\)</td><td>Mean field (average state)</td></tr>
                        <tr><td>\(k_i\)</td><td>Degree of node \(i\)</td></tr>
                        <tr><td>\(P(k)\)</td><td>Degree distribution</td></tr>
                        <tr><td>\(\langle k \rangle\)</td><td>Average degree: \(\sum_k k P(k)\)</td></tr>
                        <tr><td>\(\langle k^2 \rangle\)</td><td>Second moment of degree: \(\sum_k k^2 P(k)\)</td></tr>
                        <tr><td>\(\rho(t)\)</td><td>Density of infected nodes (SIS/SIR)</td></tr>
                        <tr><td>\(\rho_k(t)\)</td><td>Density of infected nodes with degree \(k\)</td></tr>
                        <tr><td>\(\Theta\)</td><td>Probability a link points to infected node</td></tr>
                        <tr><td>\(\beta\)</td><td>Infection rate</td></tr>
                        <tr><td>\(\gamma\)</td><td>Recovery rate</td></tr>
                        <tr><td>\(\lambda\)</td><td>Effective spreading rate: \(\beta/\gamma\)</td></tr>
                        <tr><td>\(\lambda_c\)</td><td>Epidemic threshold</td></tr>
                        <tr><td>\(C\)</td><td>Clustering coefficient</td></tr>
                        <tr><td>\(r\)</td><td>Order parameter (synchronization)</td></tr>
                    </tbody>
                </table>
            </div>
    <div class="math-content">
                <h3>References</h3>
                
                <div class="reference-list">
                    <div class="reference-item">
                        <span class="reference-citation">[Barabasi1999]</span>
                        <p class="reference-content">
                            A.-L. BarabÃ¡si and R. Albert, <em>Emergence of scaling in random networks</em>, Science <strong>286</strong>, 509â€“512 (1999).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[Barrat2013]</span>
                        <p class="reference-content">
                            A. Barrat, B. FernÃ¡ndez, K. K. Lin, and L.-S. Young, <em>Modeling temporal networks using random itineraries</em>, Phys. Rev. Lett. <strong>110</strong>, 158702 (2013).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[BarratBook2008]</span>
                        <p class="reference-content">
                            A. Barrat, M. BarthÃ©lemy, and A. Vespignani, <em>Dynamical Processes on Complex Networks</em>, Cambridge University Press (2008).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[Dorogovtsev2008]</span>
                        <p class="reference-content">
                            S. N. Dorogovtsev, A. V. Goltsev, and J. F. F. Mendes, <em>Critical phenomena in complex networks</em>, Rev. Mod. Phys. <strong>80</strong>, 1275â€“1335 (2008).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[Gomez2013]</span>
                        <p class="reference-content">
                            S. GÃ³mez, A. DÃ­az-Guilera, J. GÃ³mez-GardeÃ±es, C. J. PÃ©rez-Vicente, Y. Moreno, and A. Arenas, <em>Diffusion dynamics on multiplex networks</em>, Phys. Rev. Lett. <strong>110</strong>, 028701 (2013).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[Gross2008]</span>
                        <p class="reference-content">
                            T. Gross and B. Blasius, <em>Adaptive coevolutionary networks: a review</em>, J. R. Soc. Interface <strong>5</strong>, 259â€“271 (2008).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[Kadanoff2000]</span>
                        <p class="reference-content">
                            L. P. Kadanoff, <em>Statistical Physics: Statics, Dynamics and Renormalization</em>, World Scientific (2000).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[Keeling1999]</span>
                        <p class="reference-content">
                            M. J. Keeling, <em>The effects of local spatial structure on epidemiological invasions</em>, Proc. R. Soc. Lond. B <strong>266</strong>, 859â€“867 (1999).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[KeelingBook2008]</span>
                        <p class="reference-content">
                            M. J. Keeling and P. Rohani, <em>Modeling Infectious Diseases in Humans and Animals</em>, Princeton University Press (2008).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[Kuramoto1975]</span>
                        <p class="reference-content">
                            Y. Kuramoto, <em>Self-entrainment of a population of coupled non-linear oscillators</em>, in <em>International Symposium on Mathematical Problems in Theoretical Physics</em>, Lecture Notes in Physics, Vol. 39, Springer (1975).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[Molloy1995]</span>
                        <p class="reference-content">
                            M. Molloy and B. Reed, <em>A critical point for random graphs with a given degree sequence</em>, Random Struct. Algorithms <strong>6</strong>, 161â€“180 (1995).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[Newman2001]</span>
                        <p class="reference-content">
                            M. E. J. Newman, S. H. Strogatz, and D. J. Watts, <em>Random graphs with arbitrary degree distributions and their applications</em>, Phys. Rev. E <strong>64</strong>, 026118 (2001).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[Newman2002Structure]</span>
                        <p class="reference-content">
                            M. E. J. Newman, <em>Assortative mixing in networks</em>, Phys. Rev. Lett. <strong>89</strong>, 208701 (2002).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[Newman2002Assortative]</span>
                        <p class="reference-content">
                            M. E. J. Newman, <em>Spread of epidemic disease on networks</em>, Phys. Rev. E <strong>66</strong>, 016128 (2002).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[Newman2003]</span>
                        <p class="reference-content">
                            M. E. J. Newman, <em>The structure and function of complex networks</em>, SIAM Review <strong>45</strong>, 167â€“256 (2003).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[Newman2018]</span>
                        <p class="reference-content">
                            M. E. J. Newman, <em>Networks</em>, 2nd edition, Oxford University Press (2018).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[PastorSatorras2001]</span>
                        <p class="reference-content">
                            R. Pastor-Satorras and A. Vespignani, <em>Epidemic spreading in scale-free networks</em>, Phys. Rev. Lett. <strong>86</strong>, 3200â€“3203 (2001).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[PastorSatorras2015]</span>
                        <p class="reference-content">
                            R. Pastor-Satorras, C. Castellano, P. Van Mieghem, and A. Vespignani, <em>Epidemic processes in complex networks</em>, Rev. Mod. Phys. <strong>87</strong>, 925â€“979 (2015).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[Redner2019]</span>
                        <p class="reference-content">
                            S. Redner, <em>Reality-inspired voter models: a mini-review</em>, C. R. Physique <strong>20</strong>, 275â€“292 (2019).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[Restrepo2005]</span>
                        <p class="reference-content">
                            J. G. Restrepo, E. Ott, and B. R. Hunt, <em>Onset of synchronization in large networks of coupled oscillators</em>, Phys. Rev. E <strong>71</strong>, 036151 (2005).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[Sood2005]</span>
                        <p class="reference-content">
                            V. Sood and S. Redner, <em>Voter model on heterogeneous graphs</em>, Phys. Rev. Lett. <strong>94</strong>, 178701 (2005).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[Strogatz2000]</span>
                        <p class="reference-content">
                            S. H. Strogatz, <em>From Kuramoto to Crawford: exploring the onset of synchronization in populations of coupled oscillators</em>, Physica D <strong>143</strong>, 1â€“20 (2000).
                        </p>
                    </div>
                </div> 
        </div>     
        
        <div class="math-content">
        <h3>Appendix</h3>

<div class="math-content">
    <h3>Worked Example 1: SIS Epidemic Threshold on Scale-Free Network</h3>
    <p>
        Consider a scale-free network with degree distribution \(P(k) = C k^{-\gamma}\) for \(k \in [k_{\min}, k_{\max}]\), where \(\gamma = 2.5\). We compute the epidemic threshold using degree-based MFA.
    </p>

    <div class="example-step">
        <p class="step-title"><strong>Step 1</strong>: Normalize the distribution. The normalization constant is:</p>
        <div class="equation">
            <div class="equation-content">
                $$C = \left[\sum_{k=k_{\min}}^{k_{\max}} k^{-\gamma}\right]^{-1} \approx \left[\int_{k_{\min}}^{k_{\max}} k^{-\gamma} dk\right]^{-1}.$$
            </div>
            <div class="equation-number">(24)</div>
        </div>
        <p>For large \(k_{\max}\) and \(\gamma < 3\):</p>
        <div class="equation">
            <div class="equation-content">
                $$C \approx \frac{3-\gamma}{k_{\min}^{3-\gamma}}.$$
            </div>
            <div class="equation-number">(25)</div>
        </div>
    </div>

    <div class="example-step">
        <p class="step-title"><strong>Step 2</strong>: Calculate moments. The first moment is:</p>
        <div class="equation">
            <div class="equation-content">
                $$\langle k \rangle = \sum_{k=k_{\min}}^{k_{\max}} k P(k) = C \sum_{k=k_{\min}}^{k_{\max}} k^{1-\gamma}.$$
            </div>
            <div class="equation-number">(26)</div>
        </div>
        <p>For \(\gamma = 2.5\) and large \(k_{\max}\):</p>
        <div class="equation">
            <div class="equation-content">
                $$\langle k \rangle \approx C \int_{k_{\min}}^{k_{\max}} k^{-1.5} dk = C \cdot \frac{2}{\sqrt{k_{\min}}} \left[1 - \left(\frac{k_{\min}}{k_{\max}}\right)^{0.5}\right].$$
            </div>
            <div class="equation-number">(27)</div>
        </div>
        <p>The second moment is:</p>
        <div class="equation">
            <div class="equation-content">
                $$\langle k^2 \rangle = C \sum_{k=k_{\min}}^{k_{\max}} k^{2-\gamma} = C \int_{k_{\min}}^{k_{\max}} k^{-0.5} dk.$$
            </div>
            <div class="equation-number">(28)</div>
        </div>
        <p>This integral diverges logarithmically as \(k_{\max} \to \infty\):</p>
        <div class="equation">
            <div class="equation-content">
                $$\langle k^2 \rangle \approx 2C\sqrt{k_{\max}}.$$
            </div>
            <div class="equation-number">(29)</div>
        </div>
    </div>

    <div class="example-step">
        <p class="step-title"><strong>Step 3</strong>: Apply threshold formula. From the degree-based MFA threshold formula:</p>
        <div class="equation">
            <div class="equation-content">
                $$\lambda_c = \frac{\langle k \rangle}{\langle k^2 \rangle} \approx \frac{C \cdot 2/\sqrt{k_{\min}}}{2C\sqrt{k_{\max}}} = \frac{1}{\sqrt{k_{\min} k_{\max}}}.$$
            </div>
            <div class="equation-number">(30)</div>
        </div>
        <p>As \(k_{\max}\) grows (larger network size with preserved power-law exponent), \(\lambda_c \to 0\).</p>
    </div>

    <div class="interpretation">
        <p class="interpretation-title"><strong>Interpretation</strong>:</p>
        <p>
            For \(\gamma < 3\), the diverging second moment eliminates the epidemic threshold; diseases can spread at arbitrarily small transmission rates due to highly connected hubs. This contrasts sharply with homogeneous MFA, which predicts \(\lambda_c = 1/\langle k \rangle > 0\).
        </p>
    </div>
</div>

<div class="math-content">
    <h3>Worked Example 2: Kuramoto Model Critical Coupling</h3>
    <p>Consider \(N\) Kuramoto oscillators with natural frequencies drawn from a Lorentzian distribution:</p>
    <div class="equation">
        <div class="equation-content">
            $$g(\omega) = \frac{1}{\pi} \frac{\Delta}{(\omega - \omega_0)^2 + \Delta^2},$$
        </div>
        <div class="equation-number">(31)</div>
    </div>
    <p>where \(\omega_0\) is the center frequency and \(\Delta\) is the half-width.</p>

    <div class="example-step">
        <p class="step-title"><strong>Step 1</strong>: Identify parameters. For the Lorentzian, \(g(0) = \frac{1}{\pi\Delta}\) when centered at \(\omega_0 = 0\).</p>
    </div>

    <div class="example-step">
        <p class="step-title"><strong>Step 2</strong>: Apply MFA threshold formula. From the Kuramoto model analysis:</p>
        <div class="equation">
            <div class="equation-content">
                $$K_c = \frac{2}{\pi g(0)} = \frac{2}{\pi \cdot \frac{1}{\pi\Delta}} = 2\Delta.$$
            </div>
            <div class="equation-number">(32)</div>
        </div>
    </div>

    <div class="example-step">
        <p class="step-title"><strong>Step 3</strong>: Determine order parameter near threshold. Just above \(K_c\), the order parameter grows as:</p>
        <div class="equation">
            <div class="equation-content">
                $$r \approx \sqrt{\frac{16}{\pi^2}\left(\frac{K}{K_c} - 1\right)} \quad \text{for } K \gtrsim K_c.$$
            </div>
            <div class="equation-number">(33)</div>
        </div>
        <p>This square-root scaling is characteristic of a supercritical pitchfork bifurcation (continuous phase transition).</p>
    </div>

    <div class="example-step">
        <p class="step-title"><strong>Numerical Example</strong>: Let \(\Delta = 1.0\). Then \(K_c = 2.0\). For \(K = 2.5\):</p>
        <div class="equation">
            <div class="equation-content">
                $$r \approx \sqrt{\frac{16}{\pi^2}\left(\frac{2.5}{2.0} - 1\right)} = \sqrt{\frac{16}{\pi^2} \cdot 0.25} \approx 0.406.$$
            </div>
            <div class="equation-number">(34)</div>
        </div>
    </div>

    <div class="example-step">
        <p class="step-title"><strong>Extension to Networks</strong>: On a network with degree distribution \(P(k)\), the critical coupling becomes:</p>
        <div class="equation">
            <div class="equation-content">
                $$K_c^{\text{net}} = \frac{2\langle k \rangle}{\pi g(0) \langle k^2 \rangle}.$$
            </div>
            <div class="equation-number">(35)</div>
        </div>
    </div>
</div>

        <p>    For scale-free networks with \(\gamma < 3\), \(\langle k^2 \rangle \to \infty\), yielding \(K_c^{\text{net}} \to 0\), synchronization occurs at infinitesimally small coupling, analogous to the vanishing epidemic threshold. High-degree hubs act as synchronization facilitators. The below figures capture the time evolution and steady state of the kuramoto order parameter:
            <div class="figure-container">
            <img src="Kuramoto.png" alt="Kuramoto" class="figure-image">
            <div class="figure-caption">
                <strong>Figure 4:</strong> (a) Time evolution of the Kuramoto order parameter \( r(t) \) for different coupling strengths \( K \) relative to critical coupling \( K_c = 2.0 \). For \( K < K_c \) (red), the system remains incoherent with \( r \) fluctuating near zero. At \( K \approx K_c \) (orange), critical fluctuations appear. For\(  K > K_c \) (blue and green), the system synchronizes with \( r \) converging to a stable value. (b) Steady-state order parameter \( r^* \) versus coupling strength \( K \). The solid line shows MFA prediction with characteristic square-root scaling \( r^* \propto \sqrt{K - K_c} \) near threshold (supercritical bifurcation). Red points show simulation results confirming MFA predictions.
            </div>
            </div>
                    </p>
                </div>
            </div>
        </div>
        </div>
    </div>
</div>



<div class="main-container">
<div class="project-card">
    <div class="project-header" onclick="toggleContent('grn-project')">
        <div class="project-meta">
            <span class="meta-item">
                <i class="fas fa-calendar-alt"></i>
                Added: November 09, 2024
            </span>
            <span class="meta-item">
                <i class="fas fa-clock"></i>
                1hr 55 min read
            </span>
            <span class="meta-item">
                <i class="fas fa-tag"></i>
                Differential equations & Systems Biology
            </span>
        </div>
        <h2 class="project-title">Continuous Models of Gene Regulatory Networks: A Differential Equation Approach</h2>
        <p class="project-subtitle">Author: Clinton Oluranran Kayoh</p>
        <div class="expand-btn" id="btn-grn-project">
            <i class="fas fa-plus"></i>
        </div>
    </div>
    <div class="project-content" id="content-grn-project">
        <div class="content-inner">
            <!-- Abstract -->
            <div class="math-content">
                <h3>Abstract</h3>
                <p>
                    Gene regulatory networks orchestrate cellular behavior through complex interactions among genes, proteins, and regulatory molecules. Continuous deterministic models using ordinary differential equations provide a powerful framework for understanding the dynamical principles underlying cellular decision-making, oscillations, and homeostasis. This exposition introduces the mathematical foundations of continuous gene regulation models, focusing on Hill function formalism, steady-state analysis, and bifurcation phenomena. We examine three canonical network motifs autoregulation, toggle switches, and repressilators demonstrating how simple regulatory architectures generate bistability, oscillations, and robust control. These models, validated through synthetic biology experiments, reveal design principles that govern natural regulatory circuits and inform rational circuit design.
                </p>
            </div>

            <!-- Introduction -->
            <div class="math-content">
                <h3>1. Introduction</h3>
                
                <h4>1.1 Biological Background</h4>
                <p>
                    Gene regulatory networks (GRNs) control when, where, and how much genes are expressed, thereby determining cell fate, function, and response to environmental signals. At the molecular level, <em>transcription factors</em> (TFs) proteins that bind to specific DNA sequences called <em>promoters</em> regulate gene transcription. TFs can <em>activate</em> transcription by recruiting RNA polymerase or <em>repress</em> it by blocking polymerase binding or recruiting chromatin-modifying complexes [Alon2007]. Multiple TFs often bind cooperatively, creating nonlinear input-output relationships crucial for sharp decision-making and signal processing.
                </p>

                <h4>1.2 Why Continuous Models?</h4>
                <p>
                    When molecular copy numbers are high (hundreds to thousands of molecules per cell) and timescales of interest exceed those of individual binding events, stochastic fluctuations average out, justifying deterministic continuous models [TysonReview2003]. Ordinary differential equations (ODEs) capture mean behavior efficiently, enabling analytical insights into stability, bifurcations, and parameter sensitivity that complement stochastic simulations. Continuous models excel at revealing qualitative behavior identifying parameter regimes for switches, oscillations, or homeostasis rather than precise quantitative predictions.
                </p>

                <h4>1.3 Historical Context</h4>
                <p>
                    The mathematical study of gene regulation traces to Jacob and Monod's seminal work on the lac operon [JacobMonod1961], which introduced feedback control concepts to molecular biology. Goodwin's cyclic repression model (1965) pioneered the use of nonlinear ODEs to explain biological oscillations. Recent synthetic biology experiments have validated theoretical predictions: Gardner et al. constructed a bistable toggle switch [GardnerToggle2000], while Elowitz and Leibler engineered an oscillating repressilator [ElowitzRepressilator2000], demonstrating that simple mathematical models capture essential regulatory logic.
                </p>

                <h4>1.4 Document Scope</h4>
                <p>
                    This exposition develops the ODE framework for GRNs, introducing Hill functions to model cooperative regulation, then analyzes three canonical motifs: negative autoregulation (homeostasis), mutual repression (bistability), and cyclic repression (oscillations). We emphasize steady-state analysis, stability via linearization, and biological interpretation of mathematical results.
                </p>
            </div>

            <!-- Mathematical Framework -->
            <div class="math-content">
    <h3>2. Mathematical Framework</h3>

    <h4>2.1 Basic Formulation</h4>
    <p>
        Consider \(n\) genes with protein concentrations \(x_i(t)\) (or equivalently, mRNA levels in fast-equilibrium approximations). The rate of change of \(x_i\) balances production (transcription and translation) against degradation and dilution:
    </p>
    <div class="equation">
        <div class="equation-content">$$\frac{dx_i}{dt} = f_i(x_1, \ldots, x_n) - \gamma_i x_i,$$</div>
        <div class="equation-number">(1)</div>
    </div>
    <p>
        where \(f_i(\mathbf{x})\) encodes regulatory inputs and \(\gamma_i\) is the effective degradation rate (including dilution from cell growth). The production function \(f_i\) depends on TF concentrations according to binding thermodynamics.
    </p>

    <h5>Hill Functions: Modeling Cooperative Binding</h5>
    <p>
        Cooperative binding of \(n\) identical TF molecules to a promoter (or single-molecule binding with cooperativity factor \(n\)) yields sigmoidal dose-response curves described by <em>Hill functions</em>. For activation:
    </p>
    <div class="equation">
        <div class="equation-content">$$f_{\text{act}}(x) = \beta \frac{x^n}{K^n + x^n},$$</div>
        <div class="equation-number">(2)</div>
    </div>
    <p>and for repression:</p>
    <div class="equation">
        <div class="equation-content">$$f_{\text{rep}}(x) = \beta \frac{K^n}{K^n + x^n},$$</div>
        <div class="equation-number">(3)</div>
    </div>
    <p>
        where \(\beta\) is the maximum production rate, \(K\) is the dissociation constant (TF concentration at half-maximal response), and \(n\) is the Hill coefficient measuring cooperativity. For \(n=1\), binding is non-cooperative (hyperbolic); \(n > 1\) produces ultrasensitive switches; \(n \to \infty\) approaches a step function. Multiple regulators combine through products (independent binding) or sums (competitive binding).
    </p>

    <h4>2.2 Key Properties</h4>

    <h5>Steady States</h5>
    <p>
        Steady states \(\mathbf{x}^*\) satisfy \(f_i(\mathbf{x}^*) = \gamma_i x_i^*\) for all \(i\), representing time-invariant cellular states such as differentiated cell types or quiescent phases. Biologically, multiple stable steady states (multistability) correspond to cellular memory and distinct fates. Graphically, steady states occur where nullclines (curves where \(dx_i/dt = 0\)) intersect.
    </p>

    <h5>Stability Analysis</h5>
    <p>
        To assess steady-state stability, linearize the general ODE around \(\mathbf{x}^*\) by setting \(\mathbf{x}(t) = \mathbf{x}^* + \mathbf{\epsilon}(t)\) with \(|\mathbf{\epsilon}| \ll 1\):
    </p>
    <div class="equation">
        <div class="equation-content">$$\frac{d\epsilon_i}{dt} = \sum_{j=1}^n \left(\frac{\partial f_i}{\partial x_j}\bigg|_{\mathbf{x}^*} - \gamma_i \delta_{ij}\right) \epsilon_j.$$</div>
        <div class="equation-number">(4)</div>
    </div>
    <p>
        The Jacobian matrix \(\mathbf{J}\) has entries \(J_{ij} = \partial f_i/\partial x_j - \gamma_i \delta_{ij}\). Eigenvalues \(\lambda\) of \(\mathbf{J}\) determine stability: \(\mathbf{x}^*\) is stable if \(\text{Re}(\lambda_k) < 0\) for all \(k\), and unstable if any \(\text{Re}(\lambda_k) > 0\). A pair of complex conjugate eigenvalues crossing the imaginary axis signals a <em>Hopf bifurcation</em>, where stable steady states give way to oscillations.
    </p>

    <h5>Bifurcations</h5>
    <p>
        Parameter changes can qualitatively alter dynamics. <em>Saddle-node bifurcations</em> create or destroy steady state pairs, enabling switches between states. <em>Hopf bifurcations</em> birth oscillatory solutions from fixed points. These bifurcations underpin cell fate transitions and rhythmic processes.
    </p>
</div>

<div class="math-content">
    <h3>3. Canonical Examples</h3>

    <h4>3.1 Autoregulation: Negative Feedback</h4>
    <p>A gene encoding a TF that represses its own promoter exhibits <em>negative autoregulation</em>, one of the most common network motifs [Alon2007].</p>

    <h5>Model</h5>
    <p>
        Let \(x(t)\) be the TF concentration. The ODE is:
    </p>
    <div class="equation">
        <div class="equation-content">$$\frac{dx}{dt} = \frac{\beta K^n}{K^n + x^n} - \gamma x.$$</div>
        <div class="equation-number">(5)</div>
    </div>

    <h5>Steady-State Analysis</h5>
    <p>
        At steady state, production balances degradation:
    </p>
    <div class="equation">
        <div class="equation-content">$$\frac{\beta K^n}{K^n + (x^*)^n} = \gamma x^*.$$</div>
        <div class="equation-number">(6)</div>
    </div>
    <p>
        Define dimensionless variables \(\tilde{x} = x/K\), \(\alpha = \beta/(\gamma K)\). Then:
    </p>
    <div class="equation">
        <div class="equation-content">$$\frac{\alpha}{1 + \tilde{x}^n} = \tilde{x} \implies \alpha = \tilde{x}(1 + \tilde{x}^n).$$</div>
        <div class="equation-number">(7)</div>
    </div>
    <p>
        For any \(\alpha > 0\), a unique positive solution \(\tilde{x}^*\) exists (the right side is monotonically increasing). Thus, negative autoregulation yields a unique stable steady state.
    </p>

    <h5>Stability</h5>
    <p>
        The Jacobian at \(x^*\) is:
    </p>
    <div class="equation">
        <div class="equation-content">$$J = -\frac{n\beta K^n (x^*)^{n-1}}{(K^n + (x^*)^n)^2} - \gamma < 0,$$</div>
        <div class="equation-number">(8)</div>
    </div>
    <p>confirming stability for all parameters.</p>

    <h5>Biological Significance</h5>
    <p>
        Negative feedback provides <em>homeostasis</em>: the system resists perturbations, maintaining TF levels near \(x^*\). Additionally, it reduces response time to signals and attenuates intrinsic noise, as demonstrated by Becskei and Serrano in synthetic circuits [BecskeiSerrano2000]. This motif appears in stress responses and developmental timing.
    </p>

    <h4>3.2 Toggle Switch: Mutual Repression</h4>
    <p>Two genes mutually repressing each other form a bistable switch capable of memory [GardnerToggle2000].</p>

    <h5>Model</h5>
    <p>
        Let \(x\) and \(y\) be concentrations of repressors \(X\) and \(Y\). Assuming equal degradation rates (\(\gamma = 1\) via rescaling):
    </p>
    <div class="equation">
        <div class="equation-content">$$\frac{dx}{dt} = \frac{\alpha_1}{1 + y^n} - x, \quad \frac{dy}{dt} = \frac{\alpha_2}{1 + x^n} - y.$$</div>
        <div class="equation-number">(9)</div>
    </div>
    <p>
        Here \(\alpha_i\) are effective production strengths.
    </p>

    <h5>Steady States and Bistability</h5>
    <p>Nullclines are:</p>
    <div class="equation">
        <div class="equation-content">$$x = \frac{\alpha_1}{1 + y^n}, \quad y = \frac{\alpha_2}{1 + x^n}.$$</div>
        <div class="equation-number">(10)</div>
    </div>
    <p>
        For symmetric parameters (\(\alpha_1 = \alpha_2 = \alpha\)), one steady state is \((x^*, y^*) = (x^*, x^*)\). Two additional asymmetric states exist for sufficiently large \(\alpha\) and \(n\). Graphically, the \(x\)-nullcline (decreasing) intersects the \(y\)-nullcline (decreasing) at three points when both are sufficiently steep.
    </p>
    <p>
        Bistability requires:
    </p>
    <div class="equation">
        <div class="equation-content">$$\alpha > 1 + n \quad \text{(approximate criterion for } n \gg 1).$$</div>
        <div class="equation-number">(11)</div>
    </div>
    <p>
        The symmetric state \((x^*, x^*)\) becomes unstable (saddle point), while two stable states emerge: \((x_{\text{high}}, y_{\text{low}})\) and \((x_{\text{low}}, y_{\text{high}})\).
    </p>

    <h5>Biological Significance</h5>
    <p>
        The toggle switch exhibits <em>cellular memory</em>: once flipped to a state (e.g., by transient induction of \(X\)), the system remains there indefinitely. Gardner et al. constructed this circuit in <em>E. coli</em>, controlling lac and tet repressors, and demonstrated reversible switching via chemical inducers. Naturally, mutual repression underlies cell fate decisions (e.g., lambda phage lysis/lysogeny) and lineage commitment in development.
    </p>

    <div class="figure-container">
            <img src="Toggle_Switch.png" alt="Toggle Switch" class="figure-image">
            <div class="figure-caption">
                <strong>Figure 1:</strong>  Phase portrait of the toggle switch with parameters \( \alpha_1 = \alpha_2 = 3 \) and \( n = 3 \). Blue curve: \( x \)-nullcline where \( \frac{dx}{dt} = 0 \). Red curve: \( y \)-nullcline where \( \frac{dy}{dt} = 0 \). The system exhibits bistability with two stable fixed points (filled circles) at opposite corners representing distinct cell states, separated by an unstable saddle point (open circle). Gray arrows show example trajectories converging to stable attractors. Small black arrows indicate the vector field direction. The system maintains memory: initial conditions determine which stable state is reached.
            </div>
    </div>

    <h4>3.3 Repressilator: Cyclic Repression</h4>
    <p>Three genes arranged in a negative feedback loop can oscillate, providing a synthetic oscillator [ElowitzRepressilator2000].</p>

    <h5>Model</h5>
    <p>Genes \(x_1, x_2, x_3\) cyclically repress each other:</p>
    <div class="equation">
        <div class="equation-content">$$\frac{dx_i}{dt} = \frac{\beta}{1 + x_{i-1}^n} - \gamma x_i \quad (i = 1,2,3 \text{ mod } 3).$$</div>
        <div class="equation-number">(12)</div>
    </div>

    <h5>Oscillation Conditions</h5>
    <p>
        By symmetry, the unique steady state is \(x_1^* = x_2^* = x_3^* = x^*\), where:
    </p>
    <div class="equation">
        <div class="equation-content">$$\frac{\beta}{1 + (x^*)^n} = \gamma x^* \implies x^* = \left(\frac{\beta}{\gamma} - 1\right)^{1/n} \quad (\text{for } \beta > \gamma).$$</div>
        <div class="equation-number">(13)</div>
    </div>
    <p>
        Linearizing around \((x^*, x^*, x^*)\), the Jacobian has the form:
    </p>
    <div class="equation">
        <div class="equation-content">
            $$\mathbf{J} = \begin{pmatrix}
                -\gamma & 0 & -a \\
                -a & -\gamma & 0 \\
                0 & -a & -\gamma
            \end{pmatrix}, \quad a = \frac{n\beta (x^*)^{n-1}}{(1 + (x^*)^n)^2} > 0.$$
        </div>
        <div class="equation-number">(14)</div>
    </div>
    <p>
        The characteristic polynomial yields eigenvalues. A Hopf bifurcation occurs when a pair of complex conjugate eigenvalues crosses the imaginary axis. The condition for oscillations (simplified) is:
    </p>
    <div class="equation">
        <div class="equation-content">$$\frac{n\beta}{\gamma} > 2(1 + (x^*)^n)^2 \quad \text{or roughly} \quad n > 2.$$</div>
        <div class="equation-number">(15)</div>
    </div>
    <p>
        Strong cooperativity (\(n \geq 3\)) and high production/degradation ratio favor oscillations.
    </p>

    <h5>Biological Significance</h5>
    <p>
        Elowitz and Leibler implemented this circuit using lac, tet, and lambda repressors in <em>E. coli</em>, observing fluorescence oscillations with periods of hours. While the synthetic repressilator is noisy (due to low copy numbers), it validated the principle that cyclic repression generates oscillations. Natural counterparts include circadian clocks (Kai proteins in cyanobacteria) and the cell cycle, where cyclins and CDKs form interconnected feedback loops [TysonReview2003].
    </p>
    <div class="figure-container">
            <img src="Bi-switch.png" alt="Bi-switch" class="figure-image" style="width: 80%; max-width: 800px; max-height: 900px;">
            <div class="figure-caption">
                <strong>Figure 2:</strong> Repressilator dynamics with three mutually repressing genes. (a) Three-dimensional phase space projection showing the stable limit cycle (blue closed orbit) surrounding the unstable steady state (red open circle at center). A trajectory (red curve with arrow) spirals outward from near the fixed point, converging to the limit cycle. Semi-transparent planes represent nullcline surfaces where individual growth rates vanish. The limit cycle represents a stable oscillatory attractor, analogous to circadian rhythms. (b) Time series showing sustained oscillations in protein concentrations \( x_1(t), x_2(t) \), and \( x_3(t) \) with 120Â° phase shifts. Each repressor peaks when its repressor is at minimum, creating sequential waves.
            </div>
    </div>
</div>

<div class="math-content">
    <h3>4. Extensions and Limitations</h3>

    <h4>4.1 Extensions</h4>

    <h5>Multi-scale Models</h5>
    <p>
        When mRNA and protein timescales differ significantly, we can exploit fast mRNA equilibration. Let \(m_i\) be mRNA and \(p_i\) be protein:
    </p>
    <div class="equation">
        <div class="equation-content">$$\frac{dm_i}{dt} = f_i(\mathbf{p}) - \delta_m m_i,$$</div>
        <div class="equation-number">(16)</div>
    </div>
    <div class="equation">
        <div class="equation-content">$$\frac{dp_i}{dt} = k_{\text{tl}} m_i - \delta_p p_i.$$</div>
        <div class="equation-number">(17)</div>
    </div>
    <p>
        If \(\delta_m \gg \delta_p\), apply quasi-steady-state approximation: \(m_i \approx f_i(\mathbf{p})/\delta_m\), reducing to single-variable protein dynamics. This captures realistic time-scale separation.
    </p>

    <h5>Spatial Effects</h5>
    <p>
        For spatially extended systems (e.g., morphogen gradients in development), add diffusion:
    </p>
    <div class="equation">
        <div class="equation-content">$$\frac{\partial x_i}{\partial t} = D_i \nabla^2 x_i + f_i(\mathbf{x}) - \gamma_i x_i.$$</div>
        <div class="equation-number">(18)</div>
    </div>
    <p>
        Reaction-diffusion PDEs explain pattern formation (Turing patterns) and gradient-based positional information.
    </p>

    <h5>External Signals</h5>
    <p>
        Time-dependent inputs \(u(t)\) model environmental stimuli or drug interventions:
    </p>
    <div class="equation">
        <div class="equation-content">$$\frac{dx_i}{dt} = f_i(\mathbf{x}, u(t)) - \gamma_i x_i.$$</div>
        <div class="equation-number">(19)</div>
    </div>
    <p>
        This enables control theory applications: optimizing input signals to drive desired cellular responses.
    </p>

             <h4>4.2 Limitations</h4>
                
                <h5>Stochastic Effects</h5>
                <p>
                    When molecule counts are low (tens of proteins), intrinsic noise dominates. Stochastic models (Gillespie algorithm, chemical master equation) are necessary to capture noise-induced switching or coherence resonance. Continuous ODEs miss rare events (e.g., spontaneous toggle flips) that drive long-term behavior.
                </p>

                <h5>Model Reduction and Identifiability</h5>
                <p>
                    Real GRNs involve dozens to thousands of genes. Parameter inference from data faces identifiability challenges: many parameter sets fit data equally well. Model reduction techniques (sensitivity analysis, time-scale separation) simplify systems while retaining essential dynamics.
                </p>

                <h5>Discrete Events</h5>
                <p>
                    Cell division resets concentrations via volume doubling and chromosome segregation. DNA replication and epigenetic state changes are inherently discrete. Hybrid models combining ODEs with discrete event simulators address these limitations.
                </p>
            </div>

            <!-- Conclusion -->
            <div class="math-content">
                <h3>5. Conclusion</h3>
                <p>
                    Continuous ODE models distill the complex biochemistry of gene regulation into tractable mathematical forms, revealing how network architecture determines dynamical capabilities. Negative autoregulation ensures homeostasis, mutual repression creates bistable switches for memory, and cyclic repression generates oscillations for timing. These design principles, validated in synthetic circuits and natural systems, guide our understanding of cellular decision-making and inform rational design of genetic circuits for biotechnology.
                </p>
                <p>
                    Current frontiers integrate multi-omics data (transcriptomics, proteomics, epigenomics) with mechanistic models, aiming for predictive cell behavior models. Machine learning complements mechanistic modeling by inferring network structure from data and parameterizing equations from high-throughput experiments. As synthetic biology matures, continuous models remain indispensable tools bridging molecular mechanisms and emergent cellular functions.
                </p>
            </div>

            <!-- References -->
            <div class="math-content">
                <h3>References</h3>
                <div class="reference-list">
                    <div class="reference-item">
                        <span class="reference-citation">[Alon2007]</span>
                        <p class="reference-content">
                            U. Alon, <em>Network motifs: theory and experimental approaches</em>, Nature Reviews Genetics <strong>8</strong>, 450-461 (2007).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[BecskeiSerrano2000]</span>
                        <p class="reference-content">
                            A. Becskei and L. Serrano, <em>Engineering stability in gene networks by autoregulation</em>, Nature <strong>405</strong>, 590-593 (2000).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[ElowitzRepressilator2000]</span>
                        <p class="reference-content">
                            M. B. Elowitz and S. Leibler, <em>A synthetic oscillatory network of transcriptional regulators</em>, Nature <strong>403</strong>, 335-338 (2000).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[GardnerToggle2000]</span>
                        <p class="reference-content">
                            T. S. Gardner, C. R. Cantor, and J. J. Collins, <em>Construction of a genetic toggle switch in Escherichia coli</em>, Nature <strong>403</strong>, 339-342 (2000).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[JacobMonod1961]</span>
                        <p class="reference-content">
                            F. Jacob and J. Monod, <em>Genetic regulatory mechanisms in the synthesis of proteins</em>, Journal of Molecular Biology <strong>3</strong>, 318-356 (1961).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[TysonReview2003]</span>
                        <p class="reference-content">
                            J. J. Tyson, K. Chen, and B. Novak, <em>Sniffers, buzzers, toggles and blinkers: dynamics of regulatory and signaling pathways in the cell</em>, Current Opinion in Cell Biology <strong>15</strong>, 221-231 (2003).
                        </p>
                    </div>
                </div>

            </div>
        </div>
    </div>
</div>
</div>


<div class="main-container">
<div class="project-card">
    <div class="project-header" onclick="toggleContent('boolean-networks')">
        <div class="project-meta">
            <span class="meta-item">
                <i class="fas fa-calendar-alt"></i>
                Added: June 21, 2024
            </span>
            <span class="meta-item">
                <i class="fas fa-clock"></i>
                1hr 57 min read
            </span>
            <span class="meta-item">
                <i class="fas fa-tag"></i>
                Boolean Algebra & Systems Biology
            </span>
        </div>
        <h2 class="project-title">Discrete Models of Gene Regulatory Networks: A Boolean Algebra Perspective</h2>
        <p class="project-subtitle">Author: Clinton Oluranran Kayoh</p>
        <div class="expand-btn" id="btn-boolean-networks">
            <i class="fas fa-plus"></i>
        </div>
    </div>
    <div class="project-content" id="content-boolean-networks">
        <div class="content-inner">
            <!-- Abstract -->
            <div class="math-content">
                <h3>Abstract</h3>
                <p>
                    Boolean networks provide a discrete, qualitative framework for modeling gene regulatory networks (GRNs) by abstracting continuous molecular concentrations into binary ON/OFF states. This approach, pioneered by Stuart Kauffman in 1969, enables tractable analysis of large-scale networks through finite state space dynamics, where attractors (fixed points and cycles) correspond to cellular phenotypes. We introduce Boolean network formalism, compare synchronous and asynchronous update schemes, and analyze canonical regulatory motifs including toggle switches and feed-forward loops. Applications to developmental networks (Drosophila segment polarity) demonstrate predictive power. Extensions to probabilistic Boolean networks and multi-valued logic address model limitations. Boolean models democratize systems biology, capturing essential regulatory logic without requiring quantitative kinetic parameters.
                </p>
            </div>

            <!-- Introduction -->
            <div class="math-content">
                <h3>1. Introduction</h3>
                
                <h4>1.1 Biological Motivation</h4>
                <p>
                    At the molecular level, gene regulation exhibits fundamentally discrete characteristics. Transcription initiation is a stochastic binary event: RNA polymerase either engages the promoter or not. While mRNA and protein copy numbers vary continuously, regulatory decisions often reflect threshold-crossing behavior genes effectively switch between inactive and active states when regulator concentrations exceed critical values. This motivates Boolean abstraction: represent each gene \(i\) as binary variable \(x_i \in \{0,1\}\) (0=OFF, 1=ON), discarding quantitative details to focus on qualitative regulatory logic [Kauffman1969].
                </p>
                <p>
                    Boolean models are appropriate when: (i) qualitative behavior (cell types, oscillations) matters more than precise expression levels; (ii) networks are large (\(n > 50\) genes) making continuous models intractable; (iii) quantitative data (rate constants, binding affinities) are unavailable; (iv) regulatory interactions are well-characterized but kinetic details are not. Thresholding continuous concentrations into Boolean states captures essential control logic while dramatically reducing complexity.
                </p>

                <h4>1.2 Historical Context</h4>
                <p>
                    Stuart Kauffman's 1969 work on random Boolean networks (RBNs) established the theoretical foundation [Kauffman1969]. Kauffman explored networks where each gene's Boolean function is randomly assigned, discovering that network connectivity (average number of inputs per gene, \(K\)) determines dynamical regime: \(K=1\) yields ordered (frozen) dynamics, \(K>2\) chaotic dynamics, and \(K=2\) critical dynamics with rich attractor landscapes. His NK model demonstrated that complex systems with moderate connectivity self-organize into stable attractors resembling cellular differentiation.
                </p>
                <p>
                    RenÃ© Thomas formalized logical analysis of regulatory circuits in the 1970s-1990s, establishing connections between feedback loops and multistability/oscillations [ThieffryThomas1995]. By the 2000s, Boolean models transitioned from theoretical curiosity to practical tool, applied to real developmental and disease networks with experimental validation.
                </p>

                <h4>1.3 Advantages Over Continuous Models</h4>
                <p>
                    Boolean networks offer distinct computational and conceptual advantages:
                </p>
                <ul>
                    <li><strong>Tractability</strong>: State space size \(2^n\) is finite; for \(n=10\), only 1024 states. Genome-scale networks (\(n \sim 100\)) remain analyzable via sampling.</li>
                    <li><strong>Parameter parsimony</strong>: No rate constants, Hill coefficients, or binding affinities needed only regulatory logic (activate/repress).</li>
                    <li><strong>Qualitative insight</strong>: Attractors directly represent observable phenotypes (cell types, oscillatory behaviors).</li>
                    <li><strong>Natural representation</strong>: Regulatory rules often described verbally ("gene X activates Y if Z is absent") map directly to Boolean logic.</li>
                </ul>
                <p>
                    Contrast with ODEs: a 10-gene continuous model requires \(\sim 50\) parameters (production rates, degradation constants, Hill coefficients); Boolean version requires only connectivity and logic gates.
                </p>

                <h4>1.4 Document Roadmap</h4>
                <p>
                    We develop Boolean network formalism (Section 2), analyze update schemes and attractor dynamics, examine canonical motifs (toggle switches, feed-forward loops) and real networks (Drosophila development) in Section 3, discuss computational methods (Section 4), and survey extensions and limitations (Section 5).
                </p>
            </div>

            <!-- Mathematical Framework -->
            <div class="math-content">
                <h3>2. Mathematical Framework</h3>
                
                <h4>2.1 Boolean Network Formalism</h4>
                
                <h5>State Space</h5>
                <p>
                    A Boolean network with \(n\) genes assigns each gene \(i\) a binary state variable:
                </p>
                <div class="equation">
                    <div class="equation-content">$$x_i(t) \in \{0, 1\}, \quad i = 1, 2, \ldots, n,$$</div>
                    <div class="equation-number">(2.1)</div>
                </div>
                <p>
                    where \(t \in \mathbb{Z}_{\geq 0}\) is discrete time. The network state is the vector:
                </p>
                <div class="equation">
                    <div class="equation-content">$$\mathbf{x}(t) = (x_1(t), x_2(t), \ldots, x_n(t)) \in \{0,1\}^n.$$</div>
                    <div class="equation-number">(2.2)</div>
                </div>
                <p>
                    The state space \(\mathcal{S} = \{0,1\}^n\) contains \(2^n\) possible configurations.
                </p>

                <h5>Boolean Functions</h5>
                <p>
                    Each gene \(i\) updates according to a Boolean function:
                </p>
                <div class="equation">
                    <div class="equation-content">$$x_i(t+1) = f_i(x_1(t), x_2(t), \ldots, x_n(t)),$$</div>
                    <div class="equation-number">(2.3)</div>
                </div>
                <p>
                    where \(f_i: \{0,1\}^n \to \{0,1\}\). In practice, \(f_i\) depends on a subset of genes (regulators of \(i\)). Boolean functions are expressed using logical operators:
                </p>
                <ul>
                    <li><strong>NOT</strong>: \(\neg x\) (negation, repression)</li>
                    <li><strong>AND</strong>: \(x \land y\) (both inputs required, conjunction)</li>
                    <li><strong>OR</strong>: \(x \lor y\) (either input sufficient, disjunction)</li>
                </ul>

                <h5>Examples of Regulatory Logic</h5>
                <p>
                    <em>Activation:</em> Gene \(j\) activates gene \(i\):
                </p>
                <div class="equation">
                    <div class="equation-content">$$x_i(t+1) = x_j(t).$$</div>
                    <div class="equation-number">(2.4)</div>
                </div>
                <p>
                    <em>Repression:</em> Gene \(j\) represses gene \(i\):
                </p>
                <div class="equation">
                    <div class="equation-content">$$x_i(t+1) = \neg x_j(t).$$</div>
                    <div class="equation-number">(2.5)</div>
                </div>
                <p>
                    <em>AND gate:</em> Gene \(i\) requires both \(j\) and \(k\) active:
                </p>
                <div class="equation">
                    <div class="equation-content">$$x_i(t+1) = x_j(t) \land x_k(t).$$</div>
                    <div class="equation-number">(2.6)</div>
                </div>
                <p>
                    <em>Complex logic:</em> Gene \(i\) activated by \(j\) in absence of repressor \(k\), OR constitutively by \(m\):
                </p>
                <div class="equation">
                    <div class="equation-content">$$x_i(t+1) = (x_j(t) \land \neg x_k(t)) \lor x_m(t).$$</div>
                    <div class="equation-number">(2.7)</div>
                </div>

                <h4>2.2 Update Schemes</h4>
                <p>
                    The order in which genes update profoundly affects dynamics.
                </p>

                <h5>Synchronous (Parallel) Update</h5>
                <p>
                    All genes update simultaneously:
                </p>
                <div class="equation">
                    <div class="equation-content">$$\mathbf{x}(t+1) = F(\mathbf{x}(t)),$$</div>
                    <div class="equation-number">(2.8)</div>
                </div>
                <p>
                    where \(F: \{0,1\}^n \to \{0,1\}^n\) is the global update function with \(F = (f_1, f_2, \ldots, f_n)\). Dynamics are deterministic: each state has unique successor. However, synchrony can create artificial oscillations absent in real biology, where genes update asynchronously due to independent molecular events.
                </p>

                <h5>Asynchronous Update</h5>
                <p>
                    At each time step, one randomly selected gene updates while others remain fixed. If gene \(i\) is selected:
                </p>
                <div class="equation">
                    <div class="equation-content">$$x_i(t+1) = f_i(\mathbf{x}(t)), \quad x_j(t+1) = x_j(t) \text{ for } j \neq i.$$</div>
                    <div class="equation-number">(2.9)</div>
                </div>
                <p>
                    Selection probabilities \(p_i\) (often uniform, \(p_i = 1/n\)) introduce stochasticity. Dynamics form a Markov chain on \(\{0,1\}^n\).
                </p>
                <p>
                    <em>Biological realism:</em> Asynchronous updates better reflect reality transcription/translation events at different genes are uncorrelated. This eliminates spurious synchronous oscillations and reveals robust dynamical features.
                </p>
                <p>
                    <em>Trade-off:</em> Synchronous models are computationally simpler (deterministic trajectories) but less biologically faithful; asynchronous models require probabilistic analysis but avoid artifacts.
                </p>

                <h4>2.3 Attractors and Basins</h4>
                
                <h5>Fixed Points</h5>
                <p>
                    A state \(\mathbf{x}^* \in \{0,1\}^n\) is a <em>fixed point</em> (steady state) if:
                </p>
                <div class="equation">
                    <div class="equation-content">$$F(\mathbf{x}^*) = \mathbf{x}^*.$$</div>
                    <div class="equation-number">(2.10)</div>
                </div>
                <p>
                    Biologically, fixed points represent stable gene expression patterns: differentiated cell types (muscle, neuron), quiescent states, or homeostatic configurations.
                </p>

                <h5>Limit Cycles</h5>
                <p>
                    A <em>limit cycle</em> (oscillatory attractor) is a periodic sequence of distinct states:
                </p>
                <div class="equation">
                    <div class="equation-content">$$\mathbf{x}(0) \to \mathbf{x}(1) \to \cdots \to \mathbf{x}(T-1) \to \mathbf{x}(0),$$</div>
                    <div class="equation-number">(2.11)</div>
                </div>
                <p>
                    where \(T\) is the period and \(\mathbf{x}(i) \neq \mathbf{x}(j)\) for \(i \neq j\) (modulo \(T\)). Biological examples include cell cycle checkpoints (G1/S/G2/M phases) and circadian rhythms (clock gene oscillations).
                </p>

                <h5>Basins of Attraction</h5>
                <p>
                    The basin of attraction for attractor \(A\) is:
                </p>
                <div class="equation">
                    <div class="equation-content">$$\mathcal{B}(A) = \{\mathbf{x} \in \{0,1\}^n : \lim_{t \to \infty} \mathbf{x}(t) \in A \text{ when } \mathbf{x}(0) = \mathbf{x}\}.$$</div>
                    <div class="equation-number">(2.12)</div>
                </div>
                <p>
                    Basins partition state space. Biologically, basins represent differentiation pathways: progenitor cells (initial states) commit to fates (attractors) along developmental trajectories. Basin boundaries are decision points; perturbations there can switch cell fate.
                </p>

                <h5>State Transition Graph</h5>
                <p>
                    The <em>state transition graph</em> (STG) is a directed graph \(G = (\mathcal{S}, E)\) where nodes are states and edges \((\mathbf{x}, F(\mathbf{x})) \in E\) represent transitions. Attractors are terminal strongly connected components. For asynchronous dynamics, edges have weights (transition probabilities).
                </p>
            </div>

            <!-- Network Motifs and Canonical Examples -->
            <div class="math-content">
                <h3>3. Network Motifs and Canonical Examples</h3>
                
                <h4>3.1 Negative Autoregulation</h4>
                
                <h5>Boolean Model</h5>
                <p>
                    A single gene repressing itself:
                </p>
                <div class="equation">
                    <div class="equation-content">$$x(t+1) = \neg x(t).$$</div>
                    <div class="equation-number">(3.1)</div>
                </div>

                <h5>Dynamics</h5>
                <p>
                    <em>Synchronous update:</em> Starting from \(x(0)=0\): \(0 \to 1 \to 0 \to 1 \to \cdots\). Oscillates with period 2.
                </p>
                <p>
                    <em>Asynchronous update:</em> From any state, next state flips: \(0 \to 1\) or \(1 \to 0\) with equal probability. No stable attractor or perpetual random switching.
                </p>
                <p>
                    <em>Biological interpretation:</em> Pure negative feedback without degradation/dilution (implicit in Boolean abstraction) yields oscillations, contrasting with ODE models where degradation stabilizes fixed points. This highlights Boolean model limitation: absence of explicit degradation terms.
                </p>

                <h4>3.2 Mutual Inhibition (Toggle Switch)</h4>
                
                <h5>Boolean Model</h5>
                <p>
                    Two genes mutually repressing:
                </p>
                <div class="equation">
                    <div class="equation-content">$$x(t+1) = \neg y(t), \quad y(t+1) = \neg x(t).$$</div>
                    <div class="equation-number">(3.2)</div>
                </div>

                <h5>Dynamics Analysis</h5>
                <p>
                    State space: \(\{(0,0), (0,1), (1,0), (1,1)\}\).
                </p>
                <p>
                    <em>Synchronous update:</em> Transition table:
                </p>
                <div class="table-container">
                    <table>
                        <thead>
                            <tr>
                                <th>\(x(t)\)</th>
                                <th>\(y(t)\)</th>
                                <th>\(x(t+1)\)</th>
                                <th>\(y(t+1)\)</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr><td>0</td><td>0</td><td>1</td><td>1</td></tr>
                            <tr><td>0</td><td>1</td><td>0</td><td>1</td></tr>
                            <tr><td>1</td><td>0</td><td>1</td><td>0</td></tr>
                            <tr><td>1</td><td>1</td><td>0</td><td>0</td></tr>
                        </tbody>
                    </table>
                </div>
                <p>
                    Dynamics: \((0,1) \leftrightarrow (1,0)\) (period-2 cycle); \((0,0) \leftrightarrow (1,1)\) (period-2 cycle). No fixed points.
                </p>
                <p>
                    <em>Asynchronous update:</em> From \((0,1)\): updating \(x\) yields \((1,1)\); updating \(y\) yields \((0,0)\). From \((1,1)\): updating either gives \((1,0)\) or \((0,1)\). Careful analysis shows \((0,1)\) and \((1,0)\) are absorbing states once reached, system remains there (each is its own fixed point under asynchronous logic). States \((0,0)\) and \((1,1)\) are transient decision points.
                </p>
                <p>
                    <em>Biological significance:</em> Asynchronous toggle exhibits bistability, modeling cell fate decisions such as bacteriophage lambda lysis/lysogeny switch [ThieffryThomas1995]. The system "remembers" which state it reaches, providing cellular memory without requiring continuous protein production. Synchronous artifact (oscillations) disappears under realistic asynchronous updating.
                </p>

            <div class="figure-container">
               <img src="Attractor_Landscape.png" alt="Attractor landscape" class="figure-image" style="width: 80%; max-width: 800px; max-height: 900px;">
                 <div class="figure-caption">
                    <p><strong>Figure 1:</strong> (Left) State transition dynamics of a genetic toggle switch. The system comprises four possible states \((x,y)\) 
representing binary expression levels of two mutually repressing genes. Synchronous updates (solid arrows) 
produce two disjoint period-2 cycles: \((0,0) \leftrightarrow (1,1)\). Asynchronous updates (dashed arrows) introduce 
stochastic transitions where states \((0,0)\) and \((1,1)\) can transition to either \((0,1)\) or \((1,0)\) with equal 
probability. The fixed points \((0,1)\) and \((1,0)\) are absorbing states (sinks) under asynchronous dynamics. 
The state space partitions into two basins of attraction: \(\mathcal{B}((0,1))\) (blue) and \(\mathcal{B}((1,0))\) (red),
demonstrating how initial conditions determine the final phenotypic outcome. (Right) Attractor landscape of a three-gene regulatory network. The \(2^3 = 8\) possible gene expression states are represented 
as vertices of a cube, with coordinates \((x_1,x_2,x_3)\) indicating binary expression levels. 
The landscape exhibits three distinct attractors: two stable fixed points \(A_1 = (1,1,0)\) and \(A_2 = (0,0,1)\), 
and one limit cycle \(A_3 = \{(1,0,1), (0,1,1)\}\). State transitions (arrows) flow toward these attractors, 
partitioning the state space into three basins: red (flowing to \(A_1\)), blue (flowing to \(A_2\)), and green 
(flowing to \(A_3\)). Basin boundaries (separatrices, dashed lines) represent hypersurfaces where infinitesimal 
perturbations can redirect trajectories to different attractors, highlighting the multistability and phenotypic 
plasticity of the network.</p>
                </div>
            </div>

                <h4>3.3 Feed-Forward Loop (FFL)</h4>
                
                <h5>Coherent Type-1 FFL</h5>
                <p>
                    Gene \(X\) activates \(Y\) and \(Z\); \(Y\) also activates \(Z\). Gate logic at \(Z\) is AND:
                </p>
                <div class="equation">
                    <div class="equation-content">$$y(t+1) = x(t), \quad z(t+1) = x(t) \land y(t).$$</div>
                    <div class="equation-number">(3.3)</div>
                </div>

                <h5>Dynamics and Function</h5>
                <p>
                    Starting from \((x,y,z) = (0,0,0)\), if \(x\) switches ON at \(t=0\):
                </p>
                <div class="equation">
                    <div class="equation-content">$$\begin{align*}
t=0&: (x,y,z) = (1,0,0), \\
t=1&: y=1, \, z = 1 \land 0 = 0 \implies (1,1,0), \\
t=2&: z = 1 \land 1 = 1 \implies (1,1,1).
\end{align*}$$</div>
                    <div class="equation-number">(3.4)</div>
                </div>
                <p>
                    \(Z\) turns ON with delay (2 steps after \(X\)). If \(x\) switches OFF:
                </p>
                <div class="equation">
                    <div class="equation-content">$$\begin{align*}
t=0&: (x,y,z) = (0,1,1), \\
t=1&: z = 0 \land 1 = 0 \implies (0,0,0).
\end{align*}$$</div>
                    <div class="equation-number">(3.5)</div>
                </div>
                <p>
                    \(Z\) turns OFF immediately.
                </p>
                <p>
                    <em>Function:</em> The FFL implements <em>sign-sensitive delay</em> delays ON response (filtering transient signals, detecting persistence) but not OFF response. This motif is overrepresented in transcriptional networks, serving noise filtering and pulse detection roles [Alon2007].
                </p>

                <h4>3.4 Segment Polarity Network (Drosophila)</h4>
                
                <h5>Background</h5>
                <p>
                    Drosophila embryo segmentation requires the segment polarity network to establish parasegment boundaries. Key genes: <em>engrailed</em> (en), <em>wingless</em> (wg), <em>patched</em> (ptc), <em>hedgehog</em> (hh), <em>cubitus interruptus</em> (ci).
                </p>

                <h5>Simplified Boolean Model</h5>
                <p>
                    Albert & Othmer (2003) constructed a 15-gene Boolean model [AlbertOthmer2003]. Simplified 4-gene version:
                </p>
                <div class="equation">
                    <div class="equation-content">$$\begin{align}
\text{en}(t+1) &= \text{wg}(t), \\
\text{wg}(t+1) &= \text{en}(t) \land \neg \text{ptc}(t), \\
\text{hh}(t+1) &= \text{en}(t), \\
\text{ptc}(t+1) &= \neg \text{wg}(t).
\end{align}$$</div>
                    <div class="equation-number">(3.6)</div>
                </div>

                <h5>Analysis</h5>
                <p>
                    Fixed point search: setting left = right in the equations, solve:
                </p>
                <ul>
                    <li>\(\text{en} = \text{wg}\), \(\text{wg} = \text{en} \land \neg \text{ptc}\), \(\text{hh} = \text{en}\), \(\text{ptc} = \neg \text{wg}\).</li>
                </ul>
                <p>
                    Two fixed points exist: \((\text{en, wg, hh, ptc}) = (1,1,1,0)\) (anterior cell type expressing en/wg) and \((0,0,0,1)\) (posterior cell type expressing ptc). These correspond to experimentally observed stable expression domains in wild-type embryos.
                </p>
                <p>
                    Perturbation analysis (gene knockouts): Setting \(\text{wg}=0\) permanently leads to loss of \((1,1,1,0)\) attractor, predicting segment fusion consistent with <em>wg</em> mutant phenotypes.
                </p>
            </div>

            <!-- Computational Methods and Analysis -->
            <div class="math-content">
                <h3>4. Computational Methods and Analysis</h3>
                
                <h4>4.1 Finding Attractors</h4>
                
                <h5>Exhaustive Search</h5>
                <p>
                    For small networks (\(n \leq 15\), \(2^{15} = 32768\) states):
                </p>
                <div class="algorithm">
                    <div class="algorithm-title">Exhaustive Attractor Finding</div>
                    <div class="algorithm-content">
                        <p><strong>for</strong> each state \(\mathbf{x} \in \{0,1\}^n\) <strong>do</strong></p>
                        <p style="margin-left: 20px;">Simulate trajectory: \(\mathbf{x}(0) = \mathbf{x}\), iterate \(\mathbf{x}(t+1) = F(\mathbf{x}(t))\)</p>
                        <p style="margin-left: 20px;">Detect attractor when revisiting a state (cycle found)</p>
                        <p style="margin-left: 20px;">Store attractor and basin membership</p>
                        <p><strong>end for</strong></p>
                        <p><strong>return</strong> List of attractors with basin sizes</p>
                    </div>
                </div>
                <p>
                    Build complete STG: \(O(2^n)\) time and space.
                </p>

                <h5>Sampling Methods</h5>
                <p>
                    For large networks (\(n \sim 100\)):
                </p>
                <ul>
                    <li>Sample \(M\) random initial conditions (e.g., \(M=10000\)).</li>
                    <li>Simulate each to convergence (detect revisited state).</li>
                    <li>Estimate attractor frequencies (fraction of samples reaching each attractor).</li>
                </ul>
                <p>
                    Complexity: \(O(M \cdot T_{\text{conv}})\) where \(T_{\text{conv}} \sim \log_2(2^n) = n\) typically.
                </p>

                <h5>SAT Solvers and Model Checking</h5>
                <p>
                    Encode fixed point finding as Boolean satisfiability (SAT):
                </p>
                <div class="equation">
                    <div class="equation-content">$$\bigwedge_{i=1}^n (x_i \leftrightarrow f_i(x_1, \ldots, x_n)).$$</div>
                    <div class="equation-number">(4.1)</div>
                </div>
                <p>
                    Modern SAT solvers efficiently handle \(n \sim 100\) variables. For cycles of length \(T\), encode:
                </p>
                <div class="equation">
                    <div class="equation-content">$$\bigwedge_{t=0}^{T-1} \bigwedge_{i=1}^n (x_i(t+1) \leftrightarrow f_i(\mathbf{x}(t))) \land (\mathbf{x}(T) = \mathbf{x}(0)) \land (\mathbf{x}(0) \neq \mathbf{x}(t) \text{ for } 0 < t < T).$$</div>
                    <div class="equation-number">(4.2)</div>
                </div>

                <h4>4.2 Sensitivity and Robustness Analysis</h4>
                
                <h5>Perturbation Analysis</h5>
                <p>
                    Test network robustness by single-gene perturbations:
                </p>
                <ol>
                    <li>Start from attractor \(A\) with state \(\mathbf{x}^* \in A\).</li>
                    <li>Flip gene \(i\): \(\mathbf{x}_{\text{pert}} = \mathbf{x}^*\) with \(x_i\) flipped.</li>
                    <li>Simulate to new attractor \(A'\).</li>
                    <li>Measure: fraction of perturbations returning to \(A\) (robustness score).</li>
                </ol>
                <p>
                    <em>Biological interpretation:</em> Robust attractors resist gene knockouts/overexpression modeling drug targets or mutations. Identifying genes whose perturbation switches attractors reveals intervention points for cell fate reprogramming [Shmulevich2002].
                </p>
                <div class="figure-container">
               <img src="Basin_Size.png" alt="Basin Size" class="figure-image">
                 <div class="figure-caption">
                    <p><strong>Figure 2:</strong> Basin size for the attractor landscape for a hypothetical 3-gene network (\(2^3=8\) states). State space depicted as a cube in Figure 1, with vertices labeled by binary states \((x_1,x_2,x_3)\). Color regions as basins: red basin flows to fixed point \(A_1 = (1,1,0)\), blue basin to fixed point \(A_2 = (0,0,1)\), green basin to limit cycle \(A_3 = \{(1,0,1), (0,1,1)\}\).</p>
                </div>
                </div>
            </div>

            <!-- Extensions and Limitations -->
            <div class="math-content">
                <h3>5. Extensions and Limitations</h3>
                
                <h4>5.1 Extensions</h4>
                
                <h5>Probabilistic Boolean Networks (PBNs)</h5>
                <p>
                    Each gene \(i\) has multiple possible Boolean functions \(\{f_i^{(1)}, f_i^{(2)}, \ldots, f_i^{(k)}\}\), selected with probabilities \(\{p_i^{(1)}, \ldots, p_i^{(k)}\}\) at each time step:
                </p>
                <div class="equation">
                    <div class="equation-content">$$x_i(t+1) = f_i^{(j)}(\mathbf{x}(t)) \text{ with probability } p_i^{(j)}.$$</div>
                    <div class="equation-number">(5.1)</div>
                </div>
                <p>
                    PBNs model uncertainty in regulatory rules and context-dependent regulation. Dynamics become Markov chains with stationary distributions replacing fixed points [Shmulevich2002].
                </p>

                <h5>Multi-Valued Logic</h5>
                <p>
                    Extend states to \(x_i \in \{0,1,2,\ldots,k-1\}\), capturing multiple expression levels (OFF, LOW, MEDIUM, HIGH). Ternary logic (\(k=3\)) is common. Update functions use multi-valued logical operators. This partially recovers quantitative information while maintaining discrete tractability.
                </p>

                <h5>Time Delays and Priority Classes</h5>
                <p>
                    Assign different update rates to genes: fast (\(\tau_i\) small) vs. slow (\(\tau_i\) large). Priority asynchronous updating: genes update in order of \(1/\tau_i\). Captures time-scale separation (mRNA faster than protein) without continuous time.
                </p>

                <h4>5.2 Limitations</h4>
                
                <h5>Loss of Quantitative Information</h5>
                <p>
                    Boolean models discard expression levels: cannot distinguish 100 vs. 1000 molecules (both "ON"). Cannot predict precise dynamics speed only attractor structure. For dose-response curves or graded signals (morphogen gradients), Boolean approximation fails.
                </p>

                <h5>Parameter-Free Trade-off</h5>
                <p>
                    <em>Strength:</em> No parameter estimation needed regulatory logic suffices. <em>Weakness:</em> Cannot fit quantitative time-series data without extensions (fuzzy logic, probabilistic parameters).
                </p>

                <h5>Thresholding Ambiguity</h5>
                <p>
                    Choosing ON/OFF threshold is arbitrary. Genes near threshold (\(x \approx K\) in ODE terms) have ambiguous Boolean states. Sensitivity to thresholds can alter predictions. Mitigation: vary thresholds systematically (robustness checks) or use multi-valued logic.
                </p>

                <h5>Neglect of Stochastic Fluctuations</h5>
                <p>
                    Boolean models (even asynchronous) are coarse-grained stochastics individual molecular events (binding/unbinding) are averaged. For low copy numbers, stochastic simulations (Gillespie) are necessary [WynnReview2012].
                </p>
            </div>

            <!-- Conclusion -->
            <div class="math-content">
                <h3>6. Conclusion</h3>
                <p>
                    Boolean networks distill gene regulatory complexity into combinatorial logic, revealing global dynamical structure through attractor landscapes. Fixed points represent cellular phenotypes; basins trace differentiation pathways. Synchronous vs. asynchronous updating exposes artifacts, guiding model interpretation. Canonical motifs (toggle switches, FFLs) recur across organisms, embodying design principles selected by evolution.
                </p>
                <p>
                    Computational efficiency enables genome-scale analysis: networks with 100+ genes remain tractable via sampling and SAT solvers. Applications span development (Drosophila segment polarity, mammalian cell cycle [AlbertOthmer2003]), disease (cancer pathway dysregulation), and synthetic biology (circuit design verification).
                </p>
                <p>
                    Future directions integrate Boolean logic with omics data: infer network topology from RNA-seq/ChIP-seq, then predict phenotypes via attractor analysis. Hybrid models combine Boolean switches with continuous kinetics; Boolean at regulatory layer, ODEs for downstream signaling. Control-theoretic frameworks optimize interventions (drug combinations) to steer attractors, personalizing therapies.
                </p>
                <p>
                    Boolean models democratize systems biology: accessible to biologists without differential equations training, yet powerful enough for experimentally testable predictions. They capture the essence of Kauffman's vision that qualitative network architecture, more than quantitative details, determines biological order.
                </p>
            </div>

            <!-- References -->
            <div class="math-content">
                <h3>References</h3>
                <div class="reference-list">
                    <div class="reference-item">
                        <span class="reference-citation">[AlbertOthmer2003]</span>
                        <p class="reference-content">
                            R. Albert and H. G. Othmer, <em>The topology of the regulatory interactions predicts the expression pattern of the segment polarity genes in Drosophila melanogaster</em>, Journal of Theoretical Biology <strong>223</strong>, 1-18 (2003).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[Alon2007]</span>
                        <p class="reference-content">
                            U. Alon, <em>Network motifs: theory and experimental approaches</em>, Nature Reviews Genetics <strong>8</strong>, 450-461 (2007).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[Kauffman1969]</span>
                        <p class="reference-content">
                            S. A. Kauffman, <em>Metabolic stability and epigenesis in randomly constructed genetic nets</em>, Journal of Theoretical Biology <strong>22</strong>, 437-467 (1969).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[Shmulevich2002]</span>
                        <p class="reference-content">
                            I. Shmulevich, E. R. Dougherty, S. Kim, and W. Zhang, <em>Probabilistic Boolean networks: a rule-based uncertainty model for gene regulatory networks</em>, Bioinformatics <strong>18</strong>, 261-274 (2002).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[ThieffryThomas1995]</span>
                        <p class="reference-content">
                            D. Thieffry and R. Thomas, <em>Dynamical behaviour of biological regulatory networksâ€”II. Immunity control in bacteriophage lambda</em>, Bulletin of Mathematical Biology <strong>57</strong>, 277-297 (1995).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[WynnReview2012]</span>
                        <p class="reference-content">
                            M. L. Wynn, N. Consul, S. D. Merajver, and S. Schnell, <em>Logic-based models in systems biology: a predictive and parameter-free network analysis method</em>, Integrative Biology <strong>4</strong>, 1323-1337 (2012).
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
</div>

<div class="main-container">
    <div class="project-card">
    <div class="project-header" onclick="toggleContent('biofluid-mechanics')">
        <div class="project-meta">
            <span class="meta-item">
                <i class="fas fa-calendar-alt"></i>
                Added: April 10, 2024
            </span>
            <span class="meta-item">
                <i class="fas fa-clock"></i>
                2hr 10 min read
            </span>
            <span class="meta-item">
                <i class="fas fa-tag"></i>
                Fluid Dynamics & Biology
            </span>
        </div>
        <h2 class="project-title">Biofluid Mechanics: Principles and Physiological Applications</h2>
        <p class="project-subtitle">Author: Clinton Oluranran Kayoh</p>
        <div class="expand-btn" id="btn-biofluid-mechanics">
            <i class="fas fa-plus"></i>
        </div>
    </div>
    <div class="project-content" id="content-biofluid-mechanics">
        <div class="content-inner">
            <!-- Abstract -->
            <div class="math-content">
                <h3>Abstract</h3>
                <p>
                    Biofluid mechanics applies principles of fluid dynamics to biological systems, spanning cellular-scale transport to organ-level hemodynamics. This exposition introduces fundamental conservation laws, dimensionless parameters, and rheological properties governing biological flows, then examines three key application domains: cardiovascular fluid dynamics (steady and pulsatile flow in blood vessels, fluid-structure interaction), respiratory mechanics (airflow in branching airways, gas exchange, mucociliary transport), and microfluidic phenomena (low Reynolds number swimming, microvascular blood flow). Biological fluids exhibit unique challenges non-Newtonian rheology, compliant boundaries, complex geometries, and active transport requiring adaptations of classical fluid mechanics. We derive canonical solutions (Poiseuille flow, Womersley pulsatile flow) and discuss their physiological significance, emphasizing how mathematical analysis informs understanding of disease processes and medical device design. This interdisciplinary field integrates engineering, mathematics, biology, and medicine to elucidate transport phenomena essential to life.
                </p>
            </div>

            <!-- Introduction -->
            <div class="math-content">
                <h3>1. Introduction</h3>
                
                <h4>1.1 Definition and Scope</h4>
                <p>
                    Biofluid mechanics studies the flow of biological fluids blood, air, lymph, cerebrospinal fluid, mucus, cytoplasm and their interactions with living tissues. Applications span eight orders of magnitude in length scale: from molecular transport across cell membranes (\( \sim 10 \) nm) to blood flow in the aorta (\( \sim 2 \) cm diameter). This field underpins physiology, pathology, and biomedical engineering, informing design of artificial organs, drug delivery systems, and diagnostic devices [Fung1997, Ku1997].
                </p>

                <h4>1.2 Biological Context</h4>
                <p>
                    Fluid transport is central to homeostasis. The cardiovascular system delivers oxygen and nutrients while removing metabolic waste; the respiratory system exchanges gases between air and blood; the lymphatic system drains interstitial fluid and mediates immune responses. At cellular scales, cytoplasmic streaming distributes organelles, and bacterial motility enables chemotaxis. Understanding these flows quantitatively requires applying conservation laws of mass, momentum, and energy to complex biological geometries with active regulation.
                </p>

                <h4>1.3 Unique Challenges</h4>
                <p>
                    Biological fluids differ fundamentally from engineering fluids. Blood exhibits shear-thinning viscosity due to red blood cell (RBC) aggregation and deformability [PopelJohnson2005]. Mucus is viscoelastic, exhibiting both viscous and elastic responses. Boundaries are compliant: arterial walls distend with each heartbeat, alveoli expand during breathing. Geometries are intricate branching airways span 23 generations; microvascular networks contain billions of capillaries [West2012]. Moreover, biological systems exhibit active transport: cilia beat, cells swim, molecular motors generate force.
                </p>

                <h4>1.4 Historical Perspective</h4>
                <p>
                    William Harvey (1628) first described blood circulation, recognizing the heart as a pump. Jean Poiseuille (1840s) quantified pressure-flow relationships in capillaries, establishing Poiseuille's law. John Womersley (1955) analyzed pulsatile arterial flow, introducing the Womersley number [Womersley1955]. Thomas Pedley and colleagues formalized respiratory fluid mechanics in the 1970s-80s [Pedley1980]. Edward Purcell (1977) elucidated constraints on swimming at low Reynolds number [Purcell1977]. Today, computational fluid dynamics (CFD) enables patient-specific hemodynamic simulations, while microfluidics revolutionizes diagnostics.
                </p>

                <h4>1.5 Document Roadmap</h4>
                <p>
                    We first present governing equations and dimensionless parameters (Section 2), then examine cardiovascular fluid dynamics including Poiseuille and Womersley flows (Section 3), respiratory mechanics from trachea to alveoli (Section 4), and microfluidic phenomena at cellular scales (Section 5). Section 6 discusses advanced topics, and Section 7 concludes with clinical impact and future directions.
                </p>
            </div>

            <!-- Fundamental Principles -->
            <div class="math-content">
                <h3>2. Fundamental Principles</h3>
                
                <h4>2.1 Governing Equations</h4>
                <p>
                    Biological flows are governed by conservation of mass (continuity) and momentum (Navier-Stokes equations). For incompressible flow with constant density \( \rho \) and viscosity \( \mu \):
                </p>
                <div class="equation">
                    <div class="equation-content">$$\nabla \cdot \mathbf{u} = 0,$$</div>
                    <div class="equation-number">(1)</div>
                </div>
                <div class="equation">
                    <div class="equation-content">$$\rho\left(\frac{\partial \mathbf{u}}{\partial t} + \mathbf{u} \cdot \nabla \mathbf{u}\right) = -\nabla p + \mu \nabla^2 \mathbf{u} + \mathbf{f},$$</div>
                    <div class="equation-number">(2)</div>
                </div>
                <p>
                    where \( \mathbf{u} \) is velocity, \( p \) is pressure, and \( \mathbf{f} \) represents body forces (e.g., gravity). Boundary conditions include no-slip at rigid walls (\( \mathbf{u} = 0 \)) or prescribed motion at compliant walls. Inlet/outlet conditions specify flow rate or pressure.
                </p>

                <h4>2.2 Dimensionless Numbers</h4>
                <p>
                    Dimensionless parameters characterize flow regimes and enable scaling.
                </p>
                <p>
                    <strong>Reynolds Number:</strong> Ratio of inertial to viscous forces,
                </p>
                <div class="equation">
                    <div class="equation-content">$$Re = \frac{\rho U L}{\mu},$$</div>
                    <div class="equation-number">(3)</div>
                </div>
                <p>
                    where \( U \) is characteristic velocity and \( L \) is length scale. For \( Re \ll 1 \), viscous forces dominate (Stokes flow); for \( Re > 2000 \), turbulence typically emerges. Blood flow in the aorta has \( Re \sim 1000 \) (transitional), while capillary flow has \( Re \sim 10^{-3} \) (laminar).
                </p>
                <p>
                    <strong>Womersley Number:</strong> For oscillatory flows (frequency \( \omega \)),
                </p>
                <div class="equation">
                    <div class="equation-content">$$\alpha = L\sqrt{\frac{\omega\rho}{\mu}}$$</div>
                    <div class="equation-number">(4)</div>
                </div>
                <p>
                    measures the ratio of unsteady inertial forces to viscous forces. When \( \alpha \ll 1 \), flow quasi-steadily tracks pressure; when \( \alpha \gg 1 \), inertia creates phase lags [Womersley1955].
                </p>
                <p>
                    <strong>Strouhal Number:</strong> For flows with characteristic frequency \( f \),
                </p>
                <div class="equation">
                    <div class="equation-content">$$St = \frac{fL}{U}$$</div>
                    <div class="equation-number">(5)</div>
                </div>
                <p>
                    quantifies unsteadiness relative to convection. In cardiovascular flows, \( St \sim 0.1 \), indicating significant pulsatility.
                </p>

                <h4>2.3 Rheological Properties</h4>
                <p>
                    <strong>Newtonian Fluids:</strong> Water and plasma exhibit constant viscosity \( \mu \) (plasma: \( \mu \approx 1.2 \) cP).
                </p>
                <p>
                    <strong>Non-Newtonian Behavior:</strong> Whole blood is shear-thinning: viscosity decreases with increasing shear rate \( \dot{\gamma} \) due to RBC alignment and disaggregation. The Carreau-Yasuda model captures this:
                </p>
                <div class="equation">
                    <div class="equation-content">$$\mu_{\text{eff}}(\dot{\gamma}) = \mu_\infty + \frac{\mu_0 - \mu_\infty}{[1 + (\lambda\dot{\gamma})^a]^{(n-1)/a}},$$</div>
                    <div class="equation-number">(6)</div>
                </div>
                <p>
                    where \( \mu_0 \approx 6 \) cP (low shear), \( \mu_\infty \approx 3 \) cP (high shear), and \( n < 1 \) indicates shear-thinning. Mucus exhibits viscoelasticity, characterized by complex modulus \( G^*(\omega) \).
                </p>
                <p>
                    <strong>FÃ¥hraeus-Lindqvist Effect:</strong> In vessels \( < 300 \) \( \mu \)m, RBCs migrate centrally, creating a cell-free plasma layer near walls. This reduces apparent viscosity, enhancing flow [SecombReview2017].
                </p>

                <div class="table-container">
                    <table class="data-table">
                        <caption>Characteristic scales in biological fluid systems</caption>
                        <thead>
                            <tr>
                                <th>System</th>
                                <th>Length \( L \) (cm)</th>
                                <th>Velocity \( U \) (cm/s)</th>
                                <th>\( Re \)</th>
                                <th>\( \alpha \)</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr><td>Aorta</td><td>2.5</td><td>50</td><td>1000</td><td>20</td></tr>
                            <tr><td>Arteriole</td><td>0.01</td><td>0.5</td><td>0.05</td><td>0.05</td></tr>
                            <tr><td>Capillary</td><td>0.0008</td><td>0.05</td><td>\( 4 \times 10^{-4} \)</td><td>--</td></tr>
                            <tr><td>Trachea (inhalation)</td><td>1.8</td><td>200</td><td>2000</td><td>--</td></tr>
                            <tr><td>Alveolus</td><td>0.03</td><td>0.01</td><td>\( 2 \times 10^{-4} \)</td><td>--</td></tr>
                            <tr><td>E. coli swimming</td><td>\( 2 \times 10^{-4} \)</td><td>\( 3 \times 10^{-3} \)</td><td>\( 6 \times 10^{-5} \)</td><td>--</td></tr>
                        </tbody>
                    </table>
                </div>
            </div>

            <!-- Cardiovascular Fluid Dynamics -->
            <div class="math-content">
                <h3>3. Cardiovascular Fluid Dynamics</h3>
                
                <h4>3.1 Steady Flow in Blood Vessels: Poiseuille Flow</h4>
                <p>
                    Consider steady, fully developed laminar flow in a rigid cylindrical vessel of radius \( R \) and length \( L \), driven by pressure drop \( \Delta p \). Assuming axial symmetry and no body forces, the Navier-Stokes equation reduces to:
                </p>
                <div class="equation">
                    <div class="equation-content">$$\mu \frac{1}{r}\frac{d}{dr}\left(r\frac{du}{dr}\right) = \frac{\Delta p}{L}.$$</div>
                    <div class="equation-number">(7)</div>
                </div>
                <p>
                    Integrating twice with boundary conditions \( du/dr|_{r=0} = 0 \) (symmetry) and \( u(R) = 0 \) (no-slip):
                </p>
                <div class="equation">
                    <div class="equation-content">$$u(r) = \frac{\Delta p}{4\mu L}(R^2 - r^2).$$</div>
                    <div class="equation-number">(8)</div>
                </div>
                <p>
                    The velocity profile is parabolic, maximum at the centerline (\( u_{\max} = \Delta p R^2/(4\mu L) \)). Integrating over the cross-section yields flow rate:
                </p>
                <div class="equation">
                    <div class="equation-content">$$Q = \int_0^R u(r) \cdot 2\pi r \, dr = \frac{\pi R^4 \Delta p}{8\mu L}.$$</div>
                    <div class="equation-number">(9)</div>
                </div>
                <p>
                    This is the <em>Hagen-Poiseuille law</em>. Vascular resistance \( R_v = \Delta p / Q = 8\mu L/(\pi R^4) \) scales inversely with \( R^4 \), making radius the dominant factor.
                </p>
                <p>
                    <strong>Biological Significance:</strong> A 50% diameter reduction (stenosis) increases resistance 16-fold, drastically reducing perfusion. Hypertension (elevated \( \Delta p \)) compensates but strains vessel walls, promoting atherosclerosis [NicholsORourke2005].
                </p>

                <h4>3.2 Pulsatile Flow: Womersley Solution</h4>
                <p>
                    Heartbeat generates oscillatory pressure gradients. Consider flow in a rigid cylindrical tube with sinusoidal pressure gradient:
                </p>
                <div class="equation">
                    <div class="equation-content">$$\frac{\partial p}{\partial z} = -A\cos(\omega t),$$</div>
                    <div class="equation-number">(10)</div>
                </div>
                <p>
                    where \( \omega = 2\pi f \) (heart rate \( f \approx 1 \) Hz). The axial Navier-Stokes equation becomes:
                </p>
                <div class="equation">
                    <div class="equation-content">$$\rho\frac{\partial u}{\partial t} = A\cos(\omega t) + \mu \frac{1}{r}\frac{\partial}{\partial r}\left(r\frac{\partial u}{\partial r}\right).$$</div>
                    <div class="equation-number">(11)</div>
                </div>
                <p>
                    Seeking solutions of the form \( u(r,t) = \text{Re}[U(r)e^{i\omega t}] \), we obtain:
                </p>
                <div class="equation">
                    <div class="equation-content">$$\frac{d^2 U}{dr^2} + \frac{1}{r}\frac{dU}{dr} - i\frac{\omega\rho}{\mu}U = -\frac{A}{\mu}.$$</div>
                    <div class="equation-number">(12)</div>
                </div>
                <p>
                    The solution involves Bessel functions \( J_0 \) of complex argument. Defining the Womersley number \( \alpha \) (Eq. 4), the velocity profile is:
                </p>
                <div class="equation">
                    <div class="equation-content">$$u(r,t) = \text{Re}\left[\frac{A}{i\omega\rho}\left(1 - \frac{J_0(i^{3/2}\alpha r/R)}{J_0(i^{3/2}\alpha)}\right)e^{i\omega t}\right].$$</div>
                    <div class="equation-number">(13)</div>
                </div>
                <p>
                    <strong>Interpretation:</strong> For \( \alpha \ll 1 \) (e.g., arterioles), flow is quasi-steady, recovering Poiseuille profiles at each instant. For \( \alpha \gg 1 \) (e.g., aorta), velocity profiles flatten near the centerline, with boundary layers near walls where viscous effects concentrate [Womersley1955]. Flow lags pressure by phase angle \( \phi \approx \pi/4 \) for large \( \alpha \).
                </p>
                <div class="figure-container">
                <img src="Umax.png" alt="Umax" class="figure-image" style="width: 80%; max-width: 800px;max-height: 900px;">
                <div class="figure-caption">
                    <p><strong>Figure 1:</strong> Velocity profiles for Poiseuille (steady) flow showing parabolic shape, and Womersley (pulsatile) flow at peak systole for \( \alpha = 10 \) showing flattened central core with steep gradients near walls. Axes: radial position \( r/R \) vs. velocity \( u/u_{\max} \).</p>
                </div>
                </div>

                <h4>3.3 Arterial Wall Mechanics: Fluid-Structure Interaction</h4>
                <p>
                    Arterial walls are elastic (Young's modulus \( E \sim 10^5 \) Pa, wall thickness \( h \sim 1 \) mm). Pulsatile pressure distends vessels, storing energy during systole and releasing it during diastole, smoothing flow. Pulse wave speed is given by the Moens-Korteweg equation:
                </p>
                <div class="equation">
                    <div class="equation-content">$$c = \sqrt{\frac{Eh}{2\rho R}}.$$</div>
                    <div class="equation-number">(14)</div>
                </div>
                <p>
                    For typical values, \( c \approx 5 \) m/s in elastic arteries, increasing to \( 15 \) m/s in stiff aortas (aging, hypertension).
                </p>
                <p>
                    <strong>Physiological Relevance:</strong> Arterial stiffening elevates systolic pressure and pulse wave velocity, increasing cardiac workload and promoting end-organ damage. Aneurysms (localized wall weakening) alter flow patterns, creating regions of low wall shear stress prone to thrombosis [NicholsORourke2005].
                </p>
            </div>

            <!-- Respiratory Fluid Mechanics -->
            <div class="math-content">
                <h3>4. Respiratory Fluid Mechanics</h3>
                
                <h4>4.1 Airflow in the Lung</h4>
                <p>
                    The respiratory tree branches dichotomously from trachea (\( \sim 2 \) cm diameter) through 23 generations to alveoli (\( \sim 300 \) \( \mu \)m diameter). Total cross-sectional area increases exponentially, reducing flow velocity: tracheal velocity \( \sim 2 \) m/s during inspiration drops to \( \sim 0.01 \) cm/s in alveoli.
                </p>
                <p>
                    <strong>Flow Regimes:</strong> In the trachea, \( Re \sim 2000 \) (turbulent during exercise); in bronchioles (generation 10-15), \( Re \sim 1 \) (laminar); in alveoli, \( Re \ll 1 \) (diffusion-dominated). The Weibel model idealizes symmetric branching, enabling estimation of total airway resistance [West2012]:
                </p>
                <div class="equation">
                    <div class="equation-content">$$R_{\text{total}} = \sum_{n=0}^{23} \frac{8\mu L_n}{\pi r_n^4 N_n},$$</div>
                    <div class="equation-number">(15)</div>
                </div>
                <p>
                    where \( N_n \) is the number of airways in generation \( n \). Paradoxically, resistance is highest in generations 4-8 (intermediate bronchi), not the smallest airways, due to parallel arrangement.
                </p>

                <h4>4.2 Alveolar Gas Exchange</h4>
                <p>
                    Oxygen diffuses from alveolar air (partial pressure \( P_{A,O_2} \approx 100 \) mmHg) across the alveolar-capillary membrane (thickness \( \sim 0.5 \) \( \mu \)m) into blood (\( P_{v,O_2} \approx 40 \) mmHg). Fick's law:
                </p>
                <div class="equation">
                    <div class="equation-content">$$\dot{V}_{O_2} = D_{O_2} \cdot A \cdot \frac{P_{A,O_2} - P_{\bar{c},O_2}}{\Delta x},$$</div>
                    <div class="equation-number">(16)</div>
                </div>
                <p>
                    where \( D_{O_2} \) is diffusion coefficient, \( A \approx 70 \) m\(^2\) is alveolar surface area, and \( \Delta x \) is membrane thickness. Transit time through capillaries (\( \sim 0.75 \) s) suffices for near-complete equilibration in healthy lungs but not in fibrosis (thickened membrane) or emphysema (reduced \( A \)).
                </p>
                <p>
                    <strong>Surface Tension and Surfactant:</strong> Alveoli are lined with fluid, creating air-liquid interface with surface tension \( \gamma \). Laplace's law predicts pressure difference:
                </p>
                <div class="equation">
                    <div class="equation-content">$$\Delta p = \frac{2\gamma}{R}.$$</div>
                    <div class="equation-number">(17)</div>
                </div>
                <p>
                    Without surfactant, small alveoli (smaller \( R \)) would have higher pressure, emptying into larger ones (instability). Pulmonary surfactant reduces \( \gamma \) from \( 70 \) to \( 25 \) mN/m, stabilizing alveoli and reducing work of breathing. In acute respiratory distress syndrome (ARDS), surfactant dysfunction causes widespread alveolar collapse [West2012].
                </p>
                <div class="figure-container">
                <img src="Respiratory_Tree.png" alt="Respiratory Tree" class="figure-image">
                <div class="figure-caption">
                    <p><strong>Figure 2:</strong> Schematic of respiratory tree showing trachea, main bronchi, bronchioles, and alveolar sacs. Annotations indicate flow regimes (turbulent, laminar, diffusion-dominated) and Reynolds numbers at each level. Inset shows alveolar sac detail with capillary network and arrows indicating O\(_2\)/CO\(_2\) exchange.</p>
                </div>
                </div>

                <h4>4.3 Mucus Transport and Mucociliary Clearance</h4>
                <p>
                    Airway epithelium is covered by two layers: a low-viscosity periciliary layer (\( \sim 7 \) \( \mu \)m thick) where cilia beat, and an overlying mucus gel (\( \sim 5 \) \( \mu \)m thick, viscoelastic). Cilia beat at \( \sim 15 \) Hz with power stroke in mucus layer, propelling mucus toward pharynx at \( \sim 1 \) cm/min.
                </p>
                <p>
                    <strong>Mathematical Model:</strong> Using lubrication theory for thin films, shear stress \( \tau \) exerted by cilia:
                </p>
                <div class="equation">
                    <div class="equation-content">$$\tau = \mu_{\text{pcl}} \frac{U_{\text{cilia}}}{h_{\text{pcl}}},$$</div>
                    <div class="equation-number">(18)</div>
                </div>
                <p>
                    drives mucus flow. Mucus velocity \( U_{\text{mucus}} \sim \tau h_{\text{mucus}}/\mu_{\text{mucus}} \).
                </p>
                <p>
                    <strong>Clinical Relevance:</strong> In cystic fibrosis, dehydrated mucus (increased \( \mu_{\text{mucus}} \)) impairs clearance, causing chronic infections. Therapies aim to hydrate mucus or increase ciliary beat frequency [Pedley1980].
                </p>
            </div>

            <!-- Microfluidics and Cellular-Scale Flows -->
            <div class="math-content">
                <h3>5. Microfluidics and Cellular-Scale Flows</h3>
                
                <h4>5.1 Low Reynolds Number Flows</h4>
                <p>
                    At cellular scales (bacteria: \( L \sim 1 \) \( \mu \)m, \( U \sim 30 \) \( \mu \)m/s), \( Re \sim 10^{-5} \). Inertia is negligible; the Navier-Stokes equation reduces to Stokes equation:
                </p>
                <div class="equation">
                    <div class="equation-content">$$\mu \nabla^2 \mathbf{u} = \nabla p.$$</div>
                    <div class="equation-number">(19)</div>
                </div>
                <p>
                    Flow is reversible: reversing time (or stroke direction) reverses flow. The <em>scallop theorem</em> states that reciprocal motion (e.g., opening-closing symmetrically) produces no net displacement organisms must use non-reciprocal strokes [Purcell1977].
                </p>

                <h4>5.2 Swimming Microorganisms</h4>
                <p>
                    <strong>Flagellar Propulsion:</strong> E. coli rotates helical flagella (bundles form behind cell body), creating thrust. Resistive force theory models drag on slender filaments:
                </p>
                <div class="equation">
                    <div class="equation-content">$$F_{\parallel} = \xi_{\parallel} v_{\parallel}, \quad F_{\perp} = \xi_{\perp} v_{\perp},$$</div>
                    <div class="equation-number">(20)</div>
                </div>
                <p>
                    where \( \xi_{\parallel}/\xi_{\perp} \approx 0.5 \) for cylinders. Rotating a helix generates net thrust. Sperm cells similarly use undulatory flagellar waves, with propulsion efficiency \( \sim 1 \)-\( 2 \)% (most energy dissipated to viscosity) [LaugaPowers2009].
                </p>
                <div class="figure-container">
                <img src="Ecoli.png" alt="Ecoli" class="figure-image" style="width: 80%; max-width: 800px; max-height: 900px;">
                <div class="figure-caption">
                    <p><strong>Figure 3:</strong> Streamlines around a swimming E. coli bacterium showing flow disturbance localized near cell body (\( \sim 2 \) \( \mu \)m radius). Arrows indicate flagellar rotation and resulting thrust direction. Inset: velocity decay \( u \sim 1/r \) far from body, characteristic of Stokes flow.</p>
                </div>
                </div>

                <h4>5.3 Blood Flow in Microcirculation</h4>
                <p>
                    Capillaries (diameter \( 5 \)-\( 10 \) \( \mu \)m) are comparable to RBC size (\( \sim 8 \) \( \mu \)m). RBCs deform (biconcave to parachute shape) to traverse capillaries, increasing resistance but enabling gas exchange.
                </p>
                <p>
                    <strong>Cell-Free Layer:</strong> RBCs migrate toward vessel center via inertial lift and deformation-induced forces, creating a plasma layer (\( \sim 1 \) \( \mu \)m) near walls. This reduces apparent viscosity (FÃ¥hraeus-Lindqvist effect) and facilitates nutrient transport [PopelJohnson2005, SecombReview2017].
                </p>
                <p>
                    <strong>Margination:</strong> Platelets and leukocytes, being less deformable, are pushed to vessel periphery (margination), enabling adhesion to endothelium for hemostasis and immune surveillance. Mathematical models couple RBC dynamics (spring networks) with Stokes flow to predict margination [PopelJohnson2005].
                </p>

                <h4>5.4 Lab-on-a-Chip Applications</h4>
                <p>
                    Microfluidic devices exploit low-\( Re \) flows for diagnostics and research. <strong>Flow focusing</strong> uses sheath flows to confine sample streams (\( \sim 1 \) \( \mu \)m wide), enabling high-resolution cytometry. <strong>Organ-on-chip</strong> platforms recreate vascular networks, mimicking hemodynamic shear stress (\( \sim 10 \) dyn/cm\(^2\)) to study endothelial dysfunction, drug responses, or cancer metastasis. Precise flow control at microscales enables single-cell analysis and personalized medicine.
                </p>
            </div>

            <!-- Advanced Topics and Challenges -->
            <div class="math-content">
                <h3>6. Advanced Topics and Challenges</h3>
                
                <h4>6.1 Computational Fluid Dynamics</h4>
                <p>
                    Patient-specific CFD uses medical imaging (MRI, CT) to reconstruct vascular geometries and simulate hemodynamics. Applications include predicting aneurysm rupture risk, optimizing stent placement, and planning surgical interventions. Fluid-structure interaction (FSI) simulations couple fluid solvers with solid mechanics to model compliant arteries, requiring immense computational resources (hours to days on HPC clusters). Challenges include boundary condition uncertainty (outflow impedance) and turbulence modeling.
                </p>

                <h4>6.2 Multiphase Flows</h4>
                <p>
                    Blood is a suspension: modeling individual RBCs (billions per mL) is prohibitive. Continuum models (Eq. 6) work at macroscales but fail in capillaries. Discrete particle methods (immersed boundary, lattice Boltzmann) resolve cell-scale dynamics but are costly. Thrombosis platelet aggregation forming clots is inherently multiphase, involving biochemical activation, adhesion, and flow-induced shear [Ku1997].
                </p>

                <h4>6.3 Active Matter</h4>
                <p>
                    Bacterial suspensions exhibit collective motion and enhanced mixing, modeled by active stress in Stokes equation. Cytoplasmic streaming in cells (e.g., oocytes) is driven by myosin motors on actin networks, creating flows \( \sim 1 \) \( \mu \)m/s that distribute organelles. These systems defy equilibrium thermodynamics, requiring new theoretical frameworks.
                </p>
            </div>

            <!-- Conclusion -->
            <div class="math-content">
                <h3>7. Conclusion</h3>
                <p>
                    Biofluid mechanics bridges fundamental physics and life sciences, revealing how flow governs physiological function and disease. We have examined governing equations (Navier-Stokes, Stokes), dimensionless parameters (Reynolds, Womersley numbers), and rheology (shear-thinning blood, viscoelastic mucus), then applied these to cardiovascular systems (Poiseuille and pulsatile flows, arterial compliance), respiratory mechanics (branching airways, surfactant-stabilized alveoli, mucociliary clearance), and microfluidics (low-\( Re \) swimming, microvascular phenomena).
                </p>
                <p>
                    <strong>Clinical Impact:</strong> Understanding biofluid mechanics informs design of heart valves, stents, and ventilators; predicts thrombosis and aneurysm rupture; and guides therapies for hypertension, ARDS, and cystic fibrosis. Microfluidics enables point-of-care diagnostics and organ-on-chip drug screening.
                </p>
                <p>
                    <strong>Future Directions:</strong> Integration of real-time imaging with CFD promises personalized hemodynamics. Machine learning accelerates flow predictions from sparse data. Multi-scale modeling from molecular motors to organ systems remains a grand challenge. Addressing these requires collaboration among engineers, mathematicians, biologists, and clinicians, exemplifying biofluid mechanics' interdisciplinary essence. As Harvey recognized circulation's centrality four centuries ago, we now quantify it, harnessing fluid mechanics to decode life's flows.
                </p>
            </div>

            <!-- References -->
            <div class="math-content">
                <h3>References</h3>
                <div class="reference-list">
                    <div class="reference-item">
                        <span class="reference-citation">[Fung1997]</span>
                        <p class="reference-content">
                            Y. C. Fung, <em>Biomechanics: Circulation</em>, 2nd edition, Springer (1997).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[Ku1997]</span>
                        <p class="reference-content">
                            D. N. Ku, <em>Blood flow in arteries</em>, Annual Review of Fluid Mechanics <strong>29</strong>, 399-434 (1997).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[LaugaPowers2009]</span>
                        <p class="reference-content">
                            E. Lauga and T. R. Powers, <em>The hydrodynamics of swimming microorganisms</em>, Reports on Progress in Physics <strong>72</strong>, 096601 (2009).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[NicholsORourke2005]</span>
                        <p class="reference-content">
                            W. W. Nichols and D. A. McDonald, <em>McDonald's Blood Flow in Arteries: Theoretical, Experimental and Clinical Principles</em>, 6th edition, Hodder Arnold (2011).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[Pedley1980]</span>
                        <p class="reference-content">
                            T. J. Pedley, <em>The Fluid Mechanics of Large Blood Vessels</em>, Cambridge University Press (1980).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[PopelJohnson2005]</span>
                        <p class="reference-content">
                            A. S. Popel and P. C. Johnson, <em>Microcirculation and hemorheology</em>, Annual Review of Fluid Mechanics <strong>37</strong>, 43-69 (2005).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[Purcell1977]</span>
                        <p class="reference-content">
                            E. M. Purcell, <em>Life at low Reynolds number</em>, American Journal of Physics <strong>45</strong>, 3-11 (1977).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[SecombReview2017]</span>
                        <p class="reference-content">
                            T. W. Secomb, <em>Blood flow in the microcirculation</em>, Annual Review of Fluid Mechanics <strong>49</strong>, 443-461 (2017).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[West2012]</span>
                        <p class="reference-content">
                            J. B. West, <em>Respiratory Physiology: The Essentials</em>, 9th edition, Lippincott Williams & Wilkins (2012).
                        </p>
                    </div>

                    <div class="reference-item">
                        <span class="reference-citation">[Womersley1955]</span>
                        <p class="reference-content">
                            J. R. Womersley, <em>Method for the calculation of velocity, rate of flow and viscous drag in arteries when the pressure gradient is known</em>, The Journal of Physiology <strong>127</strong>, 553-563 (1955).
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
</div>

<div class="main-container">
<div class="project-card">
    <div class="project-header" onclick="toggleContent('fixed-point-project')">
        <div class="project-meta">
            <span class="meta-item">
                <i class="fas fa-calendar-alt"></i>
                Added:  February 12, 2024
            </span>
            <span class="meta-item">
                <i class="fas fa-clock"></i>
                1hr 55 min read
            </span>
            <span class="meta-item">
                <i class="fas fa-tag"></i>
                Numerical Methods
            </span>
        </div>
        <h2 class="project-title">Fixed Point Iterative Algorithms: Theory, Methods, and Applications</h2>
        <p class="project-subtitle">Author: Clinton Oluranran Kayoh</p>
        <div class="expand-btn" id="btn-fixed-point-project">
            <i class="fas fa-plus"></i>
        </div>
    </div>
    <div class="project-content" id="content-fixed-point-project">
        <div class="content-inner">
            <!-- Abstract -->
            <div class="math-content">
                <h3>Abstract</h3>
                <p>
                    Fixed point iterative algorithms constitute a fundamental class of numerical methods for solving equations, optimization problems, and dynamical systems. This exposition provides a comprehensive treatment of fixed point theory, beginning with Banach's contraction mapping principle and progressing through classical algorithms including Newton's method, secant methods, and successive over-relaxation. We analyze convergence properties, derive error bounds, and examine order of convergence for various schemes. Practical considerations including stopping criteria, convergence acceleration techniques, and multidimensional extensions are discussed. Through detailed worked examples and theoretical analysis, we demonstrate how fixed point formulations unify diverse computational methods and provide powerful tools for understanding iterative solution strategies in scientific computing.
                </p>
            </div>

            <!-- Introduction -->
            <div class="math-content">
                <h3>1. Introduction</h3>
                
                <h4>1.1 Motivation and Historical Context</h4>
                
                <p><strong>What is a Fixed Point?</strong> A <em>fixed point</em> of a function \(g: X \to X\) is a point \(x^* \in X\) satisfying</p>
                <div class="equation">
                    <div class="equation-content">$$g(x^*) = x^*.$$</div>
                    <div class="equation-number">(1.1)</div>
                </div>
                <p>Fixed points represent equilibrium states where the function leaves the input unchanged, a concept with profound implications across mathematics, physics, economics, and computer science.</p>

                <p><strong>Why Fixed Point Iteration?</strong> Many problems naturally reduce to finding fixed points. Root-finding (\(f(x) = 0\)) becomes fixed point iteration via \(g(x) = x - f(x)\) or more sophisticated transformations. Fixed point formulations offer several advantages: (1) <em>Simplicity</em>: the iteration \(x_{n+1} = g(x_n)\) is conceptually straightforward; (2) <em>Convergence guarantees</em>: under mild conditions (contractivity), convergence to a unique solution is assured; (3) <em>Generality</em>: applicable to transcendental equations, implicit relations, and operator equations where direct methods fail [BurdenFaires2010, Atkinson1989].</p>

                <p><strong>Historical Development.</strong> The rigorous theory of fixed points emerged in the early 20th century. Stefan Banach's contraction mapping theorem (1922) provided constructive existence and uniqueness results for fixed points in complete metric spaces [Ortega2000]. L.E.J. Brouwer's fixed point theorem (1911) established existence for continuous maps on compact convex sets, though non-constructively. These foundational results underpin modern iterative methods in numerical analysis, functional analysis, and differential equations.</p>

                <p><strong>Applications Preview.</strong> Fixed point methods pervade computational mathematics: root-finding for nonlinear equations, optimization via gradient descent (fixed point of \(x - \nabla f(x)\)), numerical integration of ODEs through Picard iteration, equilibrium computation in game theory (Nash equilibria as fixed points), and solution of integral equations arising in physics and engineering [Kelley1995].</p>

                <h4>1.2 Document Scope</h4>
                <p>
                    This exposition develops fixed point iteration from theoretical foundations through practical implementations. Section 2 establishes existence and convergence theory via Banach's theorem, analyzes error propagation, and introduces convergence orders. Section 3 reinterprets classical algorithms, Newton's method, secant method, successive over-relaxation, and Picard iteration as fixed point schemes, deriving their convergence rates. Section 4 addresses practical concerns: stopping criteria, acceleration techniques (Aitken's \(\Delta^2\) process), designing effective iteration functions, and multidimensional extensions. Section 5 surveys advanced topics including quasi-Newton methods, applications in dynamical systems and PDEs, and global convergence strategies. Throughout, worked examples illustrate concepts concretely.
                </p>
            </div>

            <!-- Theoretical Foundation -->
            <div class="math-content">
                <h3>2. Theoretical Foundation</h3>
                
                <h4>2.1 Fixed Point Existence</h4>
                
                <div class="theorem">
                    <p class="theorem-title">Theorem 2.1 (Banach Fixed Point Theorem)</p>
                    <p>Let \((X, d)\) be a complete metric space and \(g: X \to X\) a <em>contraction mapping</em>, i.e., there exists \(L \in [0,1)\) such that</p>
                    <div class="equation">
                        <div class="equation-content">$$d(g(x), g(y)) \leq L \cdot d(x, y) \quad \forall x, y \in X.$$</div>
                        <div class="equation-number">(2.1)</div>
                    </div>
                    <p>Then:</p>
                    <ol>
                        <li>\(g\) has a unique fixed point \(x^* \in X\).</li>
                        <li>For any initial guess \(x_0 \in X\), the sequence \(x_{n+1} = g(x_n)\) converges to \(x^*\).</li>
                        <li>The error satisfies \(d(x_n, x^*) \leq \frac{L^n}{1-L} d(x_1, x_0)\).</li>
                    </ol>
                </div>

                <div class="proof">
                    <p class="proof-title">Proof Sketch</p>
                    <p>Consider the sequence \(\{x_n\}\) with \(x_{n+1} = g(x_n)\). By contractivity, \(d(x_{n+1}, x_n) \leq L d(x_n, x_{n-1}) \leq L^n d(x_1, x_0)\). The triangle inequality yields \(d(x_n, x_m) \leq \sum_{k=n}^{m-1} d(x_{k+1}, x_k) \leq d(x_1, x_0) \sum_{k=n}^{m-1} L^k \to 0\) as \(n, m \to \infty\), proving \(\{x_n\}\) is Cauchy. Completeness ensures convergence to some \(x^* \in X\). Continuity of \(g\) (implied by Lipschitz condition) gives \(x^* = \lim x_{n+1} = \lim g(x_n) = g(x^*)\). Uniqueness: if \(g(y) = y\), then \(d(x^*, y) = d(g(x^*), g(y)) \leq L d(x^*, y)\), implying \(d(x^*, y) = 0\).</p>
                </div>

                <div class="remark">
                    <p class="remark-title">Remark</p>
                    <p>For \(X = \mathbb{R}\) with \(d(x,y) = |x - y|\), Equation (2.1) becomes \(|g(x) - g(y)| \leq L|x - y|\). If \(g\) is differentiable, the mean value theorem gives \(|g(x) - g(y)| = |g'(\xi)||x - y|\) for some \(\xi \in (x, y)\), so \(|g'(x)| \leq L < 1\) suffices for contractivity.</p>
                </div>

                <p><strong>Brouwer Fixed Point Theorem.</strong> For continuous \(g: [a,b] \to [a,b]\) (or more generally, \(g: K \to K\) for compact convex \(K \subset \mathbb{R}^n\)), a fixed point exists, though uniqueness and constructive convergence are not guaranteed [Atkinson1989].</p>

                <h4>2.2 Basic Fixed Point Iteration</h4>

                <div class="algorithm">
                    <p class="algorithm-title">Algorithm 2.1: Basic Fixed Point Iteration</p>
                    <div class="algorithm-content">
                        <p><strong>Input:</strong> Function \(g\), initial guess \(x_0\), tolerance \(\epsilon\), max iterations \(N\)</p>
                        <p><strong>Output:</strong> Approximate fixed point \(x^*\)</p>
                        <p>\(x \gets x_0\)</p>
                        <p><strong>for</strong> \(n = 0\) <strong>to</strong> \(N-1\) <strong>do</strong></p>
                        <p style="margin-left: 20px;">\(x_{\text{new}} \gets g(x)\)</p>
                        <p style="margin-left: 20px;"><strong>if</strong> \(|x_{\text{new}} - x| < \epsilon\) <strong>then</strong></p>
                        <p style="margin-left: 40px;"><strong>return</strong> \(x_{\text{new}}\)</p>
                        <p style="margin-left: 20px;"><strong>end if</strong></p>
                        <p style="margin-left: 20px;">\(x \gets x_{\text{new}}\)</p>
                        <p><strong>end for</strong></p>
                        <p><strong>return</strong> \(x\) (or indicate non-convergence)</p>
                    </div>
                </div>

                <p>The iteration \(x_{n+1} = g(x_n)\) for \(n = 0, 1, 2, \ldots\) generates a sequence converging to \(x^*\) under Theorem 2.1. 
                As visualized in <a href="#figure-1" class="figure-ref">Figure 1</a>, the cobweb diagram provides an intuitive geometric interpretation of fixed point iteration.</p>
                  <!-- Figure 1 placed here -->
          <div class="figure-container">
        <img src="Cobweb.png" alt="Cobweb Diagram Visualization" class="figure-image" style="width: 80%; max-width: 1000px; max-height: 1200px;">
        <div class="figure-caption">
            <strong>Figure 1:</strong> Cobweb diagram illustrating fixed point iteration. The diagonal line \(y = x\) and iteration function \(y = g(x)\) are shown. Starting from \(x_0\) on the x-axis, vertical lines connect to \(g(x_0)\), then horizontal lines connect back to the diagonal (giving \(x_1\)). For \(|g'(x^*)| < 1\), the cobweb spirals inward to the fixed point \(x^*\) at the intersection. For \(|g'(x^*)| > 1\), it spirals outward (divergence).
        </div>
        </div>
                <p><strong>Equivalence to Root-Finding.</strong> Solving \(f(x) = 0\) is equivalent to finding fixed points of \(g(x) = x + \alpha f(x)\) for any \(\alpha \neq 0\) (since \(g(x^*) = x^*\) iff \(f(x^*) = 0\)). Alternative forms include \(g(x) = x - f(x)\) or \(g(x) = x - \frac{f(x)}{f'(x)}\) (Newton's method). The choice of \(g\) critically impacts convergence rate.</p>

                <h4>2.3 Convergence Analysis</h4>

                <p><strong>Local Convergence.</strong> Assume \(g\) is continuously differentiable near a fixed point \(x^*\). Taylor expansion gives:</p>
                <div class="equation">
                    <div class="equation-content">$$x_{n+1} - x^* = g(x_n) - g(x^*) = g'(x^*)(x_n - x^*) + O((x_n - x^*)^2).$$</div>
                    <div class="equation-number">(2.2)</div>
                </div>
                <p>Define error \(e_n = x_n - x^*\). Then:</p>
                <div class="equation">
                    <div class="equation-content">$$e_{n+1} \approx g'(x^*) e_n.$$</div>
                    <div class="equation-number">(2.3)</div>
                </div>
                <p>If \(|g'(x^*)| < 1\), errors decay geometrically; convergence is <em>linear</em>. If \(|g'(x^*)| \geq 1\), iteration diverges unless \(x_0 = x^*\) exactly.</p>

                <p><strong>Order of Convergence.</strong> The iteration converges with <em>order</em> \(p\) if:</p>
                <div class="equation">
                    <div class="equation-content">$$\lim_{n \to \infty} \frac{|e_{n+1}|}{|e_n|^p} = C$$</div>
                    <div class="equation-number">(2.4)</div>
                </div>
                <p>for some constant \(0 < C < \infty\) [Ortega2000].</p>
                <ul>
                    <li>\(p = 1\): <em>Linear convergence</em> with \(C = |g'(x^*)|\) (requires \(g'(x^*) \neq 0\)).</li>
                    <li>\(1 < p < 2\): <em>Superlinear convergence</em>.</li>
                    <li>\(p = 2\): <em>Quadratic convergence</em> (requires \(g'(x^*) = 0\), \(g''(x^*) \neq 0\)).</li>
                </ul>

                <p>Higher order convergence dramatically reduces iteration count. For quadratic convergence, correct digits roughly double each iteration (once errors are small). The convergence rates discussed theoretically are demonstrated empirically in 
<a href="#figure-2" class="figure-ref">Figure 2</a>, which clearly shows the advantage of higher-order methods.</p>
               <!-- Figure 2 placed here -->
         <div class="figure-container">
        <img src="Convergence_Comparison.png" alt="Convergence Rate Comparison" class="figure-image" style="width: 80%; max-width: 800px; max-height: 900px;">
        <div class="figure-caption">
            <strong>Figure 2:</strong> Convergence comparison showing \(\log_{10}|e_n|\) versus iteration number \(n\) for different methods solving the same problem. Newton's method shows a steep linear decline (slope proportional to \(2^n\), indicating quadratic convergence). The secant method displays less steep but still rapid convergence. Basic fixed point iteration shows gentle linear decline. This demonstrates the dramatic advantage of higher-order methods once within the convergence basin.
        </div>
    </div>
            
            </div>

            <!-- Classical Algorithms -->
            <div class="math-content">
                <h3>3. Classical Algorithms as Fixed Point Iterations</h3>
                
                <h4>3.1 Newton's Method</h4>

                <p>To solve \(f(x) = 0\), Newton's method iterates:</p>
                <div class="equation">
                    <div class="equation-content">$$x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}.$$</div>
                    <div class="equation-number">(3.1)</div>
                </div>
                <p>This is fixed point iteration with \(g(x) = x - f(x)/f'(x)\).</p>

                <p><strong>Convergence Analysis.</strong> Compute \(g'(x)\):</p>
                <div class="equation">
                    <div class="equation-content">$$g'(x) = 1 - \frac{[f'(x)]^2 - f(x)f''(x)}{[f'(x)]^2} = \frac{f(x)f''(x)}{[f'(x)]^2}.$$</div>
                    <div class="equation-number">(3.2)</div>
                </div>
                <p>At a simple root \(f(x^*) = 0\) with \(f'(x^*) \neq 0\), we have \(g'(x^*) = 0\). Thus Newton's method achieves quadratic convergence.</p>

                <p>For \(g''(x)\), further differentiation (omitted for brevity) yields:</p>
                <div class="equation">
                    <div class="equation-content">$$g''(x^*) = \frac{f''(x^*)}{f'(x^*)}.$$</div>
                    <div class="equation-number">(3.3)</div>
                </div>
                <p>From Equation (2.3) with second-order terms:</p>
                <div class="equation">
                    <div class="equation-content">$$e_{n+1} \approx \frac{g''(x^*)}{2} e_n^2 = \frac{f''(x^*)}{2f'(x^*)} e_n^2.$$</div>
                    <div class="equation-number">(3.4)</div>
                </div>

                <p><strong>Practical Example: Computing \(\sqrt{2}\).</strong> Solve \(f(x) = x^2 - 2 = 0\). Newton's iteration becomes:</p>
                <div class="equation">
                    <div class="equation-content">$$x_{n+1} = x_n - \frac{x_n^2 - 2}{2x_n} = \frac{x_n + 2/x_n}{2}.$$</div>
                    <div class="equation-number">(3.5)</div>
                </div>
                <p>This is the Babylonian method. Starting with \(x_0 = 1\):</p>

                <table class="results-table">
                    <thead>
                        <tr>
                            <th>\(n\)</th>
                            <th>\(x_n\)</th>
                            <th>\(|e_n|\)</th>
                            <th>\(|e_{n+1}|/|e_n|^2\)</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr><td>0</td><td>1.000000</td><td>0.414214</td><td>---</td></tr>
                        <tr><td>1</td><td>1.500000</td><td>0.085786</td><td>0.500</td></tr>
                        <tr><td>2</td><td>1.416667</td><td>0.002453</td><td>0.333</td></tr>
                        <tr><td>3</td><td>1.414216</td><td>\(2.12 \times 10^{-6}\)</td><td>0.353</td></tr>
                        <tr><td>4</td><td>1.414214</td><td>\(1.59 \times 10^{-12}\)</td><td>0.354</td></tr>
                    </tbody>
                </table>
                <p class="table-caption">Table 3.1: Newton's method for \(\sqrt{2}\) (\(x^* \approx 1.414214\)). Quadratic convergence evident as \(|e_{n+1}|/|e_n|^2 \approx\) constant.</p>

                <h4>3.2 Secant Method</h4>

                <p>When \(f'(x)\) is unavailable or expensive, the secant method approximates the derivative:</p>
                <div class="equation">
                    <div class="equation-content">$$x_{n+1} = x_n - f(x_n) \frac{x_n - x_{n-1}}{f(x_n) - f(x_{n-1})}.$$</div>
                    <div class="equation-number">(3.6)</div>
                </div>
                <p>This is a two-point iteration (requires \(x_0\) and \(x_1\)).</p>

                <p><strong>Superlinear Convergence.</strong> Analysis shows the secant method converges with order \(p = (1 + \sqrt{5})/2 \approx 1.618\), the golden ratio [BurdenFaires2010]. This superlinear rate lies between linear (FPI) and quadratic (Newton), with the advantage of not requiring derivative evaluation.</p>

                <h4>3.3 Successive Over-Relaxation (SOR)</h4>

                <p>For linear systems \(Ax = b\), split \(A = D - L - U\) (diagonal, strict lower, strict upper). The SOR iteration is:</p>
                <div class="equation">
                    <div class="equation-content">$$x_{n+1} = (D - \omega L)^{-1}[\omega b + ((1-\omega)D + \omega U)x_n],$$</div>
                    <div class="equation-number">(3.7)</div>
                </div>
                <p>or component-wise:</p>
                <div class="equation">
                    <div class="equation-content">$$x_i^{(n+1)} = (1-\omega)x_i^{(n)} + \frac{\omega}{a_{ii}} \left(b_i - \sum_{j < i} a_{ij}x_j^{(n+1)} - \sum_{j>i} a_{ij}x_j^{(n)} \right).$$</div>
                    <div class="equation-number">(3.8)</div>
                </div>
                <p>The relaxation parameter \(\omega \in (0, 2)\) controls convergence; \(\omega = 1\) recovers Gauss-Seidel. Optimal \(\omega \approx 1.8\) typically accelerates convergence for elliptic PDEs [Young1971].</p>

                <h4>3.4 Picard Iteration for ODEs</h4>

                <p>Consider the initial value problem \(y' = f(t, y)\), \(y(t_0) = y_0\). The equivalent integral equation is:</p>
                <div class="equation">
                    <div class="equation-content">$$y(t) = y_0 + \int_{t_0}^t f(s, y(s)) ds.$$</div>
                    <div class="equation-number">(3.9)</div>
                </div>
                <p>Picard iteration defines:</p>
                <div class="equation">
                    <div class="equation-content">$$y_{n+1}(t) = y_0 + \int_{t_0}^t f(s, y_n(s)) ds.$$</div>
                    <div class="equation-number">(3.10)</div>
                </div>
                <p>Under Lipschitz condition \(|f(t, y_1) - f(t, y_2)| \leq K|y_1 - y_2|\), this is a contraction on \(C([t_0, T])\) with appropriate norm. The Picard-LindelÃ¶f theorem guarantees convergence to the unique solution [Coddington1955, Iserles2009].</p>
            </div>

           <!-- Practical Considerations -->
<div class="math-content">
    <h3>4. Practical Considerations</h3>
    
    <h4>4.1 Stopping Criteria</h4>
    
    <p><strong>Absolute Error Test:</strong> \(|x_{n+1} - x_n| < \epsilon\).</p>
    <p><strong>Relative Error Test:</strong> \(|x_{n+1} - x_n|/|x_n| < \epsilon\).</p>
    <p><strong>Residual Test:</strong> For root-finding, \(|f(x_n)| < \epsilon\).</p>
    
    <p>Combined criteria improve robustness: stop when \((|x_{n+1} - x_n| < \epsilon_1)\) AND \((|f(x_n)| < \epsilon_2)\).</p>

    <h4>4.2 Convergence Acceleration</h4>
    
    <p>For linearly convergent sequences, Aitken's \(\Delta^2\) process extrapolates:</p>
    <div class="equation">
        <div class="equation-content">$$\hat{x}_n = x_n - \frac{(\Delta x_n)^2}{\Delta^2 x_n} = x_n - \frac{(x_{n+1} - x_n)^2}{x_{n+2} - 2x_{n+1} + x_n}.$$</div>
        <div class="equation-number">(4.1)</div>
    </div>
    <p>This accelerates convergence from linear to quadratic when applied systematically (Steffensen's method) [Brezinski1991].</p>

    <h4>4.3 Choosing the Iteration Function</h4>
    
    <p><strong>Example: Solve \(e^{-x} = x\).</strong></p>
    
    <p><em>Bad choice:</em> \(g_1(x) = e^{-x}\). Near \(x^* \approx 0.567\), \(g_1'(x^*) = -e^{-x^*} \approx -0.567\), so \(|g_1'(x^*)| < 1\) (converges, but slowly).</p>
    
    <p><em>Better choice:</em> \(g_2(x) = -\ln x\) (rearranging \(x = e^{-x}\)). However, \(g_2'(x) = -1/x\), so \(g_2'(x^*) \approx -1.76\) (diverges!).</p>
    
    <p><em>Optimal choice:</em> Apply Newton to \(f(x) = e^{-x} - x\):</p>
    <div class="equation">
        <div class="equation-content">$$g_3(x) = x - \frac{e^{-x} - x}{-e^{-x} - 1} = \frac{x + xe^{-x} + e^{-x}}{1 + e^{-x}}.$$</div>
        <div class="equation-number">(4.2)</div>
    </div>
    <p>This achieves \(g_3'(x^*) = 0\) (quadratic convergence).</p>

    <table class="results-table">
        <thead>
            <tr>
                <th><strong>Method</strong></th>
                <th><strong>Order</strong></th>
                <th><strong>Cost/Iter</strong></th>
                <th><strong>Derivative?</strong></th>
                <th><strong>Global Conv.</strong></th>
            </tr>
        </thead>
        <tbody>
            <tr><td>Basic FPI</td><td>Linear</td><td>1 func eval</td><td>No</td><td>Limited</td></tr>
            <tr><td>Newton</td><td>Quadratic</td><td>1 func + 1 deriv</td><td>Yes</td><td>Local only</td></tr>
            <tr><td>Secant</td><td>Superlinear</td><td>1 func eval</td><td>No</td><td>Local only</td></tr>
            <tr><td>Broyden</td><td>Superlinear</td><td>1 func eval</td><td>No</td><td>With globalization</td></tr>
            <tr><td>SOR</td><td>Linear</td><td>\(O(n)\) ops</td><td>N/A</td><td>Depends on \(\omega\)</td></tr>
        </tbody>
    </table>
    <p class="table-caption">Table 4.1: Comparison of iterative methods.</p>

    <h4>4.4 Multidimensional Extensions</h4>
    
    <p>For \(\mathbf{x} \in \mathbb{R}^n\), iterate \(\mathbf{x}_{n+1} = \mathbf{g}(\mathbf{x}_n)\). Convergence requires spectral radius \(\rho(J_g(\mathbf{x}^*)) < 1\), where \(J_g\) is the Jacobian matrix with entries \((J_g)_{ij} = \partial g_i/\partial x_j\) [Dennis1996].</p>
    
    <p><strong>Newton for Systems:</strong> To solve \(\mathbf{f}(\mathbf{x}) = \mathbf{0}\):</p>
    <div class="equation">
        <div class="equation-content">$$\mathbf{x}_{n+1} = \mathbf{x}_n - J_f(\mathbf{x}_n)^{-1} \mathbf{f}(\mathbf{x}_n),$$</div>
        <div class="equation-number">(4.3)</div>
    </div>
    <p>achieving quadratic convergence when \(J_f(\mathbf{x}^*)\) is nonsingular.</p>
</div>

<!-- Advanced Topics and Applications -->
<div class="math-content">
    <h3>5. Advanced Topics and Applications</h3>
    
    <h4>5.1 Quasi-Newton Methods</h4>
    
    <p>Computing and inverting Jacobians at each iteration is expensive for large \(n\). Quasi-Newton methods approximate \(J_f\) or \(J_f^{-1}\) via rank-one updates [Nocedal2006].</p>
    
    <p><strong>Broyden's Method:</strong> Update \(B_n \approx J_f(\mathbf{x}_n)\) satisfying the secant equation \(B_{n+1}(\mathbf{x}_{n+1} - \mathbf{x}_n) = \mathbf{f}(\mathbf{x}_{n+1}) - \mathbf{f}(\mathbf{x}_n)\):</p>
    <div class="equation">
        <div class="equation-content">$$B_{n+1} = B_n + \frac{(\mathbf{y}_n - B_n \mathbf{s}_n)\mathbf{s}_n^T}{\mathbf{s}_n^T \mathbf{s}_n},$$</div>
        <div class="equation-number">(5.1)</div>
    </div>
    <p>where \(\mathbf{s}_n = \mathbf{x}_{n+1} - \mathbf{x}_n\), \(\mathbf{y}_n = \mathbf{f}(\mathbf{x}_{n+1}) - \mathbf{f}(\mathbf{x}_n)\).</p>
    
    <p><strong>BFGS for Optimization:</strong> To minimize \(f(\mathbf{x})\), iterate \(\mathbf{x}_{n+1} = \mathbf{x}_n - \alpha_n H_n \nabla f(\mathbf{x}_n)\) with \(H_n\) approximating the inverse Hessian, updated via the BFGS formula.</p>

    <h4>5.2 Applications</h4>
    
    <p><strong>Nash Equilibria:</strong> In game theory, Nash equilibria correspond to fixed points of best-response maps. Iterating best responses can locate equilibria in certain games.</p>
    
    <p><strong>Dynamical Systems:</strong> Fixed points \(\mathbf{x}^*\) of \(\mathbf{x}_{n+1} = \mathbf{g}(\mathbf{x}_n)\) are steady states. Stability analysis via \(\rho(J_g(\mathbf{x}^*))\) determines whether perturbations grow or decay.</p>
    
    <p><strong>Nonlinear PDEs:</strong> Semilinear elliptic equations \(-\Delta u = f(u)\) can be solved via Picard iteration on the integral formulation, discretizing spatially to yield finite-dimensional fixed point problems.</p>
    
    <p><strong>Machine Learning:</strong> Implicit layers in deep learning define outputs as fixed points \(y = \sigma(Wy + x)\), solved iteratively. Equilibrium models leverage fixed point theory for neural network design.</p>

    <h4>5.3 Global Convergence Strategies</h4>
    
    <p>Local convergence of Newton-type methods requires \(x_0\) near \(x^*\). Globalization ensures convergence from arbitrary starting points:</p>
    
    <p><strong>Line Search:</strong> Choose step size \(\alpha_n\) to ensure sufficient decrease: \(f(\mathbf{x}_n + \alpha_n \mathbf{p}_n) < f(\mathbf{x}_n) + c\alpha_n \nabla f(\mathbf{x}_n)^T \mathbf{p}_n\).</p>
    
    <p><strong>Trust Region:</strong> Solve \(\min_{\mathbf{p}} m_n(\mathbf{p})\) subject to \(\|\mathbf{p}\| \leq \Delta_n\), where \(m_n\) is a local model (e.g., quadratic approximation). Adjust \(\Delta_n\) based on agreement between \(m_n\) and \(f\).</p>
    
    <p><strong>Continuation:</strong> Solve a family of problems \(\mathbf{f}(\mathbf{x}, \lambda) = \mathbf{0}\) for \(\lambda \in [0, 1]\), with \(\lambda = 0\) trivial and \(\lambda = 1\) the target. Track solutions along the homotopy path.</p>
</div>

<!-- Conclusion -->
<div class="math-content">
    <h3>6. Conclusion</h3>
    
    <p>Fixed point iterative algorithms provide a unifying framework for diverse numerical methods. Banach's contraction principle guarantees existence, uniqueness, and constructive convergence under mild conditions, namely, contractivity \(|g'(x^*)| < 1\). The order of convergence depends critically on derivatives at the fixed point: Newton's method achieves quadratic convergence by ensuring \(g'(x^*) = 0\), while basic fixed point iteration typically converges linearly.</p>
    
    <p><strong>Key Insights:</strong></p>
    <ul>
        <li>Convergence requires \(|g'(x^*)| < 1\) (or \(\rho(J_g) < 1\) in \(\mathbb{R}^n\)).</li>
        <li>Higher-order convergence dramatically reduces iteration counts.</li>
        <li>Newton's method is optimal for smooth problems but requires derivatives and good initialization.</li>
        <li>Quasi-Newton methods balance cost and convergence rate.</li>
    </ul>
    
    <p><strong>Trade-offs:</strong> Simplicity (basic FPI) versus speed (Newton), derivative-free (secant, Broyden) versus fast convergence. Problem structure dictates method selection.</p>
    
    <p><strong>Future Directions:</strong> Stochastic fixed point iteration for noisy problems, fixed point formulations in deep learning (implicit layers, equilibrium models), proximal algorithms for compressed sensing, and operator splitting for large-scale optimization continue to expand the reach of fixed point theory into modern computational challenges.</p>
</div>

<!-- Notation Summary -->
<div class="math-content">
    <h3>Notation Summary</h3>
    
    <table class="results-table">
        <thead>
            <tr>
                <th><strong>Symbol</strong></th>
                <th><strong>Meaning</strong></th>
            </tr>
        </thead>
        <tbody>
            <tr><td>\(x^*\)</td><td>Fixed point satisfying \(g(x^*) = x^*\)</td></tr>
            <tr><td>\(x_n\)</td><td>\(n\)-th iterate</td></tr>
            <tr><td>\(e_n\)</td><td>Error: \(e_n = x_n - x^*\)</td></tr>
            <tr><td>\(g: X \to X\)</td><td>Iteration function</td></tr>
            <tr><td>\(L\)</td><td>Lipschitz/contraction constant</td></tr>
            <tr><td>\(p\)</td><td>Order of convergence</td></tr>
            <tr><td>\(J_g\)</td><td>Jacobian matrix of \(\mathbf{g}\)</td></tr>
            <tr><td>\(\rho(A)\)</td><td>Spectral radius of matrix \(A\)</td></tr>
            <tr><td>\(\epsilon\)</td><td>Tolerance for stopping criterion</td></tr>
        </tbody>
    </table>
</div>

<!-- Appendix -->
<div class="math-content">
    <h3>Appendix</h3>
    
    <h4>Worked Example 2: Comparing Iteration Functions</h4>
    
    <p><strong>Problem:</strong> Solve \(e^{-x} = x\) (intersection of exponential decay and identity).</p>
    
    <p><strong>Approach 1:</strong> \(g_1(x) = e^{-x}\). Starting with \(x_0 = 0.5\):</p>
    
    <table class="results-table">
        <thead>
            <tr>
                <th>\(n\)</th>
                <th>\(x_n\)</th>
                <th>\(|x_{n+1} - x_n|\)</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>0</td><td>0.500000</td><td>---</td></tr>
            <tr><td>1</td><td>0.606531</td><td>0.106531</td></tr>
            <tr><td>2</td><td>0.545239</td><td>0.061292</td></tr>
            <tr><td>3</td><td>0.579612</td><td>0.034373</td></tr>
            <tr><td>4</td><td>0.559898</td><td>0.019714</td></tr>
            <tr><td>5</td><td>0.571143</td><td>0.011245</td></tr>
            <tr><td>10</td><td>0.567143</td><td>0.000729</td></tr>
        </tbody>
    </table>
    
    <p>Convergence is linear with \(g_1'(x^*) = -e^{-x^*} \approx -0.567\), so \(|g_1'(x^*)| < 1\) (converges slowly).</p>
    
    <p><strong>Approach 2:</strong> Apply Newton to \(f(x) = e^{-x} - x\), \(f'(x) = -e^{-x} - 1\):</p>
    <div class="equation">
        <div class="equation-content">$$g_{\text{Newton}}(x) = x - \frac{e^{-x} - x}{-e^{-x} - 1} = \frac{x(1 + e^{-x}) + e^{-x}}{1 + e^{-x}}.$$</div>
        <div class="equation-number">(A.1)</div>
    </div>
    
    <p>Starting with \(x_0 = 0.5\):</p>
    
    <table class="results-table">
        <thead>
            <tr>
                <th>\(n\)</th>
                <th>\(x_n\)</th>
                <th>\(|x_{n+1} - x_n|\)</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>0</td><td>0.500000</td><td>---</td></tr>
            <tr><td>1</td><td>0.566311</td><td>0.066311</td></tr>
            <tr><td>2</td><td>0.567143</td><td>0.000832</td></tr>
            <tr><td>3</td><td>0.567143</td><td>\(7.6 \times 10^{-7}\)</td></tr>
        </tbody>
    </table>
    
    <p>Quadratic convergence: errors square at each step. Newton converges in 3 iterations versus 10+ for basic FPI.</p>
    
    <p><strong>Lesson:</strong> Choosing \(g\) to minimize \(|g'(x^*)|\) (ideally \(g'(x^*) = 0\)) dramatically accelerates convergence.</p>

    <h4>Implementation Remarks</h4>
    
    <p><strong>Practical Tips:</strong></p>
    <ol>
        <li><em>Division by zero:</em> In Newton's method, check \(|f'(x_n)| > \epsilon_{\min}\) before dividing. Near turning points or multiple roots, switch to bisection or secant.</li>
        <li><em>Overflow prevention:</em> For exponentials or powers, monitor intermediate values. Use logarithmic transformations when possible.</li>
        <li><em>Initial guess:</em> Poor \(x_0\) can cause divergence even for Newton. Combine with bracketing methods (bisection) or use continuation from simpler problems.</li>
        <li><em>Stagnation detection:</em> If \(|x_{n+1} - x_n| < \epsilon\) but \(|f(x_n)| \gg \epsilon\), the iteration may be converging to a point where \(f \neq 0\) (e.g., local minimum of \(|f|\)). Check residual independently.</li>
    </ol>
    
    <p><strong>Modern Connections:</strong></p>
    <ul>
        <li><em>Deep Learning:</em> Implicit layers define outputs via \(y = \phi(Wy + x; \theta)\), solved by fixed point iteration. Training optimizes \(\theta\) while ensuring contractivity for fast inference.</li>
        <li><em>Compressed Sensing:</em> Proximal algorithms (ISTA, FISTA) are fixed point iterations for solving \(\min_x \frac{1}{2}\|Ax - b\|^2 + \lambda\|x\|_1\). Fixed point formulation enables convergence analysis.</li>
        <li><em>Game Theory:</em> Computing Nash equilibria via iterated best responses is fixed point iteration on strategy spaces. Uniqueness follows from contraction when games are contractive.</li>
    </ul>
</div>

            <!-- References -->
            <div class="math-content">
                <h3>References</h3>
                <div class="reference-list">
                    <div class="reference-item">
                        <span class="reference-citation">[Atkinson1989]</span>
                        <p class="reference-content">
                            K. E. Atkinson, <em>An Introduction to Numerical Analysis</em>, 2nd Edition, John Wiley & Sons, New York (1989).
                        </p>
                    </div>
                    <div class="reference-item">
                        <span class="reference-citation">[Brezinski1991]</span>
                        <p class="reference-content">
                            C. Brezinski and M. Redivo-Zaglia, <em>Extrapolation Methods: Theory and Practice</em>, North-Holland (1991).
                        </p>
                    </div>
                    <div class="reference-item">
                        <span class="reference-citation">[BurdenFaires2010]</span>
                        <p class="reference-content">
                            R. L. Burden and J. D. Faires, <em>Numerical Analysis</em>, 9th ed., Brooks/Cole, , Cengage Learning, Boston (2010).
                        </p>
                    </div>
                    <div class="reference-item">
                        <span class="reference-citation">[Coddington1955]</span>
                        <p class="reference-content">
                            E. A. Coddington and N. Levinson, <em>Theory of Ordinary Differential Equations</em>, McGraw-Hill (1955).
                        </p>
                    </div>
                    <div class="reference-item">
                        <span class="reference-citation">[Dennis1996]</span>
                        <p class="reference-content">
                            J. E. Dennis and R. B. Schnabel, <em>Numerical Methods for Unconstrained Optimization and Nonlinear Equations</em>, SIAM (1996).
                        </p>
                    </div>
                    <div class="reference-item">
                        <span class="reference-citation">[Iserles2009]</span>
                        <p class="reference-content">
                            A. Iserles, <em>A First Course in the Numerical Analysis of Differential Equations</em>, 2nd ed., Cambridge University Press (2009).
                        </p>
                    </div>
                    <div class="reference-item">
                        <span class="reference-citation">[Kelley1995]</span>
                        <p class="reference-content">
                            C. T. Kelley, <em>Iterative Methods for Linear and Nonlinear Equations</em>, SIAM (1995).
                        </p>
                    </div>
                    <div class="reference-item">
                        <span class="reference-citation">[Nocedal2006]</span>
                        <p class="reference-content">
                            J. Nocedal and S. J. Wright, <em>Numerical Optimization</em>, 2nd ed., Springer (2006).
                        </p>
                    </div>
                    <div class="reference-item">
                        <span class="reference-citation">[Ortega2000]</span>
                        <p class="reference-content">
                            J. M. Ortega and W. C. Rheinboldt, <em>Iterative Solution of Nonlinear Equations in Several Variables</em>, SIAM (2000). [Originally published 1970]
                        </p>
                    </div>
                    <div class="reference-item">
                        <span class="reference-citation">[Young1971]</span>
                        <p class="reference-content">
                            D. M. Young, <em>Iterative Solution of Large Linear Systems</em>, Academic Press (1971).
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
</div>


                <!-- Add more project cards as needed -->
            </div>
        </div>
    </div>

    <script>
        // Quotes array
        const quotes = [
            {
                text: "Mathematics is not about numbers, equations, computations, or algorithms: it is about understanding.",
                author: "William Paul Thurston"
            },
            {
                text: "Pure mathematics is, in its way, the poetry of logical ideas.",
                author: "Albert Einstein"
            },
            {
                text: "Mathematics is the music of reason.",
                author: "James Joseph Sylvester"
            },
            {
                text: "In mathematics, you don't understand things. You just get used to them.",
                author: "John von Neumann"
            },
            {
                text: "Mathematics is the most beautiful and most powerful creation of the human spirit.",
                author: "Stefan Banach"
            },
            {
                text: "The essence of mathematics lies in its freedom.",
                author: "Georg Cantor"
            }
        ];

        let currentQuoteIndex = 0;
        let expositoryListVisible = false;

        // Quote rotation
        document.querySelector('.quote-section').addEventListener('click', function() {
            currentQuoteIndex = (currentQuoteIndex + 1) % quotes.length;
            const quote = quotes[currentQuoteIndex];
            
            const quoteText = document.getElementById('quoteText');
            const quoteAuthor = document.getElementById('quoteAuthor');
            
            quoteText.style.opacity = '0';
            quoteAuthor.style.opacity = '0';
            
            setTimeout(() => {
                quoteText.textContent = quote.text;
                quoteAuthor.textContent = quote.author;
                
                quoteText.style.transition = 'opacity 0.5s';
                quoteAuthor.style.transition = 'opacity 0.5s';
                quoteText.style.opacity = '1';
                quoteAuthor.style.opacity = '1';
            }, 300);
        });

        // Toggle Expository List
        function toggleExpositoryList() {
            const list = document.getElementById('expositoryList');
            const btnText = document.getElementById('btnText');
            const btnIcon = document.getElementById('btnIcon');
            
            expositoryListVisible = !expositoryListVisible;
            
            if (expositoryListVisible) {
                list.classList.add('show');
                btnText.textContent = 'Hide Expositions';
                btnIcon.className = 'fas fa-arrow-up';
            } else {
                list.classList.remove('show');
                btnText.textContent = 'Start Exploring';
                btnIcon.className = 'fas fa-arrow-right';
            }
        }

        // Toggle Individual Project Content
        function toggleProjectContent(projectId) {
            const content = document.getElementById(`content-${projectId}`);
            const btn = document.getElementById(`btn-${projectId}`);
            
            content.classList.toggle('active');
            btn.classList.toggle('active');
        }
    </script>


    

   

    
  


<footer id="contact">
  <div class="container">
    <div class="social-links">
      <a href="https://web.facebook.com/people/Clinton-Oluranran-Kayoh"><i class="fab fa-facebook-f"></i></a>
      <a href=" https://x.com/KayohClinton?t=mo8mduk1w13YogTb0kQzeA&s=08"><i class="fab fa-twitter"></i></a>
      <a href="https://www.linkedin.com/KayohClinton"><i class="fab fa-linkedin"></i></a>
      <a href="https://github.com/ClintonKayoh"><i class="fab fa-github"></i></a>
      <a href="https://www.researchgate.net/publication/394704096_On_the_Time_to_Stationarity_of_Peer-Driven_Adoption_and_Event-Driven_Abandonment_of_Digital_Asset_Trends_on_Social_Networks"><i class="fab fa-researchgate"></i></a>
      <a href="https://scholar.google.com/citations?user=tQ_u2kEAAAAJ&hl=en"><i class="fab fa-google"></i></a>
      <a href="https://independent.academia.edu/kayohclinton"><i class="fas fa-graduation-cap"></i></a>
    </div>
    <p>&copy; 2025 Clinton Oluranran Kayoh. All rights reserved.</p>
  </div>
</footer>

  <style>
       
.reference-note {
    font-size: 0.9rem;
    color: #6c757d;
    margin-top: 0.5rem;
    font-style: italic;
}

        .definition, .theorem, .proposition, .lemma, .example, .remark, .proof {
            background: #f8f9fa;
            border-left: 4px solid #667eea;
            padding: 1rem;
            margin: 1rem 0;
            border-radius: 5px;
        }

        .theorem, .proposition, .lemma {
            border-left-color: #28a745;
        }

        .example {
            border-left-color: #ffc107;
        }

        .remark {
            border-left-color: #17a2b8;
        }

        .proof {
            border-left-color: #6c757d;
        }

        .definition-title, .theorem-title, .proposition-title, 
        .example-title, .remark-title, .proof-title {
            font-weight: bold;
            color: #667eea;
            margin-bottom: 0.5rem;
        }

        .theorem-title, .proposition-title {
            color: #28a745;
        }

        .example-title {
            color: #ffc107;
        }

        .remark-title {
            color: #17a2b8;
        }

        .proof-title {
            color: #6c757d;
        }

        .equation {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin: 1rem 0;
            padding: 0.5rem;
        }

        .equation-content {
            flex: 1;
            text-align: center;
        }

        .equation-number {
            color: #667eea;
            font-weight: bold;
            margin-left: 1rem;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1rem 0;
            background: white;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        table caption {
            font-weight: bold;
            margin-bottom: 0.5rem;
            color: #667eea;
            font-size: 1.1rem;
        }

        th {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 1rem;
            text-align: left;
        }

        td {
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #ddd;
        }

        tr:hover {
            background: #f8f9fa;
        }

        .algorithm {
            background: #f8f9fa;
            border: 2px solid #667eea;
            padding: 1rem;
            margin: 1rem 0;
            border-radius: 5px;
            font-family: 'Courier New', monospace;
        }

        .algorithm-title {
            font-weight: bold;
            color: #667eea;
            margin-bottom: 0.5rem;
        }


        .reference-list {
            margin-left: 0;
        }

        .reference-item {
            margin-bottom: 1rem;
            display: flex;
            gap: 1rem;
        }

        .reference-citation {
            font-weight: bold;
            color: #667eea;
            min-width: 120px;
        }

        .reference-content {
            flex: 1;
        }

        .glossary-list dt {
            font-weight: bold;
            color: #667eea;
            margin-top: 1rem;
        }

        .glossary-list dd {
            margin-left: 2rem;
            margin-bottom: 0.5rem;
        }

        .historical-timeline {
            list-style: none;
            margin-left: 0;
        }

        .historical-timeline li {
            padding-left: 2rem;
            position: relative;
            margin-bottom: 1rem;
        }

        .historical-timeline li:before {
            content: "â—";
            position: absolute;
            left: 0;
            color: #667eea;
            font-size: 1.5rem;
        }

        .quote {
            background: #f8f9fa;
            border-left: 4px solid #667eea;
            padding: 1rem;
            margin: 1rem 0;
            font-style: italic;
        }

        .quote-author {
            text-align: right;
            font-weight: bold;
            margin-top: 0.5rem;
        }

        @media (max-width: 768px) {
            body {
                padding: 1rem;
            }

            .project-title {
                font-size: 1.5rem;
            }

            .project-meta {
                flex-direction: column;
                gap: 0.5rem;
            }

            table {
                font-size: 0.9rem;
            }

            th, td {
                padding: 0.5rem;
            }
        }
    </style>

<style>
/* Additional styles for the fixed point content */
.algorithm {
    margin: 1.5rem 0;
    padding: 1.5rem;
    background: #f8f9fa;
    border-left: 4px solid #007bff;
    border-radius: 4px;
}

.algorithm-title {
    font-weight: bold;
    color: #007bff;
    margin-bottom: 1rem;
}

.algorithm-content {
    font-family: 'Courier New', monospace;
    background: white;
    padding: 1rem;
    border-radius: 4px;
    border: 1px solid #dee2e6;
}

.proof {
    margin: 1.5rem 0;
    padding: 1rem;
    background: #e8f5e8;
    border-left: 4px solid #28a745;
    border-radius: 4px;
}

.proof-title {
    font-weight: bold;
    color: #28a745;
    margin-bottom: 0.5rem;
}

.results-table {
    width: 100%;
    border-collapse: collapse;
    margin: 1rem 0;
}

.results-table th,
.results-table td {
    padding: 0.75rem;
    text-align: center;
    border: 1px solid #dee2e6;
}

.results-table th {
    background-color: var(--primary-color);
    font-weight: bold;
}

.table-caption {
    text-align: center;
    font-style: italic;
    color: #6c757d;
    margin-top: 0.5rem;
}
</style>

<style>
/* Table Styling */

.notation-table {
    width: 100%;
    border-collapse: collapse;
    background: white;
    box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    border-radius: 8px;
    overflow: hidden;
}

.notation-table caption {
    font-weight: bold;
    font-size: 1.1em;
    margin-bottom: 1rem;
    color: var(--text-color);
}

.notation-table th,
.notation-table td {
    padding: 1rem;
    text-align: left;
    border-bottom: 1px solid #eee;
}

.notation-table th {
    background: var(--secondary-color);
    color: var(--primary-color);
    font-weight: 600;
}

.notation-table tr:last-child td {
    border-bottom: none;
}

.notation-table tr:hover {
    background: var(--bg-color-light);
}

/* Code Styling */
.code-container {
    margin: 1.5rem 0;
    background: #f8f9fa;
    border-radius: 8px;
    overflow: hidden;
    box-shadow: 0 2px 6px rgba(0,0,0,0.1);
}

.code-container pre {
    margin: 0;
    padding: 1.5rem;
    overflow-x: auto;
    font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
    font-size: 0.9em;
    line-height: 1.4;
}

.code-container code {
    background: none;
    padding: 0;
}

/* Equation Styling */
.equation {
    display: flex;
    align-items: center;
    justify-content: space-between;
    margin: 1.5rem 0;
    padding: 1.5rem;
    background: white;
    border-radius: 8px;
    box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    border-left: 4px solid var(--secondary-color);
}

.equation-content {
    flex: 1;
    text-align: center;
    padding: 0 2rem;
}

.equation-number {
    color: var(--secondary-color);
    font-weight: bold;
    font-size: 1em;
    padding: 0.5rem 1rem;
    background: var(--bg-color-light);
    border-radius: 4px;
    min-width: 60px;
    text-align: center;
}

/* Glossary Styling */
.glossary-list {
    margin: 2rem 0;
}

.glossary-list dt {
    font-weight: bold;
    color: var(--secondary-color);
    margin-top: 1.5rem;
    margin-bottom: 0.5rem;
    font-size: 1.1em;
}

.glossary-list dd {
    margin-left: 1rem;
    margin-bottom: 1rem;
    padding-left: 1rem;
    border-left: 3px solid var(--bg-color-light);
}

/* Historical Timeline */
.historical-timeline {
    margin: 2rem 0;
    padding-left: 1.5rem;
    border-left: 3px solid var(--secondary-color);
}

.historical-timeline li {
    margin-bottom: 1.5rem;
    padding-left: 1rem;
    position: relative;
}

.historical-timeline li::before {
    content: '';
    position: absolute;
    left: -1.5rem;
    top: 0.5rem;
    width: 12px;
    height: 12px;
    background: var(--secondary-color);
    border-radius: 50%;
}

/* Quote Styling */
.quote {
    margin: 2rem 0;
    padding: 2rem;
    background: var(--bg-color-light);
    border-left: 4px solid var(--primary-color);
    border-radius: 8px;
    font-style: italic;
}

.quote-author {
    text-align: right;
    margin-top: 1rem;
    font-weight: bold;
    color: var(--text-color);
    font-style: normal;
}

/* Responsive Design */
@media (max-width: 768px) {
    .equation {
        flex-direction: column;
        padding: 1rem;
    }
    
    .equation-content {
        padding: 0;
        margin-bottom: 1rem;
    }
    
    .notation-table th,
    .notation-table td {
        padding: 0.75rem;
    }
    
    .code-container pre {
        padding: 1rem;
        font-size: 0.85em;
    }
    
    .historical-timeline {
        padding-left: 1rem;
    }
}
</style>


<style>
/* Additional styling for this specific content */
.toc-container {
    background: var(--bg-color-light);
    padding: 1.5rem;
    border-radius: 8px;
    margin: 2rem 0;
    border-left: 4px solid var(--primary-color);
}

.toc-list {
    list-style-type: none;
    padding-left: 0;
}

.toc-list li {
    margin: 0.5rem 0;
    padding-left: 1rem;
    position: relative;
}

.toc-list li:before {
    content: "â€¢";
    color: var(--secondary-color);
    font-weight: bold;
    position: absolute;
    left: 0;
}

.toc-list a {
    color: var(--text-color);
    text-decoration: none;
    transition: color 0.3s ease;
}

.toc-list a:hover {
    color: var(--secondary-color);
}

.algorithm {
    background: #f8f9fa;
    border: 1px solid #e9ecef;
    border-radius: 6px;
    padding: 1.5rem;
    margin: 1.5rem 0;
    font-family: 'Courier New', monospace;
}

.algorithm-title {
    font-weight: bold;
    color: var(--secondary-color);
    margin-bottom: 1rem;
    font-size: 1.1em;
}

.algorithm-content {
    line-height: 1.6;
}

.result-table {
    width: 100%;
    border-collapse: collapse;
    margin: 1rem 0;
    font-size: 0.9em;
}

.result-table th,
.result-table td {
    padding: 0.75rem;
    text-align: center;
    border: 1px solid #dee2e6;
}

.result-table th {
    background-color: var(--bg-color-light);
    font-weight: bold;
}

.result-table tr:nth-child(even) {
    background-color: #f8f9fa;
}

/* Responsive tables */
@media (max-width: 768px) {
    .result-table {
        font-size: 0.8em;
    }
    
    .result-table th,
    .result-table td {
        padding: 0.5rem;
    }
}
</style>

<style>

.notation-table {
    width: 100%;
    border-collapse: collapse;
    margin: 15px 0;
}

.notation-table th, .notation-table td {
    padding: 8px 12px;
    border: 1px solid #ddd;
    text-align: left;
}

.notation-table th {
    background: #f8f9fa;
    font-weight: bold;
}

.notation-table tr:nth-child(even) {
    background: #f8f9fa;
}

/* Math styling */
.math-content p {
    line-height: 1.6;
    margin-bottom: 12px;
}

.math-content ul {
    margin: 10px 0;
    padding-left: 20px;
}

.math-content li {
    margin-bottom: 5px;
    line-height: 1.5;
}

 .reference-list {
                    margin: 20px 0;
                }

                .reference-item {
                    margin-bottom: 15px;
                    padding: 10px;
                    border-left: 3px solid var(--secondary-color);
                    background-color: var(--bg-color-light);
                }

                .reference-citation {
                    font-weight: bold;
                    color: var(--secondary-color);
                    display: block;
                    margin-bottom: 5px;
                }

                .reference-content {
                    margin: 0;
                    line-height: 1.5;
                }

                .reference-content em {
                    font-style: italic;
                    color: var(--text-color);
                }

                .reference-content strong {
                    font-weight: bold;
                    color: var(--text-color);
                }

    /* Figure Container Styles */
.figure-container {
    margin: 2rem 0;
    padding: 1.5rem;
    background: var(--bg-color-light);
    border-radius: 8px;
    border-left: 4px solid var(--secondary-color);
    text-align: center;
}

.figure-title {
    font-weight: bold;
    font-size: 1.1em;
    margin-bottom: 1rem;
    color: var(--text-color);
    text-align: center;
}

.figure-image {
    max-width: 50%;
    height: auto;
    border-radius: 4px;
    box-shadow: 0 2px 8px rgba(0,0,0,0.1);
}

.figure-svg {
    display: flex;
    justify-content: center;
    margin: 1rem 0;
}

.figure-caption {
    margin-top: 1rem;
    font-size: 0.9em;
    line-height: 1.4;
    color: var(--text-color);
    text-align: left;
    font-style: italic;
}

/* Subfigure Grid */
.subfigure-grid {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
    gap: 2rem;
    margin: 1rem 0;
}

.subfigure {
    text-align: center;
}

.subfigure-image {
    max-width: 100%;
    height: auto;
    border-radius: 4px;
    box-shadow: 0 2px 6px rgba(0,0,0,0.1);
}

.subfigure-caption {
    margin-top: 0.5rem;
    font-size: 0.85em;
    line-height: 1.3;
    color: var(--text-color);
}

/* Plot Container */
.plot-container {
    background: white;
    padding: 1rem;
    border-radius: 4px;
    box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    margin: 1rem 0;
}

/* Responsive Design */
@media (max-width: 768px) {
    .figure-container {
        margin: 1.5rem 0;
        padding: 1rem;
    }
    
    .subfigure-grid {
        grid-template-columns: 1fr;
        gap: 1.5rem;
    }
    
    .plot-container {
        padding: 0.5rem;
    }
}

.equation {
    display: flex;
    align-items: center;
    justify-content: center;
    margin: 1rem 0;
    padding: 1rem;
    background: var(--bg-color-light);
    border-radius: 4px;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    position: relative;
}

.equation-content {
    flex: 1;
    text-align: center;
    padding-right: 3rem;
}

.equation-number {
    position: absolute;
    right: 1rem;
    color: var(--secondary-color);
    font-weight: bold;
    font-size: 0.9em;
}

/* For responsive design */
@media (max-width: 768px) {
    .equation {
        flex-direction: column;
        padding: 0.75rem;
    }
    
    .equation-content {
        padding-right: 0;
        margin-bottom: 0.5rem;
    }
    
    .equation-number {
        position: static;
        text-align: center;
    }
}
</style>

<script>
function toggleContent(projectId) {
    const content = document.getElementById(`content-${projectId}`);
    const btn = document.getElementById(`btn-${projectId}`);
    const icon = btn.querySelector('i');
    
    if (content.style.maxHeight) {
        content.style.maxHeight = null;
        icon.className = 'fas fa-plus';
    } else {
        content.style.maxHeight = content.scrollHeight + "px";
        icon.className = 'fas fa-minus';
    }
}
</script>    

    <script>
        // 3D Torus Animation
        let scene, camera, renderer, torus;

        function init3D() {
            scene = new THREE.Scene();
            camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
            renderer = new THREE.WebGLRenderer({ canvas: document.getElementById('canvas3d'), alpha: true });
            renderer.setSize(window.innerWidth, window.innerHeight);

            // Create torus
            const geometry = new THREE.TorusGeometry(10, 3, 16, 100);
            const material = new THREE.MeshPhongMaterial({
                color: 0x667eea,
                specular: 0xffffff,
                shininess: 100,
                wireframe: false,
                transparent: true,
                opacity: 0.7
            });
            torus = new THREE.Mesh(geometry, material);
            scene.add(torus);

            // Lights
            const pointLight = new THREE.PointLight(0xffffff, 1);
            pointLight.position.set(20, 20, 20);
            scene.add(pointLight);

            const ambientLight = new THREE.AmbientLight(0x404040);
            scene.add(ambientLight);

            camera.position.z = 30;

            animate();
        }

        function animate() {
            requestAnimationFrame(animate);
            torus.rotation.x += 0.01;
            torus.rotation.y += 0.005;
            renderer.render(scene, camera);
        }

        // Toggle project content
        function toggleContent(projectId) {
            const content = document.getElementById(`content-${projectId}`);
            const btn = document.getElementById(`btn-${projectId}`);
            
            content.classList.toggle('expanded');
            btn.classList.toggle('expanded');

            if (content.classList.contains('expanded')) {
                // Trigger MathJax to render
                MathJax.typesetPromise([content]);
            }
        }

        // Reading progress bar
        function updateReadingProgress() {
            const scrollTotal = document.documentElement.scrollHeight - window.innerHeight;
            const scrollProgress = (window.scrollY / scrollTotal) * 100;
            document.getElementById('readingProgress').style.width = scrollProgress + '%';
        }

        // Initialize
        window.addEventListener('load', () => {
            init3D();
            window.addEventListener('scroll', updateReadingProgress);
        });

        // Handle window resize
        window.addEventListener('resize', () => {
            camera.aspect = window.innerWidth / window.innerHeight;
            camera.updateProjectionMatrix();
            renderer.setSize(window.innerWidth, window.innerHeight);
        });
    </script>

<script src="scriptrest.js"></script> 

</body>
</html>